// File: clean.py
import subprocess
import os

def clean_project(project_dir):
  """
  Cleans a Python project by removing unused imports and variables using Autoflake.

  Args:
    project_dir: The path to the root directory of the Python project.
  """

  exclude_paths = [
      "venv",
      "migrations",
      ".mypy_cache",
      "__pycache__",
      # Add any other directories or file names to exclude
  ]

  for root, _, files in os.walk(project_dir):
      for file in files:
          if file.endswith(".py") and not any(exclude in root for exclude in exclude_paths):
              file_path = os.path.join(root, file)
              try:
                  subprocess.run(
                      [
                          "autoflake",
                          "--in-place",
                          "--remove-unused-variables",
                          "--remove-all-unused-imports",
                          file_path,
                      ],
                      check=True,
                  )
                  print(f"Cleaned: {file_path}")
              except subprocess.CalledProcessError as e:
                  print(f"Error cleaning {file_path}: {e}")

if __name__ == "__main__":
  project_directory = r"C:\Users\dylan\Desktop\sheeplz-crypto-bot"  # Your project directory
  clean_project(project_directory)

// File: config.py
import yaml
import logging
from typing import List, Dict, Any
from pydantic import BaseModel, field_validator, PositiveInt

class Config(BaseModel):
    """Configuration for DataIngestion and indicators."""

    symbol: str = "BTC_USDT"
    interval: str = "Min1"
    timeframes: List[str] = ["1m", "5m", "15m", "1h", "4h"]
    indicators: List[str] = ["price", "volume", "rsi", "macd", "fibonacci"]

    # GMN Parameters
    max_history_length: int = 1000

    # LNN Parameters
    lnn_model_path: str = "models/lnn/lnn_model.pth"
    lnn_hidden_size: int = 64
    lnn_training_epochs: int = 10
    training_history_length: int = 500
    lnn_learning_rate: float = 0.001

    # Agent Parameters
    threshold_buy: float = 0.7
    threshold_sell: float = 0.3

    # Risk Management
    risk_parameters: Dict[str, Any] = {}

    # Trade Execution
    trade_parameters: Dict[str, Any] = {}

    # System
    agent_loop_delay: PositiveInt = 1
    reconnect_delay: PositiveInt = 5
    log_level: str = "INFO"

    # Device
    device: str = "cpu"

    # Private Channels
    private_channels: List[str] = []

    @field_validator('timeframes', mode='before')
    def validate_timeframes(cls, v):
        if isinstance(v, str):
            return [tf.strip() for tf in v.split(',')]
        elif isinstance(v, list):
            return v
        else:
            raise ValueError("timeframes must be a comma-separated string or a list")

    def load_from_yaml(self, config_path: str):
        """Loads configuration from a YAML file."""
        try:
            with open(config_path, "r") as f:
                config_data: Dict[str, Any] = yaml.safe_load(f)
                for key, value in config_data.items():
                    if hasattr(self, key):
                        setattr(self, key, value)
        except FileNotFoundError:
            raise FileNotFoundError(f"Config file not found: {config_path}")
        except yaml.YAMLError as e:
            raise ValueError(f"Error parsing YAML config file: {e}")

// File: crypto_trading_bot.py
# crypto_trading_bot.py
import asyncio
import logging
import tracemalloc
import os
from typing import Tuple, Optional, Dict, List, Any
import torch
import numpy as np
import aiohttp
import joblib
from sklearn.preprocessing import MinMaxScaler

from data.mexc_data_ingestion import DataIngestion, Config as DataIngestionConfig
from models.agents.agent import TradingAgent
from models.gmn.gmn import CryptoGMN
from models.lnn.lnn_model import LiquidNeuralNetwork
from models.utils.risk_management import RiskManager
from models.utils.config import Config

# Load configuration
config = Config("configs/config.yaml")

# Configure logging
log_file_path = os.path.join(os.getcwd(), "logs", "trading_bot.log")
os.makedirs(os.path.dirname(log_file_path), exist_ok=True)  # Ensure log directory exists

logging.basicConfig(
    level=getattr(logging, config.log_level.upper(), logging.INFO),
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file_path),
        logging.StreamHandler()
    ]
)

# Initialize scaler path (should be defined before use in functions)
scaler_path = "models/lnn/scaler.joblib"

async def main():
    """Main function to run the crypto trading bot."""
    try:
        timeframes = config.timeframes
        indicators = config.indicators
        max_history_length = config.max_history_length

        # Initialize GMN
        gmn = CryptoGMN(timeframes, indicators, max_history_length=max_history_length)

        # Initialize Data Ingestion Config and DataIngestion
        data_ingestion_config = DataIngestionConfig(
            symbol=config.symbol,
            timeframes=config.timeframes,
            private_channels=config.private_channels,
            reconnect_delay=config.reconnect_delay,
            # ... add other data ingestion config parameters as needed
        )
        data_ingestion = DataIngestion(gmn, data_ingestion_config)

        # Initialize Risk Manager
        risk_manager = RiskManager(config.risk_parameters)

        # Load or initialize and fit scaler  (moved here)
        scaler = await load_or_fit_scaler(scaler_path, gmn, config.training_history_length) # Modified

        # Initialize or train LNN Model
        lnn_model_path = config.lnn_model_path
        model = await load_or_train_lnn(gmn, lnn_model_path, config, scaler) # Function handles both loading and training

        if model is None:  # Check if model loading/training failed
            logging.error("Failed to load or train LNN model. Exiting.")
            await shutdown(gmn=gmn, data_ingestion=data_ingestion, risk_manager=risk_manager, scaler=scaler)
            return

        # Initialize TradingAgent *after* model is successfully loaded or trained
        agent = TradingAgent(timeframes, indicators, model, config, risk_manager, scaler)

        tracemalloc.start()  # Start tracemalloc after objects are initialized
        tasks = [
            asyncio.create_task(data_ingestion.connect()),
            asyncio.create_task(agent_loop(agent, gmn))
        ]


        try:
            await asyncio.gather(*tasks)

        except asyncio.CancelledError:
            logging.info("Main tasks cancelled.")
        except Exception as e:
            logging.exception(f"Unhandled exception in main loop: {e}") # Log traceback
        finally:
            # Memory profiling
            snapshot = tracemalloc.take_snapshot()
            top_stats = snapshot.statistics('lineno')
            logging.info("Top 10 memory allocations:")
            for stat in top_stats[:10]:
                logging.info(stat)

            tracemalloc.stop()
            await shutdown(gmn=gmn, data_ingestion=data_ingestion, risk_manager=risk_manager, scaler=scaler, agent=agent)

    except KeyboardInterrupt:
        logging.info("Trading bot stopped by user.")
        # If tasks are running, cancel them here before shutdown.


async def agent_loop(agent: TradingAgent, gmn: CryptoGMN):
    """Agent loop."""
    while True:
        try: # Handle exceptions within the agent loop
            market_data = gmn.get_all_data() # More efficient

            if not all(market_data.values()): # Check if all timeframes have data
                await asyncio.sleep(config.agent_loop_delay)
                continue

            await agent.make_decision(market_data)

        except Exception as e:
            logging.error(f"Error in agent loop: {e}", exc_info=True)  # Include traceback
        await asyncio.sleep(config.agent_loop_delay)





async def load_or_train_lnn(gmn, model_path, config, scaler):
    """Loads or trains the LNN model."""
    try:
        model = LiquidNeuralNetwork(len(config.timeframes) * len(config.indicators), config.lnn_hidden_size, 1)
        model.load_state_dict(torch.load(model_path, map_location=config.device)) # Load on specified device
        model.to(config.device) # Move model to device after loading
        model.eval()
        logging.info("Loaded pre-trained LNN model.")
        return model
    except FileNotFoundError:
        logging.info("Pre-trained LNN model not found. Training a new model...")
        return await train_and_save_lnn(gmn, model_path, config, scaler)
    except Exception as e:
        logging.error(f"Error loading LNN model: {e}", exc_info=True)
        return None


async def train_and_save_lnn(gmn, model_path, config, scaler):
    """Trains and saves LNN model."""

    try:
        X_train, y_train = await prepare_lnn_training_data(gmn, config.training_history_length, scaler) # Pass scaler here

        if X_train is None or y_train is None:
            return None # Return None on failure


        # Convert to tensors
        X_train = torch.tensor(X_train, dtype=torch.float32, device=config.device) # Create on correct device
        y_train = torch.tensor(y_train, dtype=torch.float32, device=config.device) # Use config.device here

        input_size = X_train.shape[1]
        model = LiquidNeuralNetwork(input_size, config.lnn_hidden_size, 1).to(config.device)  # Send to device
        # ... (rest of training loop, including criterion, optimizer)


        # Save the trained model
        torch.save(model.state_dict(), model_path)  # Use state_dict
        model.eval()
        logging.info(f"LNN model trained and saved to {model_path}")

        return model

    except Exception as e:
        logging.error(f"Error during LNN training: {e}", exc_info=True) # Log traceback
        return None



async def prepare_lnn_training_data(gmn: CryptoGMN, history_length: int, scaler: MinMaxScaler) -> Tuple[np.ndarray, np.ndarray]: # Changed
    """Prepares training data."""
    try:
        market_data = gmn.get_all_data()
        # ... (data preparation logic, same as before)


        # Fit and Transform with the scaler
        X_scaled = scaler.fit_transform(X) # Fit the scaler here
        # ... (rest of the method)

    except Exception as e:
        logging.error(f"Error preparing LNN training data: {e}", exc_info=True)  # More detailed
        return None, None


async def load_or_fit_scaler(scaler_path: str, gmn: CryptoGMN, history_length: int) -> MinMaxScaler:  # Changed
    """Loads scaler or fits a new one."""
    try:
        scaler = joblib.load(scaler_path)
        logging.info(f"Loaded scaler from {scaler_path}")
    except FileNotFoundError:
        logging.info("Scaler file not found. Fitting a new scaler...")
        # Prepare data for scaler fitting
        X, _ = await prepare_lnn_training_data(gmn, history_length, None)  # Use unscaled data
        if X is None:
            raise ValueError("Failed to prepare data for scaler fitting.")

        scaler = MinMaxScaler()
        scaler.fit(X)
        joblib.dump(scaler, scaler_path)
        logging.info(f"New scaler fitted and saved to {scaler_path}")

    return scaler


async def shutdown(gmn: CryptoGMN = None, data_ingestion: DataIngestion = None, risk_manager: RiskManager = None, scaler: MinMaxScaler = None, agent: Optional[TradingAgent] = None):
    # ... (same as before)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logging.info("Trading bot stopped by user.")

// File: main.py
import asyncio
import logging
import tracemalloc
import os
from typing import Optional, List

import torch
import numpy as np
import joblib
from sklearn.preprocessing import MinMaxScaler

from data.ingestion.mexc_data_ingestion import DataIngestion
from data.config import Config as DataIngestionConfig
from models.agents.agent import TradingAgent
from models.gmn.gmn import CryptoGMN
from models.lnn.lnn_model import LiquidNeuralNetwork
from models.utils.risk_management import RiskManager
from models.utils.config import Config

config = Config("configs/config.yaml")

log_file_path = os.path.join(os.getcwd(), "logs", "trading_bot.log")
os.makedirs(os.path.dirname(log_file_path), exist_ok=True)

logging.basicConfig(
    level=getattr(logging, config.log_level.upper(), logging.INFO),
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file_path),
        logging.StreamHandler()
    ]
)

scaler_path = "models/lnn/scaler.joblib"

bot_tasks: List[asyncio.Task] = []

async def main():
    global bot_tasks

    try:
        timeframes = config.timeframes
        max_history_length = config.max_history_length

        gmn = CryptoGMN(timeframes, config.indicators, max_history_length=max_history_length, config=config)

        data_ingestion_config = DataIngestionConfig(
            symbol=config.symbol,
            timeframes=config.timeframes,
            private_channels=config.private_channels,
            reconnect_delay=config.reconnect_delay,
            max_retry_attempts=config.max_retry_attempts
        )
        data_ingestion = DataIngestion(gmn, data_ingestion_config)

        risk_manager = RiskManager(config.risk_parameters)
        scaler = await load_scaler(scaler_path)  # No need to pass gmn and history_length
        model = await load_or_train_lnn(gmn, config.lnn_model_path, config, scaler)

        if model is None:
            logging.error("Failed to load or train LNN model. Exiting.")
            await shutdown()  # Call simplified shutdown
            return

        agent = TradingAgent(timeframes, config.indicators, model, config, risk_manager, scaler)
        tracemalloc.start()
        bot_tasks = [
            asyncio.create_task(data_ingestion.connect()),
            asyncio.create_task(agent_loop(agent, gmn))
        ]

        try:
            await asyncio.gather(*bot_tasks)
        except asyncio.CancelledError:
            logging.info("Main tasks cancelled.")
        except Exception as e:
            logging.exception(f"Unhandled exception in main loop: {e}")
        finally:
            snapshot = tracemalloc.take_snapshot()
            top_stats = snapshot.statistics('lineno')
            logging.info("Top 10 memory allocations:")
            for stat in top_stats[:10]:
                logging.info(stat)

            tracemalloc.stop()
            await shutdown()

    except KeyboardInterrupt:
        logging.info("Trading bot stopped by user.")


async def agent_loop(agent: TradingAgent, gmn: CryptoGMN):
    while True:
        try:
            market_data = gmn.get_all_data()
            if not all(market_data.values()):
                await asyncio.sleep(config.agent_loop_delay)
                continue
            await agent.make_decision(market_data)
        except Exception as e:
            logging.error(f"Error in agent loop: {e}", exc_info=True)
        await asyncio.sleep(config.agent_loop_delay)


async def load_or_train_lnn(gmn, model_path, config, scaler):
    try:
        input_size = len(config.timeframes) * len(config.indicators)
        model = LiquidNeuralNetwork(input_size, config.lnn_hidden_size, 1).to(config.device)
        model.load_state_dict(torch.load(model_path, map_location=config.device))
        model.eval()
        logging.info("Loaded pre-trained LNN model.")
        return model
    except FileNotFoundError:
        logging.info("Pre-trained LNN model not found. Training a new model...")
        return await train_and_save_lnn(gmn, model_path, config, scaler)
    except Exception as e:
        logging.exception(f"Error loading LNN model: {e}")
        return None


async def train_and_save_lnn(gmn, model_path, config, scaler):
    try:
        X_train, y_train = await prepare_lnn_training_data(gmn, config.training_history_length, scaler)
        if X_train is None or y_train is None:
            return None
        X_train = torch.tensor(X_train, dtype=torch.float32, device=config.device)
        y_train = torch.tensor(y_train, dtype=torch.float32, device=config.device)
        input_size = X_train.shape[1]
        model = LiquidNeuralNetwork(input_size, config.lnn_hidden_size, 1).to(config.device)
        criterion = torch.nn.BCEWithLogitsLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=config.lnn_learning_rate)
        epochs = config.lnn_training_epochs
        batch_size = 32
        dataset = torch.utils.data.TensorDataset(X_train, y_train)
        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

        for epoch in range(epochs):
            model.train()
            epoch_loss = 0.0
            for batch_X, batch_y in dataloader:
                optimizer.zero_grad()
                outputs = model(batch_X.unsqueeze(1))
                loss = criterion(outputs.squeeze(), batch_y)
                loss.backward()
                optimizer.step()
                epoch_loss += loss.item()
            avg_loss = epoch_loss / len(dataloader)
            logging.info(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")

        torch.save(model.state_dict(), model_path)
        model.eval()
        logging.info(f"LNN model trained and saved to {model_path}")
        joblib.dump(scaler, scaler_path)
        logging.info(f"Scaler saved to {scaler_path}")
        return model
    except Exception as e:
        logging.error(f"Error during LNN training: {e}", exc_info=True)
        return None


async def prepare_lnn_training_data(gmn: CryptoGMN, history_length: int, scaler: MinMaxScaler):
    try:
        market_data = gmn.get_all_data()
        if not market_data or len(market_data['1m']['price']) < history_length + 1:
            logging.error("Not enough data to prepare training dataset.")
            return None, None

        X = []
        y = []
        for i in range(history_length, len(market_data['1m']['price']) - 1):
            features = []
            for timeframe in gmn.timeframes:
                for indicator in gmn.indicators:
                    data_series = market_data[timeframe].get(indicator)
                    if data_series and len(data_series) > i:
                        value = data_series[i]
                        if isinstance(value, dict):
                            features.extend(list(value.values()))
                        else:
                            features.append(value)
                    else:
                        features.append(0.0)

            future_price = market_data['1m']['price'][i + 1]
            current_price = market_data['1m']['price'][i]
            price_change = (future_price - current_price) / current_price
            y.append(1 if price_change > 0 else 0)
            X.append(features)

        X = np.array(X, dtype=np.float32)
        y = np.array(y, dtype=np.float32)
        X_scaled = scaler.fit_transform(X)  # Fit and transform inside the function
        return X_scaled, y

    except Exception as e:
        logging.error(f"Error preparing LNN training data: {e}", exc_info=True)
        return None, None


async def load_scaler(scaler_path: str) -> MinMaxScaler:
    """Loads a saved scaler or creates a new one."""
    try:
        scaler = joblib.load(scaler_path)
        logging.info(f"Loaded scaler from {scaler_path}")
        return scaler
    except FileNotFoundError:
        logging.warning(f"Scaler file not found at {scaler_path}. Initializing a new scaler.")
        return MinMaxScaler()


async def shutdown():
    """Shuts down the application gracefully."""
    global bot_tasks
    for task in bot_tasks:
        if task:
            task.cancel()
            try:
                await task
            except asyncio.CancelledError:
                pass
    logging.info("Shutdown complete.")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logging.info("Trading bot stopped by user. Shutting down...")
        asyncio.run(shutdown())

// File: data\config.py
from pydantic import BaseModel, Field, field_validator
from typing import List, Dict, Any
import yaml

class DataIngestionConfig(BaseModel):
    """Configuration for Data Ingestion and market data settings."""

    symbol: str = "BTC_USDT"
    interval: str = "Min1"
    timeframes: List[str] = ["1m", "5m", "15m", "1h", "4h"]
    indicators: List[str] = ["price", "volume", "rsi", "macd", "fibonacci"]

    # Private Channels (Optional)
    private_channels: List[str] = []

    # Batch Processing Parameters
    batch_size: int = Field(default=100, ge=1, description="Number of messages per batch to process.")
    batch_time_limit: float = Field(default=5.0, ge=0.1, description="Time limit (in seconds) for batching.")
    rate_limit: int = Field(default=100, ge=1, description="Max number of messages to send per second.")
    reconnect_delay: int = Field(default=5, ge=1, description="Delay (in seconds) between reconnect attempts.")
    max_reconnect_attempts: int = Field(default=10, ge=1, description="Maximum number of allowed reconnect attempts.")
    max_reconnect_delay: float = Field(default=300.0, ge=1.0, description="Maximum delay between reconnect attempts.")

    @field_validator('timeframes', mode='before')
    def validate_timeframes(cls, v):
        """Validates that 'timeframes' is either a comma-separated string or a list."""
        if isinstance(v, str):
            return [tf.strip() for tf in v.split(',')]
        elif isinstance(v, list):
            return v
        else:
            raise ValueError("timeframes must be either a comma-separated string or a list.")

    def load_from_yaml(self, config_path: str) -> None:
        """Loads configuration from a YAML file and updates the instance attributes."""
        try:
            with open(config_path, "r") as f:
                config_data: Dict[str, Any] = yaml.safe_load(f)
                for key, value in config_data.items():
                    if hasattr(self, key):
                        setattr(self, key, value)
        except FileNotFoundError as e:
            raise FileNotFoundError(f"Configuration file '{config_path}' not found: {e.filename}")
        except yaml.YAMLError as e:
            raise ValueError(f"Error parsing the YAML config file '{config_path}': {e}")


// File: data\data_processor.py
# File: data/data_processor.py

import logging
from typing import Dict, Any, List
from marshmallow import Schema, fields, validates, ValidationError
from indicator_calculations import IndicatorCalculator
from error_handler import ErrorHandler
from datetime import datetime

class DataSchema(Schema):
    """Schema to validate the incoming data structure."""
    data = fields.Dict(required=True)
    channel = fields.Str(required=True)
    symbol = fields.Str(required=False)  # Optional, default to 'unknown'

    @validates('data')
    def validate_data(self, value):
        if not isinstance(value, dict):
            raise ValidationError('Data must be a dictionary')
        if 'price' not in value or not isinstance(value['price'], (float, int)) or value['price'] <= 0:
            raise ValidationError('Invalid or missing price field in data')
        if 'volume' in value and not isinstance(value['volume'], (float, int)):
            raise ValidationError('Invalid volume field in data')


class DataProcessor:
    def __init__(self, gmn, indicator_calculator: IndicatorCalculator, error_handler: ErrorHandler):
        self.gmn = gmn
        self.indicator_calculator = indicator_calculator
        self.error_handler = error_handler
        self.logger = logging.getLogger(__name__)

    async def process_data(self, data_batch: List[Dict[str, Any]]) -> None:
        """Processes a batch of data messages."""
        for data in data_batch:
            try:
                # Validate incoming data using the schema
                schema = DataSchema()
                validated_data = schema.load(data)

                if 'kline' in validated_data['channel']:
                    await self._process_kline_data(validated_data)
                else:
                    self.logger.warning(f"Unhandled channel: {validated_data['channel']}")
            except ValidationError as e:
                self.error_handler.handle_error(f"Validation error in data: {e.messages}")
            except Exception as e:
                self.error_handler.handle_error(f"Error processing data: {e}", exc_info=True)

    async def _process_kline_data(self, data: Dict[str, Any]) -> None:
        """Processes kline (candlestick) data."""
        try:
            kline_data = data['data']
            symbol = data.get('symbol', 'unknown')

            # Validate kline_data structure
            if not isinstance(kline_data, dict):
                raise ValueError(f"Invalid kline data format: {kline_data}")

            # Add lineage and transform the data
            transformed_data = self._transform_data(self._add_data_lineage(kline_data))

            # Deduplicate data
            if self._is_duplicate(symbol, '1m', transformed_data):
                self.logger.info(f"Duplicate data for {symbol} at 1m detected, skipping.")
                return

            # Update the graph with the new kline data
            await self.gmn.update_graph([transformed_data])

            # Calculate indicators (ensure enough data is present)
            if self._enough_data_for_indicators(symbol, '1m'):
                indicators = self.indicator_calculator.calculate_indicators(symbol, '1m', transformed_data)
                self.gmn.store_indicators(symbol, '1m', indicators)

        except KeyError as e:
            self.error_handler.handle_error(f"KeyError in processing kline data: {e}")
        except ValueError as e:
            self.error_handler.handle_error(f"ValueError in processing kline data: {e}")
        except Exception as e:
            self.error_handler.handle_error(f"Unhandled error in kline processing: {e}", exc_info=True)

    def _transform_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Transforms the incoming data if necessary."""
        # Example transformation: round values
        data['price'] = round(data['price'], 2)
        if 'volume' in data:
            data['volume'] = round(data['volume'], 2)
        return data

    def _add_data_lineage(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Adds lineage metadata to the data."""
        data['source'] = 'mexc_websocket'
        data['processed_at'] = datetime.utcnow().isoformat()
        return data

    def _is_duplicate(self, symbol: str, timeframe: str, data: Dict[str, Any]) -> bool:
        """Check if the incoming data is a duplicate."""
        existing_data = self.gmn.get_data(symbol, timeframe)
        if existing_data and existing_data[-1]['price'] == data['price'] and existing_data[-1]['volume'] == data.get('volume'):
            return True
        return False

    def _enough_data_for_indicators(self, symbol: str, timeframe: str) -> bool:
        """Checks if there is enough data to calculate indicators."""
        data = self.gmn.get_data(symbol, timeframe)
        if len(data['price']) < 14:  # Example: 14 periods needed for indicators like RSI
            self.logger.info(f"Not enough data to calculate indicators for {symbol} at {timeframe}")
            return False
        return True


// File: data\error_handler.py
import logging
import traceback

# from .alerts import send_alert  # Uncomment if you want alert functionality

class ErrorHandler:
    def __init__(self, config=None):
        """
        Initializes the error handler.

        Args:
            config: Configuration settings, if needed for alerting or additional logging options.
        """
        self.logger = logging.getLogger(__name__)
        # self.config = config  # Uncomment if using config for alerting or other settings

    def handle_error(self, message: str, exc_info=False, alert=False):
        """
        Handles errors by logging the error and optionally sending alerts.

        Args:
            message (str): The error message to log.
            exc_info (bool): If True, includes the traceback in the log and prints it.
            alert (bool): If True, sends an alert notification (if implemented).
        """
        # 1. Log the error
        self.logger.error(message, exc_info=exc_info)  # Log the error message, with traceback if exc_info=True
        if exc_info:
            traceback.print_exc()  # Optionally print the full traceback to the console

        # 2. Optional: Send an alert (uncomment if needed)
        # if alert:  # Example alerting functionality (e.g., if config.alerts_enabled)
        #     try:
        #         send_alert(message)  # This would be your alerting function (e.g., email/SMS)
        #     except Exception as alert_error:
        #         self.logger.error(f"Failed to send alert: {alert_error}")


// File: data\indicator_calculations.py
import pandas as pd
import pandas_ta as ta
from error_handler import ErrorHandler

class IndicatorCalculator:
    def __init__(self, data_storage):
        self.data_storage = data_storage
        self.error_handler = ErrorHandler()  # Initialize the error handler

    def calculate_indicators(self, symbol: str, timeframe: str, data: pd.DataFrame):
        """
        Calculates technical indicators based on the provided data.

        Args:
            symbol (str): The trading symbol (e.g., 'BTCUSDT').
            timeframe (str): The timeframe (e.g., '1m', '5m').
            data (pd.DataFrame): The market data, containing 'close' prices.

        Returns:
            dict: A dictionary containing calculated indicators.
        """
        try:
            indicators = {}

            # Calculate RSI
            rsi_values = self.calculate_rsi(data['close'])
            indicators['rsi'] = rsi_values

            # Calculate MACD
            macd_values, macd_signal, macd_hist = self.calculate_macd(data['close'])
            indicators['macd'] = macd_values
            indicators['macd_signal'] = macd_signal
            indicators['macd_hist'] = macd_hist

            # Calculate Bollinger Bands
            bbands_values = self.calculate_bbands(data['close'])
            indicators['bbands'] = bbands_values

            # Calculate Fibonacci Retracement
            fib_values = self.calculate_fibonacci(data['close'])
            indicators['fibonacci'] = fib_values

            return indicators

        except KeyError as e:
            # Log specific key errors (e.g., missing columns in data)
            self.error_handler.handle_error(f"KeyError: Missing data column - {e}", exc_info=True)
        except Exception as e:
            # Handle all other exceptions
            self.error_handler.handle_error(f"Error calculating indicators: {e}", exc_info=True)

        # Return an empty dictionary if the calculation fails
        return {}

    def calculate_rsi(self, close_prices: pd.Series):
        """Calculates the RSI (Relative Strength Index)."""
        try:
            rsi_values = ta.rsi(close_prices, length=14)
            return rsi_values.tolist()
        except Exception as e:
            self.error_handler.handle_error(f"Error calculating RSI: {e}", exc_info=True)
            return []

    def calculate_macd(self, close_prices: pd.Series):
        """Calculates the MACD (Moving Average Convergence Divergence)."""
        try:
            macd = ta.macd(close_prices, fast=12, slow=26, signal=9)
            macd_values = macd['MACD_12_26_9'].tolist()
            macd_signal = macd['MACDs_12_26_9'].tolist()
            macd_hist = macd['MACDh_12_26_9'].tolist()
            return macd_values, macd_signal, macd_hist
        except KeyError as e:
            self.error_handler.handle_error(f"Error calculating MACD: Missing key - {e}", exc_info=True)
            return [], [], []
        except Exception as e:
            self.error_handler.handle_error(f"Error calculating MACD: {e}", exc_info=True)
            return [], [], []

    def calculate_bbands(self, close_prices: pd.Series):
        """Calculates Bollinger Bands."""
        try:
            bbands = ta.bbands(close_prices, length=20, std=2)
            return bbands['BBM_20_2'].tolist()
        except Exception as e:
            self.error_handler.handle_error(f"Error calculating Bollinger Bands: {e}", exc_info=True)
            return []

    def calculate_fibonacci(self, close_prices: pd.Series):
        """Calculates Fibonacci Retracement levels."""
        try:
            fib_values = []
            for i in range(len(close_prices)):
                if i < 14:
                    fib_values.append(float('nan'))
                else:
                    recent_prices = close_prices.iloc[i - 14:i]
                    high = max(recent_prices)
                    low = min(recent_prices)
                    close = recent_prices.iloc[-1]
                    diff = high - low
                    fib_values.append((close - low) / diff if diff != 0 else float('nan'))
            return fib_values
        except Exception as e:
            self.error_handler.handle_error(f"Error calculating Fibonacci Retracement: {e}", exc_info=True)
            return []


// File: data\mexc_websocket_connector.py
import asyncio
import json
import logging
import os
import time
from typing import Dict, Any, List
from asyncio import Queue
from dotenv import load_dotenv
import websockets
import requests

from .config import Config
from .websocket_manager import WebSocketManager  # Corrected import

load_dotenv()

RECONNECT_JITTER = 0.2  # 20% jitter


class MexcRestAPI:
    """Handles REST API interactions with MEXC for listenKey management."""

    def __init__(self, api_key: str, api_secret: str):
        self.base_url = "https://api.mexc.com"
        self.api_key = api_key
        self.api_secret = api_secret

    def get_listen_key(self) -> str:
        """Fetch the listenKey from MEXC API to start the WebSocket connection."""
        url = f"{self.base_url}/api/v3/userDataStream"
        headers = {
            'X-MEXC-APIKEY': self.api_key,
        }
        response = requests.post(url, headers=headers)
        if response.status_code == 200:
            listen_key = response.json().get('listenKey')
            if listen_key:
                logging.info("Successfully obtained listenKey.")
                return listen_key
            else:
                raise Exception("Failed to obtain listenKey: No listenKey in response.")
        else:
            raise Exception(f"Failed to get listenKey: {response.text}")

    def refresh_listen_key(self, listen_key: str):
        """Refresh the listenKey before it expires (every 60 minutes)."""
        url = f"{self.base_url}/api/v3/userDataStream"
        headers = {
            'X-MEXC-APIKEY': self.api_key,
        }
        params = {"listenKey": listen_key}
        response = requests.put(url, headers=headers, params=params)
        if response.status_code == 200:
            logging.info("Successfully refreshed listenKey.")
        else:
            raise Exception(f"Failed to refresh listenKey: {response.text}")


class MexcWebsocketConnector:
    """
    Connects to the MEXC WebSocket API, handles subscriptions,
    and passes raw data to a processing queue.
    """

    def __init__(self, config: Config, data_queue: Queue):
        self.config = config
        self.data_queue = data_queue
        self.ws_url = os.getenv("MEXC_WS_URL", "wss://wbs.mexc.com/ws")
        self.api_key = os.getenv("MEXC_API_KEY")
        self.api_secret = os.getenv("MEXC_API_SECRET")
        self.websocket_manager = WebSocketManager(
            self.ws_url, self.api_key, self.api_secret, config.rate_limit, config=config
        )

        self.reconnect_attempts = 0
        self._last_reconnect_time = 0
        self.connected_event = asyncio.Event()

        # Initialize REST API client
        self.rest_api = MexcRestAPI(self.api_key, self.api_secret)
        self.listen_key = None
        self.listen_key_task = None

    async def connect(self) -> None:
        """Connects to the WebSocket and handles reconnections with backoff."""
        try:
            # Get the initial listenKey
            self.listen_key = self.rest_api.get_listen_key()

            # Start the listenKey refresh task
            self.listen_key_task = asyncio.create_task(self._refresh_listen_key_periodically())

            while self.reconnect_attempts < self.config.max_reconnect_attempts:
                try:
                    await self._connect_and_receive()
                except websockets.exceptions.ConnectionClosedError as e:
                    logging.error(f"WebSocket connection error: {e}")
                except Exception as e:
                    logging.exception(f"An unexpected error occurred: {e}")
                finally:
                    self.connected_event.clear()

                await self._handle_reconnection_delay()

        except Exception as e:
            logging.exception(f"Failed to establish WebSocket connection: {e}")

    async def _connect_and_receive(self):
        """Handles the WebSocket connection and data reception."""
        connection_url = f"{self.ws_url}?listenKey={self.listen_key}"
        async with websockets.connect(connection_url, ping_interval=20, ping_timeout=20) as ws:
            self.connected_event.set()
            logging.info("WebSocket connection established.")

            # Subscribe to multiple timeframes
            subscribe_message = {
                "method": "SUBSCRIBE",
                "params": [f"spot@public.kline.v3.api@BTCUSDT@{tf}" for tf in self.config.timeframes],
                "id": 1
            }
            await self.websocket_manager.send_message(json.dumps(subscribe_message), ws)
            logging.info(f"Subscribed to timeframes: {self.config.timeframes}")

            # Start receiving data
            await self._receive_data_loop(ws)

    async def _receive_data_loop(self, ws) -> None:
        """Receives data from WebSocket and processes it in batches."""
        try:
            data_batch = []
            batch_size = self.config.batch_size  # Ensure Config has batch_size
            batch_time_limit = self.config.batch_time_limit  # Ensure Config has batch_time_limit
            batch_start_time = time.time()

            while True:
                try:
                    message = await ws.recv()
                    data = json.loads(message)
                    data_batch.append(data)

                    # Check if batch size or time limit is reached
                    current_time = time.time()
                    if len(data_batch) >= batch_size or (current_time - batch_start_time) >= batch_time_limit:
                        if data_batch:
                            await self.data_queue.put(data_batch.copy())
                            logging.debug(f"Batch of {len(data_batch)} messages put into queue.")
                            data_batch.clear()
                            batch_start_time = current_time  # Reset timer after each batch

                except asyncio.CancelledError:
                    logging.info("Receive data loop cancelled.")
                    break
                except websockets.exceptions.ConnectionClosedOK as e:
                    logging.warning(f"WebSocket closed gracefully: {e.reason}")
                    break
                except websockets.exceptions.ConnectionClosedError as e:
                    logging.error(f"WebSocket closed with error: {e.reason}")
                    break
                except Exception as e:
                    logging.exception(f"Error in receive_data_loop: {e}")
                    break

            # Put remaining data in queue
            if data_batch:
                await self.data_queue.put(data_batch.copy())
                logging.debug(f"Final batch of {len(data_batch)} messages put into queue.")

        except Exception as e:
            logging.exception(f"Exception in _receive_data_loop: {e}")

    async def _refresh_listen_key_periodically(self):
        """Periodically refreshes the listenKey before it expires."""
        try:
            while True:
                await asyncio.sleep(30 * 60)  # Refresh every 30 minutes
                try:
                    self.rest_api.refresh_listen_key(self.listen_key)
                except Exception as e:
                    logging.error(f"Error refreshing listenKey: {e}")
        except asyncio.CancelledError:
            logging.info("ListenKey refresh task cancelled.")

    async def _handle_reconnection_delay(self) -> None:
        """Handles delay between reconnection attempts with exponential backoff and jitter."""
        current_time = time.monotonic()
        backoff_delay = self.config.reconnect_delay * (
            self.config.backoff_factor ** self.reconnect_attempts
        )
        jitter = backoff_delay * RECONNECT_JITTER
        wait_time = min(backoff_delay + jitter, self.config.max_reconnect_delay)
        next_reconnect = self._last_reconnect_time + wait_time
        sleep_duration = max(0, next_reconnect - current_time)
        logging.info(
            f"Next reconnection attempt in {sleep_duration:.2f} seconds..."
        )
        await asyncio.sleep(sleep_duration)
        self._last_reconnect_time = time.monotonic()
        self.reconnect_attempts += 1

    async def close(self) -> None:
        """Closes the WebSocket connection and cancels background tasks."""
        if self.listen_key_task:
            self.listen_key_task.cancel()
            try:
                await self.listen_key_task
            except asyncio.CancelledError:
                logging.info("ListenKey refresh task cancelled.")

        await self.websocket_manager.close()
        logging.info("WebSocket connection closed.")


// File: data\websocket_manager.py
import asyncio
import logging
import websockets

class WebSocketManager:
    """Manages the WebSocket connection."""

    def __init__(self, ws_url: str, api_key: str, api_secret: str, rate_limit: int, config=None):
        self.ws_url = ws_url
        self.api_key = api_key
        self.api_secret = api_secret
        self.rate_limit = rate_limit
        self.config = config
        self.ws = None  # The websocket connection object
        self.logger = logging.getLogger(__name__)

    async def __aenter__(self):
        """Establishes the WebSocket connection."""
        # The connector will handle setting the listen key in the ws_url now.
        self.ws = await websockets.connect(self.ws_url)  
        return self.ws

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Closes the WebSocket connection."""
        await self.close()

    async def send_message(self, message: str, ws=None):
        """Sends a message over the WebSocket connection."""
        try:
            if ws:
                await ws.send(message)
            elif self.ws:
                await self.ws.send(message)  # Use self.ws if no ws argument is provided
            else:
                raise ValueError("No active WebSocket connection.")

        except websockets.exceptions.ConnectionClosed:
            self.logger.error("WebSocket connection closed when sending message.")
            raise  # Re-raise the exception to be handled by the connector

        except Exception as e:
            self.logger.exception(f"Error sending message: {e}")
            raise

    async def receive_message(self, ws):
        """Receives a message from the WebSocket."""
        try:
            message = await ws.recv()
            return message
        except websockets.exceptions.ConnectionClosed:
            self.logger.error("WebSocket connection closed when receiving message.")
            raise  # Re-raise the exception

        except Exception as e:
            self.logger.exception(f"Error receiving message: {e}")
            raise

    async def close(self):
        """Closes the WebSocket connection."""
        if self.ws and not self.ws.closed:
            await self.ws.close()
            self.ws = None  # Reset the websocket object
            self.logger.info("WebSocket connection closed.")


// File: data\__init__.py
from .config import Config
from .mexc_websocket_connector import MexcWebsocketConnector
from .data_processor import DataProcessor
from .websocket_manager import WebSocketManager

__all__ = ['Config', 'MexcWebsocketConnector', 'DataProcessor', 'WebSocketManager']


// File: data\storage\data_storage.py
# data_storage.py
import pandas as pd

class DataStorage:
    def __init__(self):
        self.data = {}

    def store_data(self, symbol: str, timeframe: str, data: pd.DataFrame):
        if symbol not in self.data:
            self.data[symbol] = {}
        self.data[symbol][timeframe] = data
        self.save_to_file(symbol, timeframe, data)

    def get_data(self, symbol: str, timeframe: str) -> pd.DataFrame:
        return self.data.get(symbol, {}).get(timeframe)

    def save_to_file(self, symbol: str, timeframe: str, data: pd.DataFrame):
        filename = f"{symbol}_{timeframe}.csv"
        data.to_csv(filename, index=False)
        print(f"Data saved to {filename}")

// File: models\__init__.py


// File: models\agents\agent.py
# File: models/agents/agent.py

import logging
import numpy as np
import torch
from sklearn.preprocessing import MinMaxScaler
from models.utils.config import Config
from models.utils.risk_management import RiskManager
import os
import aiohttp  # Ensure aiohttp is imported
import hashlib
import hmac
import time

class TradingAgent:
    def __init__(
        self, 
        timeframes, 
        indicators, 
        model, 
        config: Config, 
        risk_manager: RiskManager, 
        scaler: MinMaxScaler
    ):
        self.timeframes = timeframes
        self.indicators = indicators
        self.model = model
        self.config = config
        self.threshold_buy = config.threshold_buy
        self.threshold_sell = config.threshold_sell
        self.risk_manager = risk_manager
        self.trade_parameters = config.trade_parameters
        self.leverage = self.trade_parameters.get("leverage", 20)
        self.position = None
        self.scaler = scaler
        self.api_key = os.getenv("MEXC_API_KEY")
        self.api_secret = os.getenv("MEXC_API_SECRET")
        self.base_url = 'https://contract.mexc.com/api/v1/'  # Update if necessary

        self.peak_portfolio_value = 1.0  # Initialize for drawdown calculations
        self.portfolio_value = 1.0  # Initialize portfolio value

        # Initialize aiohttp session
        self.session = aiohttp.ClientSession()

    async def make_decision(self, market_data):
        """Processes market data, makes predictions, and executes trades based on the model's output."""
        try:
            input_vector = self._prepare_input(market_data)
            if input_vector is None:
                logging.warning("Input vector is None. Skipping decision.")
                return

            # Model expects input in float32
            input_tensor = torch.tensor([input_vector], dtype=torch.float32)

            with torch.no_grad():
                self.model.eval()
                prediction = self.model(input_tensor)
                prediction = torch.sigmoid(prediction)  # Apply sigmoid to get probability
                prediction_value = prediction.item()

            logging.info(f"Model Prediction Probability: {prediction_value:.4f}")

            current_drawdown = self.calculate_current_drawdown()

            if self.risk_manager.check_risk(current_drawdown, self.position, market_data):
                if prediction_value >= self.threshold_buy and self.position != 'long':
                    await self._execute_trade('buy')
                    self.position = 'long'
                elif prediction_value <= self.threshold_sell and self.position != 'short':
                    await self._execute_trade('sell')
                    self.position = 'short'
                elif self.threshold_sell < prediction_value < self.threshold_buy:
                    if self.position is not None:
                        await self._execute_trade('close')
                        self.position = None
            else:
                logging.warning("Risk management check failed. Not executing trade.")

        except Exception as e:
            logging.error(f"Error in make_decision: {e}")

    def _prepare_input(self, market_data):
        """Prepares and scales the input vector for the LNN model."""
        input_vector = []
        for timeframe in self.timeframes:
            for indicator in self.indicators:
                data = market_data.get(timeframe, {}).get(indicator)
                if data is None or len(data) == 0:
                    logging.warning(f"Missing data for {timeframe} {indicator}. Skipping.")
                    return None

                if isinstance(data[-1], dict):
                    values = [v for v in data[-1].values() if isinstance(v, (int, float))]
                    input_vector.extend(values)
                else:
                    input_vector.append(data[-1])

        if not input_vector:
            logging.warning("Input vector is empty. No data available for making a decision.")
            return None

        input_vector = np.array([input_vector], dtype=np.float32)
        input_vector = self.scaler.transform(input_vector).astype(np.float32)
        return input_vector.flatten()

    async def _execute_trade(self, action, symbol="BTC_USDT"):
        """Executes a trade action (buy, sell, close) via the MEXC API using aiohttp."""
        try:
            timestamp = int(time.time() * 1000)
            params = {
                "symbol": symbol,
                "timestamp": timestamp
            }

            # Determine trade parameters based on action
            if action == 'buy':
                side = 'OPEN_LONG'
                quantity = self.trade_parameters.get("volume", 1)
            elif action == 'sell':
                side = 'OPEN_SHORT'
                quantity = self.trade_parameters.get("volume", 1)
            elif action == 'close':
                if self.position == 'long':
                    side = 'CLOSE_LONG'
                elif self.position == 'short':
                    side = 'CLOSE_SHORT'
                else:
                    logging.warning("No position to close.")
                    return
                quantity = self.trade_parameters.get("volume", 1)
            else:
                logging.warning(f"Invalid trade action: {action}")
                return

            # Set additional parameters
            params.update({
                "price": '',  # Empty for market orders
                "vol": quantity,
                "side": side,
                "type": self.trade_parameters.get("order_type", 1),  # 1: Market order
                "leverage": self.leverage,
                "openType": self.trade_parameters.get("open_type", 1),  # 1: Isolated margin
            })

            # Generate signature
            query_string = '&'.join([f"{key}={params[key]}" for key in sorted(params)])
            signature = hmac.new(
                self.api_secret.encode('utf-8'),
                query_string.encode('utf-8'),
                hashlib.sha256
            ).hexdigest()
            params['sign'] = signature

            # Send request using aiohttp
            url = self.base_url + 'order/submit'
            async with self.session.post(url, params=params) as response:
                response_data = await response.json()
                if response.status == 200 and response_data.get("success", False):
                    logging.info(f"Successfully executed {action} order: {response_data}")
                    # Update portfolio value based on trade execution
                    self.update_portfolio(action, response_data)
                else:
                    logging.error(f"Failed to execute {action} order: {response_data}")

        except Exception as e:
            logging.error(f"Error executing trade: {e}")

    def calculate_current_drawdown(self):
        """Calculates the current drawdown based on portfolio value."""
        self.peak_portfolio_value = max(self.peak_portfolio_value, self.portfolio_value)
        drawdown = (self.peak_portfolio_value - self.portfolio_value) / self.peak_portfolio_value
        logging.info(f"Current Drawdown: {drawdown:.4f}")
        return drawdown

    def update_portfolio(self, action: str, response_data: Dict):
        """
        Updates the portfolio value based on the executed trade.
        This is a placeholder function. You need to implement actual portfolio management logic.
        """
        # Example: Update portfolio based on the price and quantity
        try:
            price = float(response_data.get('data', {}).get('price', self.portfolio_value))
            quantity = float(response_data.get('data', {}).get('vol', 0))
            if action == 'buy':
                # Example logic: Increase portfolio value
                self.portfolio_value += price * quantity
            elif action == 'sell':
                # Example logic: Decrease portfolio value
                self.portfolio_value -= price * quantity
            elif action == 'close':
                # Example logic: Neutralize position
                pass
            logging.info(f"Portfolio updated after {action}: {self.portfolio_value}")
        except Exception as e:
            logging.error(f"Error updating portfolio: {e}")

    async def close(self):
        """Closes the aiohttp session."""
        await self.session.close()

// File: models\agents\rl_agent.py
# models/agents/rl_agent.py

import gym
import numpy as np
from stable_baselines3 import PPO

class TradingEnvironment(gym.Env):
    def __init__(self, market_data):
        super(TradingEnvironment, self).__init__()
        self.market_data = market_data
        self.current_step = 0
        self.action_space = gym.spaces.Discrete(3)  # Buy, Sell, Hold
        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(OBSERVATION_SPACE_SIZE,), dtype=np.float32)

    def reset(self):
        self.current_step = 0
        return self._next_observation()

    def _next_observation(self):
        # Return the next market observation
        obs = self.market_data.iloc[self.current_step]
        return obs.values

    def step(self, action):
        # Execute one time step within the environment
        self.current_step += 1

        reward = self._calculate_reward(action)
        done = self.current_step >= len(self.market_data) - 1
        obs = self._next_observation() if not done else np.zeros(self.observation_space.shape)

        return obs, reward, done, {}

    def _calculate_reward(self, action):
        # Implement reward calculation
        return reward

def train_rl_agent():
    # Load market data
    market_data = pd.read_csv("data/distilled/distilled_data.csv")
    env = TradingEnvironment(market_data)
    model = PPO("MlpPolicy", env, verbose=1)
    model.learn(total_timesteps=10000)
    model.save("models/agents/ppo_agent")

if __name__ == "__main__":
    train_rl_agent()


// File: models\agents\__init__.py


// File: models\gmn\config.py
import os
from pydantic import BaseModel, Field, validator
from typing import List

class CryptoGMNConfig(BaseModel):
    timeframes: List[str] = ["1m", "5m"]
    indicators: List[str] = ["price", "volume", "rsi", "macd", "fibonacci"]
    max_history: int = 1000
    executor_workers: int = 5
    cache_size: int = 100
    db_path: str = "market_data.db"
    performance_threshold: float = 0.1
    
    @validator('timeframes', 'indicators', pre=True)
    def split_comma_separated(cls, v):
        if isinstance(v, str):
            return [item.strip() for item in v.split(",")]
        return v

def load_config():
    env = os.getenv("ENVIRONMENT", "DEFAULT").upper()
    config_file = f"config_{env.lower()}.ini" if env != "DEFAULT" else "config.ini"
    
    # Here you would typically load from the config file
    # For simplicity, we're using default values
    return CryptoGMNConfig()

// File: models\gmn\crypto_gmn.py
import asyncio
from collections import deque
from concurrent.futures import ThreadPoolExecutor
import logging
from typing import Any, Dict, List, Optional, Union
import numpy as np
from pydantic import BaseModel, ValidationError

logger = logging.getLogger(__name__)

class CryptoGMNError(Exception):
    """Base exception for CryptoGMN."""
    pass

class DataValidationError(CryptoGMNError):
    """Exception raised when data validation fails."""
    pass

class DataModel(BaseModel):
    c: float
    v: Optional[float] = 0.0

    class Config:
        arbitrary_types_allowed = True

class CryptoGMN:
    def __init__(self, timeframes: List[str], max_history_length: int, executor_workers: int):
        self.timeframes = timeframes
        self.max_history_length = max_history_length
        self.market_data: Dict[str, Dict[str, deque]] = {
            timeframe: {'price': deque(maxlen=max_history_length), 'volume': deque(maxlen=max_history_length)}
            for timeframe in timeframes
        }
        self.locks: Dict[str, asyncio.Lock] = {timeframe: asyncio.Lock() for timeframe in timeframes}
        self._executor_lock = asyncio.Lock()
        self._executor: Optional[ThreadPoolExecutor] = None
        self.executor_workers = executor_workers

    async def __aenter__(self):
        async with self._executor_lock:
            if self._executor is None:
                self._executor = ThreadPoolExecutor(max_workers=self.executor_workers)
                logger.info(f"ThreadPoolExecutor initialized with {self.executor_workers} workers.")
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.shutdown()

    async def update_graph(self, new_data_items: List[Dict[str, Any]]) -> None:
        try:
            await asyncio.gather(*(self._update_single_data(data) for data in new_data_items))
        except Exception as e:
            logger.error(f"Error updating graph: {e}", exc_info=True)
            raise CryptoGMNError("Failed to update graph") from e

    async def _update_single_data(self, new_data: Dict[str, Any]) -> None:
        try:
            validated_data = DataModel(**new_data)
            price = validated_data.c
            volume = validated_data.v
        except ValidationError as e:
            logger.error(f"Data validation error: {e}. Data: {new_data}")
            raise DataValidationError(f"Invalid data format: {e}") from e

        try:
            await asyncio.gather(*(self._update_timeframe(tf, price, volume) for tf in self.timeframes))
        except Exception as e:
            logger.error(f"Error updating timeframes: {e}", exc_info=True)
            raise CryptoGMNError("Failed to update timeframes") from e

    async def _update_timeframe(self, timeframe: str, price: float, volume: float) -> None:
        async with self.locks[timeframe]:
            try:
                data = self.market_data[timeframe]
                data['price'].append(price)
                data['volume'].append(volume)
            except Exception as e:
                logger.error(f"Error updating timeframe {timeframe}: {e}", exc_info=True)
                raise CryptoGMNError(f"Failed to update timeframe {timeframe}") from e

    def get_data(self, timeframe: str, data_type: str) -> Optional[List[Union[float]]]:
        try:
            return list(self.market_data[timeframe][data_type])
        except KeyError:
            logger.warning(f"No data found for timeframe '{timeframe}' and data type '{data_type}'.")
            return None

    def get_all_data(self) -> Dict[str, Dict[str, List[Union[float]]]]:
        return {
            timeframe: {
                data_type: list(data)
                for data_type, data in indicators.items()
            }
            for timeframe, indicators in self.market_data.items()
        }

    async def shutdown(self) -> None:
        if self._executor:
            self._executor.shutdown(wait=True)
            self._executor = None
        logger.info("CryptoGMN has been shut down gracefully.")

// File: models\gmn\data_model.py
from pydantic import BaseModel, validator
from typing import Optional

class DataModel(BaseModel):
    c: float  # Close price
    v: Optional[float] = 0.0  # Volume

    @validator('c', 'v', pre=True)
    def validate_numeric(cls, v):
        if isinstance(v, (int, float)):
            return float(v)
        try:
            return float(v)
        except (ValueError, TypeError):
            raise ValueError(f"Value '{v}' is not a valid float.")

// File: models\gmn\gmn.py
import asyncio
from collections import deque
from concurrent.futures import ThreadPoolExecutor
from typing import Any, Dict, List, Optional, Union, Callable, cast
import numpy as np
import talib  # type: ignore  # Suppress type errors for TA-Lib
import logging
from configparser import ConfigParser
from pydantic import BaseModel, ValidationError, validator

logger = logging.getLogger(__name__)

# Load configuration from external file (config.ini)
config = ConfigParser()
config.read("config.ini")  # Ensure a config.ini file exists in the same directory

# Default configuration values
DEFAULT_TIMEFRAMES = config.get("DEFAULT", "timeframes", fallback="1m,5m").split(",")  # Example: ['1m', '5m']
DEFAULT_INDICATORS = config.get("DEFAULT", "indicators", fallback="price,volume,rsi,macd,fibonacci").split(",")  # Example: ['price', 'volume', 'rsi', 'macd', 'fibonacci']
MAX_HISTORY = config.getint("DEFAULT", "max_history", fallback=1000)
EXECUTOR_WORKERS = config.getint("DEFAULT", "executor_workers", fallback=5)


class CryptoGMNError(Exception):
    """Base exception for CryptoGMN."""
    pass


class IndicatorCalculationError(CryptoGMNError):
    """Exception raised when an indicator calculation fails."""
    pass


class DataModel(BaseModel):
    """Pydantic model for validating incoming data."""
    c: float  # Close price
    v: Optional[float] = 0.0  # Volume

    @validator('c', 'v', pre=True)
    def validate_numeric(cls, v):
        if isinstance(v, (int, float)):
            return float(v)
        try:
            return float(v)
        except (ValueError, TypeError):
            raise ValueError(f"Value '{v}' is not a valid float.")


class CryptoGMN:
    """Manages cryptocurrency market data and technical indicators."""

    def __init__(self, timeframes: List[str] = DEFAULT_TIMEFRAMES,  # Default values
                 indicators: List[str] = DEFAULT_INDICATORS,
                 max_history_length: int = MAX_HISTORY,
                 executor_workers: int = EXECUTOR_WORKERS) -> None:
        """
        Initializes the CryptoGMN instance.

        :param timeframes: List of timeframes (e.g., ['1m', '5m', '1h'])
        :param indicators: List of indicators (e.g., ['price', 'volume', 'rsi', 'macd', 'fibonacci'])
        :param max_history_length: Maximum number of data points to store per indicator
        :param executor_workers: Number of worker threads for indicator calculations
        """
        self.timeframes = timeframes
        self.indicators = indicators
        self.max_history_length = max_history_length
        self.market_data: Dict[str, Dict[str, deque]] = {
            timeframe: {indicator: deque(maxlen=max_history_length) for indicator in indicators}
            for timeframe in timeframes
        }
        self.locks: Dict[str, asyncio.Lock] = {timeframe: asyncio.Lock() for timeframe in timeframes}
        self.executor: ThreadPoolExecutor  # Explicitly type hint
        # Asynchronous context manager for Executor
        self._executor_lock = asyncio.Lock()
        self._executor: Optional[ThreadPoolExecutor] = None

    async def __aenter__(self):
        async with self._executor_lock:
            if self._executor is None:
                self._executor = ThreadPoolExecutor(max_workers=self.executor_workers)  # Use the config value here
            self.executor = cast(ThreadPoolExecutor, self._executor)  # Cast to make mypy happy.
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.shutdown()

    async def update_graph(self, new_data_items: List[Dict[str, Any]]) -> None:
        """Updates market data with new items concurrently."""
        await asyncio.gather(*(self._update_single_data(data) for data in new_data_items))  # More concise

    async def _update_single_data(self, new_data: Dict[str, Any]) -> None:
        """Updates data for a single new item across all timeframes."""
        try:
            validated_data = DataModel(**new_data)
            price = validated_data.c
            volume = validated_data.v
        except ValidationError as e:
            logger.error(f"Data validation error: {e}. Data: {new_data}")
            return

        await asyncio.gather(*(self._update_timeframe(tf, price, volume) for tf in self.timeframes))

    async def _update_timeframe(self, timeframe: str, price: float, volume: float) -> None:
        """Updates data for a specific timeframe, including batching for high-frequency data."""
        async with self.locks[timeframe]:
            data = self.market_data[timeframe]
            data['price'].append(price)
            data['volume'].append(volume)

            prices_array = np.array(data['price'], dtype=np.float64)

            if len(prices_array) >= 14:
                await self._calculate_indicators(timeframe, prices_array)

    async def _calculate_indicators(self, timeframe: str, prices_array: np.ndarray):
        """Calculates all indicators for a timeframe concurrently."""
        tasks = []
        if 'rsi' in self.indicators:
            tasks.append(self._calculate_and_append(timeframe, 'rsi', talib.RSI, prices_array, {'timeperiod': 14}))
        if 'macd' in self.indicators:
            tasks.append(self._calculate_and_append(
                timeframe, 'macd',
                lambda prices: self._macd_wrapper(prices),  # Lambda for MACD extraction
                prices_array
            ))
        if 'fibonacci' in self.indicators:
            tasks.append(self._calculate_and_append_fibonacci(timeframe, prices_array))
        # Add other indicators similarly if needed
        if tasks:
            await asyncio.gather(*tasks)

    async def _calculate_and_append(self, timeframe: str, indicator: str,
                                    indicator_func: Callable, prices: np.ndarray,
                                    kwargs: Dict[str, Any] = {}) -> None:
        """
        Calculates an indicator and appends the result to the market data.

        :param timeframe: The timeframe for which to calculate the indicator
        :param indicator: The name of the indicator
        :param indicator_func: The TA-Lib function to calculate the indicator
        :param prices: NumPy array of prices
        :param kwargs: Additional keyword arguments for the indicator function
        """
        loop = asyncio.get_running_loop()
        try:
            result = await loop.run_in_executor(
                self.executor, indicator_func, prices, **kwargs  # Simplify executor call
            )
        except Exception as e:
            logger.error(f"Error running indicator '{indicator}' for timeframe '{timeframe}': {e}")
            return

        if result is not None:
            try:
                latest = result[-1]
                if isinstance(latest, np.ndarray):
                    latest = latest.item()  # Convert numpy scalar to Python float
                if np.isnan(latest):
                    logger.warning(f"{indicator.upper()} returned NaN or empty result for timeframe {timeframe}")
                    return
                self.market_data[timeframe][indicator].append(float(latest))
            except IndexError:
                logger.warning(f"{indicator.upper()} returned NaN or empty result for timeframe {timeframe}")

    async def _calculate_and_append_fibonacci(self, timeframe: str, prices: np.ndarray, lookback: int = 14) -> None:
        """Calculates and appends Fibonacci levels."""
        loop = asyncio.get_running_loop()
        try:
            fibonacci_levels = await loop.run_in_executor(
                self.executor, self._calculate_fibonacci, prices, lookback
            )
        except Exception as e:
            logger.error(f"Error calculating Fibonacci for timeframe '{timeframe}': {e}")
            return

        if fibonacci_levels:
            self.market_data[timeframe]['fibonacci'].append(fibonacci_levels)

    def _macd_wrapper(self, prices: np.ndarray, fastperiod: int = 12, slowperiod: int = 26, signalperiod: int = 9) -> Optional[float]:
        """Wrapper for talib.MACD to return only the MACD line."""
        try:
            macd, _, _ = talib.MACD(prices, fastperiod=fastperiod, slowperiod=slowperiod, signalperiod=signalperiod)
            if macd is None or len(macd) == 0:
                logger.warning(f"MACD calculation returned no data for timeframe.")
                return None
            latest_macd = macd[-1]
            if np.isnan(latest_macd):
                logger.warning(f"MACD returned NaN for timeframe.")
                return None
            return float(latest_macd)
        except Exception as e:
            logger.error(f"Error in _macd_wrapper: {e}")
            return None

    def _calculate_fibonacci(self, prices: np.ndarray, lookback: int) -> Optional[Dict[str, Union[float, bool]]]:
        """Calculates Fibonacci retracement levels and indicates the closest level."""
        try:
            if len(prices) < lookback:
                logger.warning("Not enough data to calculate Fibonacci retracement.")
                return None
            recent_prices = prices[-lookback:]
            high = np.max(recent_prices)
            low = np.min(recent_prices)
            close = recent_prices[-1]
            diff = high - low
            if diff == 0:
                logger.warning("High and low prices are the same; cannot calculate Fibonacci levels.")
                return None  # Avoid division by zero

            levels = {
                "23.6%": high - 0.236 * diff,
                "38.2%": high - 0.382 * diff,
                "50%": high - 0.5 * diff,
                "61.8%": high - 0.618 * diff,
                "78.6%": high - 0.786 * diff,
                "100%": low,
            }
            closest_level_key = min(
                levels.keys(), key=lambda k: abs(levels[k] - close)
            )
            for k in levels:
                levels[k] = {"value": levels[k], "is_closest": k == closest_level_key}  # Indicate closest

            return levels
        except Exception as e:
            logger.error(f"Error calculating Fibonacci retracement: {e}")
            return None

    def get_data(self, timeframe: str, indicator: str) -> Optional[List[Union[float, Dict[str, Union[float, bool]]]]]:
        """Retrieves the latest data for a specific timeframe and indicator."""
        try:
            return list(self.market_data[timeframe][indicator])
        except KeyError:
            logger.warning(
                f"No data found for timeframe '{timeframe}' and indicator '{indicator}'."
            )
            return None

    def get_all_data(self) -> Dict[str, Dict[str, List[Union[float, Dict[str, Union[float, bool]]]]]]:
        """Retrieves all market data across all timeframes and indicators."""
        return {
            timeframe: {
                indicator: list(data)
                for indicator, data in indicators.items()
            }
            for timeframe, indicators in self.market_data.items()
        }

    async def shutdown(self) -> None:
        """Shuts down the executor gracefully."""
        if self._executor:
            self._executor.shutdown(wait=True)
            self._executor = None  # Important for proper cleanup
        logger.info("CryptoGMN has been shut down gracefully.")


// File: models\gmn\logger.py
import logging

class CryptoGMNLogger:
    def __init__(self, name: str, level: int = logging.INFO):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(level)
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        
        file_handler = logging.FileHandler('cryptogmn.log')
        file_handler.setFormatter(formatter)
        self.logger.addHandler(file_handler)
        
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        self.logger.addHandler(console_handler)

logger = CryptoGMNLogger("CryptoGMN").logger

// File: models\gmn\main.py
import asyncio
import logging
import os
from crypto_gmn import CryptoGMN
from config import load_config

async def main():
    # Optionally set the environment variable before running
    # os.environ["ENVIRONMENT"] = "PRODUCTION"

    config = load_config()
    
    async with CryptoGMN(config) as gmn:
        # Example batch of new data points
        new_data = [
            {'c': 150.0, 'v': 1000},
            {'c': 155.0, 'v': 1200},
            {'c': 160.0, 'v': 1500},
            {'c': 158.0, 'v': 1100},
            {'c': 162.0, 'v': 1300},
        ]

        # Update the graph with new data
        await gmn.update_graph(new_data)

        # Retrieve specific indicator data
        rsi_data = gmn.get_data('1m', 'rsi')
        macd_data = gmn.get_data('1m', 'macd')
        fibonacci_data = gmn.get_data('1m', 'fibonacci')

        print(f"RSI Data for 1m: {rsi_data}")
        print(f"MACD Data for 1m: {macd_data}")
        print(f"Fibonacci Data for 1m: {fibonacci_data}")

        # Retrieve all market data
        all_data = gmn.get_all_data()
        print(f"All Market Data: {all_data}")

        # Dynamically add a new indicator (e.g., EMA)
        ema_func = gmn.indicator_factory.create_ema(timeperiod=50)
        gmn.add_indicator('1m', 'ema', ema_func)

        # Update with new data to trigger EMA calculation
        new_data = [
            {'c': 165.0, 'v': 1400},
            {'c': 170.0, 'v': 1600},
        ]
        await gmn.update_graph(new_data)

        ema_data = gmn.get_data('1m', 'ema')
        print(f"EMA Data for 1m: {ema_data}")

        # Fetch real-time data from Binance
        await gmn.fetch_real_time_data(exchange='binance', symbol='BTCUSDT')

        # Access cached indicator data
        cached_rsi = gmn.get_cached_indicator('1m', 'rsi', window=10)
        print(f"Cached RSI Data for 1m (last 10): {cached_rsi}")

if __name__ == "__main__":
    asyncio.run(main())

// File: models\gmn\performance_monitor.py
from collections import deque

class PerformanceMonitor:
    def __init__(self, window_size: int = 100):
        self.processing_times = deque(maxlen=window_size)

    def record(self, start_time: float, end_time: float):
        self.processing_times.append(end_time - start_time)

    @property
    def average_processing_time(self):
        return sum(self.processing_times) / len(self.processing_times) if self.processing_times else 0

// File: models\gmn\utils.py
import functools
from logger import logger

def retry(exceptions, tries=3, delay=1.0):
    def decorator(func):
        @functools.wraps(func)
        async def wrapper(*args, **kwargs):
            for attempt in range(tries):
                try:
                    return await func(*args, **kwargs)
                except exceptions as e:
                    logger.warning(f"Attempt {attempt + 1} failed with error: {e}")
                    if attempt < tries - 1:
                        await asyncio.sleep(delay)
                    else:
                        logger.error(f"All {tries} attempts failed for function {func.__name__}")
                        raise
        return wrapper
    return decorator

// File: models\gmn\__init__.py


// File: models\lnn\lnn_model.py
# File: models/lnn/lnn_model.py

import torch
import torch.nn as nn

class LiquidNeuralNetwork(nn.Module):
    def __init__(self, input_size: int, hidden_size: int, output_size: int, num_layers: int = 2, dropout: float = 0.2):
        super(LiquidNeuralNetwork, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, output_size)
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.ReLU()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass of the LNN.
        Args:
            x: Input tensor of shape (batch_size, seq_len, input_size)
        Returns:
            Output tensor of shape (batch_size, output_size)
        """
        out, _ = self.lstm(x)  # out: (batch_size, seq_len, hidden_size)
        out = self.dropout(out[:, -1, :])  # Take the output from the last time step
        out = self.relu(out)
        out = self.fc(out)
        return out


// File: models\lnn\train_lnn.py
# File: models/lnn/train_lnn.py

import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from models.lnn.lnn_model import LiquidNeuralNetwork
from models.utils.config import Config
from sklearn.preprocessing import MinMaxScaler
import logging


def train_lnn():
    """Trains the LNN model and saves it."""
    config = Config("configs/config.yaml")
    
    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )

    try:
        # Load and prepare your data
        # Replace 'your_training_data.csv' with your actual data source
        data = pd.read_csv("data/your_training_data.csv")  # Ensure this file exists and is correctly formatted
        X = data.iloc[:, :-1].values
        y = data.iloc[:, -1].values

        # Scale the input features
        scaler = MinMaxScaler()
        X_scaled = scaler.fit_transform(X)

        # Convert to tensors
        X_tensor = torch.tensor(X_scaled, dtype=torch.float32).unsqueeze(1)  # Shape: (batch_size, seq_len, input_size)
        y_tensor = torch.tensor(y, dtype=torch.float32)  # Shape: (batch_size,)

        # Create DataLoader
        dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)
        dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)

        # Initialize the model, loss function, and optimizer
        input_size = X_scaled.shape[1]
        model = LiquidNeuralNetwork(input_size, config.lnn_hidden_size, 1)
        criterion = nn.BCEWithLogitsLoss()  # Suitable for binary classification
        optimizer = optim.Adam(model.parameters(), lr=config.lnn_learning_rate)

        # Training loop
        epochs = config.lnn_training_epochs
        for epoch in range(epochs):
            model.train()
            epoch_loss = 0.0
            for batch_X, batch_y in dataloader:
                optimizer.zero_grad()
                outputs = model(batch_X.half()).squeeze()  # Forward pass
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                epoch_loss += loss.item()
            avg_loss = epoch_loss / len(dataloader)
            logging.info(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")

        # Save the trained model
        torch.save(model.state_dict(), config.lnn_model_path)
        logging.info(f"LNN model trained and saved to {config.lnn_model_path}")

    except FileNotFoundError:
        logging.error("Training data file not found. Please provide a valid CSV file.")
    except Exception as e:
        logging.error(f"Error during LNN training: {e}")


if __name__ == "__main__":
    train_lnn()


// File: models\lnn\__init__.py


// File: models\utils\config.py
# File: models/utils/config.py

import yaml
from typing import List, Dict, Any


class Config:
    def __init__(self, config_path: str):
        try:
            with open(config_path, "r") as f:
                self.config: Dict[str, Any] = yaml.safe_load(f)

                # Trading Parameters
                self.symbol: str = self.config.get("symbol", "BTC_USDT")
                self.interval: str = self.config.get("interval", "Min1")
                self.timeframes: List[str] = self.config.get("timeframes", ["1m", "5m", "15m", "1h", "4h"])
                self.indicators: List[str] = self.config.get("indicators", ["price", "volume", "rsi", "macd", "fibonacci"])

                # GMN Parameters
                self.max_history_length: int = self.config.get("max_history_length", 1000)

                # LNN Parameters
                self.lnn_model_path: str = self.config.get("lnn_model_path", "models/lnn/lnn_model.pth")
                self.lnn_hidden_size: int = self.config.get("lnn_hidden_size", 64)
                self.lnn_training_epochs: int = self.config.get("lnn_training_epochs", 10)
                self.training_history_length: int = self.config.get("training_history_length", 500)
                self.lnn_learning_rate: float = self.config.get("lnn_learning_rate", 0.001)

                # Agent Parameters
                self.threshold_buy: float = self.config.get("threshold_buy", 0.7)
                self.threshold_sell: float = self.config.get("threshold_sell", 0.3)

                # Risk Management
                self.risk_parameters: Dict[str, Any] = self.config.get("risk_parameters", {})

                # Trade Execution
                self.trade_parameters: Dict[str, Any] = self.config.get("trade_parameters", {})

                # System
                self.agent_loop_delay: int = self.config.get("agent_loop_delay", 1)
                self.reconnect_delay: int = self.config.get("reconnect_delay", 5)
                self.log_level = self.config.get("log_level", "INFO")

        except FileNotFoundError:
            raise FileNotFoundError(f"Config file not found: {config_path}")
        except yaml.YAMLError as e:
            raise ValueError(f"Error parsing YAML config file: {e}")


// File: models\utils\risk_management.py
# File: models/utils/risk_management.py

import logging
from typing import Dict, Any


class RiskManager:
    def __init__(self, risk_parameters: Dict[str, Any]):
        self.max_drawdown = risk_parameters.get("max_drawdown", 0.1)
        self.max_position_size = risk_parameters.get("max_position_size", 0.05)  # Example: 5% of portfolio
        # ... (initialize other risk parameters as needed)

    def check_risk(self, current_drawdown: float, current_position: str, market_data: Dict) -> bool:
        """
        Checks if the current trade action is within risk parameters.
        Args:
            current_drawdown (float): Current drawdown ratio.
            current_position (str): Current position ('long', 'short', or None).
            market_data (Dict): Latest market data for additional risk checks.
        Returns:
            bool: True if within risk parameters, False otherwise.
        """
        if current_drawdown > self.max_drawdown:
            logging.warning(
                f"Risk check failed: Drawdown ({current_drawdown:.2f}) exceeds maximum allowed ({self.max_drawdown:.2f})."
            )
            return False

        # Example: Prevent increasing position size beyond maximum allowed
        # Implement additional risk checks based on current_position and market_data
        # For instance, limit the number of concurrent positions, check volatility, etc.

        return True


// File: models\utils\__init__.py


// File: tests\test_processed_data.py
# File: tests/test_processed_data.py

import asyncio
import pytest
from data.data_processor import DataProcessor
from models.gmn.crypto_gmn import CryptoGMN
from data.config import Config as DataIngestionConfig

@pytest.fixture(scope="module")
def config():
    """Load data ingestion configuration."""
    return DataIngestionConfig()

@pytest.fixture(scope="module")
def gmn(config):
    """Create a CryptoGMN instance."""
    return CryptoGMN(config.timeframes, config.max_history_length, 5)

@pytest.fixture(scope="module")
def data_processor(gmn):
    """Create a DataProcessor instance."""
    return DataProcessor(gmn)

@pytest.mark.asyncio
async def test_processed_data_sample(data_processor):
    """
    Test to check the processed data sample and ensure it's ready for GMN calculations.
    """

    # Example raw data batch that would be processed in the data flow
    raw_data_batch = [
        {'channel': 'push.kline', 'data': {'price': 50000, 'volume': 1.5}},
        {'channel': 'push.kline', 'data': {'price': 50100, 'volume': 1.6}},
        {'channel': 'push.kline', 'data': {'price': 50200, 'volume': 1.7}},
        {'channel': 'push.kline', 'data': {'price': 50300, 'volume': 1.8}},
        {'channel': 'push.kline', 'data': {'price': 50400, 'volume': 2.0}},
        # Add more data for multiple timeframes/indicators as necessary
    ]

    # Process the raw data through the data processor
    await data_processor.process_data(raw_data_batch)

    # Fetch the processed data from the GMN
    processed_data = data_processor.gmn.get_all_data()

    # Check if the processed data contains required indicators and is in expected format
    assert '1m' in processed_data, "Processed data missing '1m' timeframe"
    assert 'price' in processed_data['1m'], "Processed data missing 'price' indicator in '1m' timeframe"
    assert 'volume' in processed_data['1m'], "Processed data missing 'volume' indicator in '1m' timeframe"
    
    # Print the processed data for debugging and analysis
    print("Processed Data Sample for GMN Calculations:")
    print(processed_data)

    # Validate structure of processed data for GMN calculations
    assert isinstance(processed_data['1m']['price'], list), "Price data is not in the correct format (list)"
    assert isinstance(processed_data['1m']['volume'], list), "Volume data is not in the correct format (list)"
    assert len(processed_data['1m']['price']) > 0, "Price data list is empty"
    assert len(processed_data['1m']['volume']) > 0, "Volume data list is empty"

    # You can add further checks to ensure that all other indicators are processed properly

if __name__ == "__main__":
    pytest.main(["-v", "tests/test_processed_data.py"])


// File: tests\__init__.py


