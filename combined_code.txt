**File Tree (Relevant Files Only)**
  .
    - run_preprocessing.py
  data\final
    - processed_data_15m.csv
  data\raw
    - btc_15m_data_2018_to_2024-2024-10-10.csv
    - btc_1d_data_2018_to_2024-2024-10-10.csv
    - btc_1h_data_2018_to_2024-2024-10-10.csv
    - btc_4h_data_2018_to_2024-2024-10-10.csv
  src
    - __init__.py
  src\agent
    - agent.py
    - custom_policy.py
    - environment.py
    - __init__.py
  src\config
    - config.yaml
    - __init__.py
  src\data_preprocessing
    - indicators.py
    - preprocess.py
    - utils.py
    - __init__.py
  src\models
    - liquid_nn.py
    - __init__.py
  src\training
    - train.py
    - __init__.py
// File: run_preprocessing.py
# run_preprocessing.py

import os
import sys
from datetime import datetime

# Ensure the src directory is in the PYTHONPATH
project_root = os.path.abspath(os.path.dirname(__file__))
src_dir = os.path.join(project_root, 'src')
if src_dir not in sys.path:
    sys.path.insert(0, src_dir)

from src.data_preprocessing.preprocess import main

if __name__ == "__main__":
    start_time = datetime.now()
    print(f"Starting data preprocessing at {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    main()
    end_time = datetime.now()
    print(f"Data preprocessing completed at {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Total duration: {end_time - start_time}")


// File: data\final\processed_data_15m.csv
// Snippet:
          Open time      open      high       low     close    volume timeframe  return_15m  return_1h  return_4h  return_1d    ema_14    sma_14    rsi_14  stoch_k  stoch_d   bb_mavg  bb_hband  bb_lband  bb_pband  bb_wband  kc_hband  kc_lband  kc_mband  kc_pband  kc_wband   atr_14      obv      macd  macd_signal  macd_diff       adx   adx_pos   adx_neg  ulcer_index       adi      cmf       eom      vpt  target_15m  target_1h  target_4h  target_1d
2018-01-02 08:15:00 -0.650249 -0.649854 -0.651853 -0.652802 -0.479828       15m   -0.948318   1.118089   1.558018  -0.607880 -0.655741 -0.658159  0.145356 0.967241 1.638338 -0.658179 -0.653196 -0.663132  0.681056  0.779322 -0.654277 -0.662415 -0.658336  0.470376  1.718020 0.090990 0.270198 -0.199154    -0.497168   0.893645 -0.538575  0.137923 -0.092188     1.388288 -1.434890 1.278633  0.296820 1.781499           0          1          1          1
2018-01-02 08:30:00 -0.652795 -0.652909 -0.652401 -0.653054 -0.495970       15m   -0.097393   0.184753   1.405218  -0.219456 -0.655382 -0.657595  0.114156 0.919495 1.355023 -0.658373 -0.653874 -0.662834  0.704164  0.705694 -0.654573 -0.662369 -0.658462  0.470738  1.647853 0.046731 0.269087 -0.152085    -0.430033   0.839996 -0.658614  0.051452 -0.097265     1.186745 -1.434925 0.995821 -0.209438 1.781485           0          1          1          1
2018-01-02 08:45:00 -0.653046 -0.653159 -0.653509 -0.654021 -0.484050       15m   -0.363928  -0.171318   1.010842  -0.390759 -0.655200 -0.657049 -0.010562 0.736145 0.954567 -0.658444 -0.654073 -0.662774  0.588905  0.686151 -0.654837 -0.662280 -0.658550  0.399495  1.575160 0.015231 0.267896 -0.125811    -0.370745   0.741811 -0.758162 -0.061001 -0.045244     0.970939 -1.434977 0.920483 -0.095335 1.781429           1          1          1          1
2018-01-02 09:00:00 -0.653154 -0.645850 -0.652307 -0.646172 -0.514480       15m    2.924090   0.766556   1.472767  -0.258326 -0.653995 -0.655912  0.836702 1.524905 1.157525 -0.658331 -0.653519 -0.663109  1.592141  0.753319 -0.654662 -0.662261 -0.658452  1.127525  1.607158 0.054721 0.268881 -0.004877    -0.297629   0.914832 -0.767657  0.699485 -0.337897     0.779443 -1.434903 0.896526  1.930892 1.781804           0          1          1          1
2018-01-02 09:15:00 -0.645824 -0.646801 -0.650062 -0.647648 -0.528700       15m   -0.547805   0.973166   1.397995  -0.236341 -0.653148 -0.654877  0.637847 1.291313 1.292846 -0.657301 -0.652537 -0.662030  1.263801  0.744766 -0.654861 -0.660726 -0.657788  1.058353  1.247912 0.055549 0.267992  0.071174    -0.222983   0.933969 -0.776475  0.458936 -0.514066     0.563215 -1.434876 1.499806  0.218340 1.781741           0          1          1          1

// File: data\raw\btc_15m_data_2018_to_2024-2024-10-10.csv
// Snippet:
          Open time     Open     High      Low    Close     Volume              Close time  Quote asset volume  Number of trades  Taker buy base asset volume  Taker buy quote asset volume  Ignore
2018-01-01 00:00:00 13715.65 13715.65 13400.01 13556.15 123.616013 2018-01-01 00:14:59.999        1.675545e+06              1572                    63.227133                  8.576108e+05       0
2018-01-01 00:15:00 13533.75 13550.87 13402.00 13521.12  98.136430 2018-01-01 00:29:59.999        1.321757e+06              1461                    47.686389                  6.422812e+05       0
2018-01-01 00:30:00 13500.00 13545.37 13450.00 13470.41  79.904037 2018-01-01 00:44:59.999        1.078825e+06              1000                    43.710406                  5.900347e+05       0
2018-01-01 00:45:00 13494.65 13690.87 13450.00 13529.01 141.699719 2018-01-01 00:59:59.999        1.917783e+06              1195                    73.897993                  1.000614e+06       0
2018-01-01 01:00:00 13528.99 13571.74 13402.28 13445.63  72.537533 2018-01-01 01:14:59.999        9.778198e+05               898                    34.257652                  4.618369e+05       0

// File: data\raw\btc_1d_data_2018_to_2024-2024-10-10.csv
// Snippet:
 Open time     Open     High      Low    Close       Volume              Close time  Quote asset volume  Number of trades  Taker buy base asset volume  Taker buy quote asset volume  Ignore
2018-01-01 13715.65 13818.55 12750.00 13380.00  8609.915844 2018-01-01 23:59:59.999        1.147997e+08            105595                  3961.938946                  5.280975e+07       0
2018-01-02 13382.16 15473.49 12890.02 14675.11 20078.092111 2018-01-02 23:59:59.999        2.797171e+08            177728                 11346.326739                  1.580801e+08       0
2018-01-03 14690.00 15307.56 14150.00 14919.51 15905.667639 2018-01-03 23:59:59.999        2.361169e+08            162787                  8994.953566                  1.335873e+08       0
2018-01-04 14919.51 15280.00 13918.04 15059.54 21329.649574 2018-01-04 23:59:59.999        3.127816e+08            170310                 12680.812951                  1.861168e+08       0
2018-01-05 15059.56 17176.24 14600.00 16960.39 23251.491125 2018-01-05 23:59:59.999        3.693220e+08            192969                 13346.622293                  2.118299e+08       0

// File: data\raw\btc_1h_data_2018_to_2024-2024-10-10.csv
// Snippet:
          Open time     Open     High      Low    Close     Volume              Close time  Quote asset volume  Number of trades  Taker buy base asset volume  Taker buy quote asset volume  Ignore
2018-01-01 00:00:00 13715.65 13715.65 13400.01 13529.01 443.356199 2018-01-01 00:59:59.999        5.993910e+06              5228                   228.521921                  3.090541e+06       0
2018-01-01 01:00:00 13528.99 13595.89 13155.38 13203.06 383.697006 2018-01-01 01:59:59.999        5.154522e+06              4534                   180.840403                  2.430449e+06       0
2018-01-01 02:00:00 13203.00 13418.43 13200.00 13330.18 429.064572 2018-01-01 02:59:59.999        5.710192e+06              4887                   192.237935                  2.558505e+06       0
2018-01-01 03:00:00 13330.26 13611.27 13290.00 13410.03 420.087030 2018-01-01 03:59:59.999        5.657448e+06              4789                   137.918407                  1.858041e+06       0
2018-01-01 04:00:00 13434.98 13623.29 13322.15 13601.01 340.807329 2018-01-01 04:59:59.999        4.588047e+06              4563                   172.957635                  2.328058e+06       0

// File: data\raw\btc_4h_data_2018_to_2024-2024-10-10.csv
// Snippet:
          Open time     Open     High      Low    Close      Volume              Close time  Quote asset volume  Number of trades  Taker buy base asset volume  Taker buy quote asset volume  Ignore
2018-01-01 00:00:00 13715.65 13715.65 13155.38 13410.03 1676.204807 2018-01-01 03:59:59.999        2.251607e+07             19438                   739.518666                  9.937537e+06       0
2018-01-01 04:00:00 13434.98 13818.55 13322.15 13570.35 1302.214836 2018-01-01 07:59:59.999        1.770254e+07             18061                   588.516222                  7.998796e+06       0
2018-01-01 08:00:00 13569.98 13735.24 13001.13 13220.56 1319.755931 2018-01-01 11:59:59.999        1.765121e+07             17200                   582.668293                  7.792109e+06       0
2018-01-01 12:00:00 13220.56 13330.00 12750.00 13247.00 1831.933153 2018-01-01 15:59:59.999        2.391259e+07             21357                   830.314747                  1.083304e+07       0
2018-01-01 16:00:00 13247.00 13290.65 12940.00 13240.37 1092.337234 2018-01-01 19:59:59.999        1.434887e+07             13178                   557.453779                  7.324320e+06       0

// File: src\__init__.py


// File: src\agent\agent.py
# USE SWARM by OPEN AI



// File: src\agent\custom_policy.py
# USE PYTORCH not stable baseline



// File: src\agent\environment.py
#use SWARM

// File: src\agent\__init__.py


// File: src\config\config.yaml
# src/config/config.yaml

data:
  raw_dir: "../../data/raw"
  final_dir: "../../data/final"
  visuals_dir: "../../data/visuals"

preprocessing:
  rolling_window_sizes:
    '15m': [[3, '45m'], [12, '3h'], [48, '12h']]
    '1h': [[4, '4h'], [24, '1d'], [168, '1w']]
    '4h': [[2, '8h'], [6, '1d'], [42, '1w']]
    '1d': [[7, '1w'], [30, '1m'], [365, '1y']]
  correlation_threshold: 0.95
  scaler_type: 'standard'  # options: 'standard', 'minmax'

imbalance_handling:
  oversample_strategy:
    3: 2000
    2: 2000
    -3: 2000
    -2: 2000
  undersample_strategy:
    1: 2000
    0: 2000
    -1: 2000

training:
  sequence_length: 30
  batch_size: 64
  total_timesteps: 200000
  save_freq: 50000


// File: src\config\__init__.py


// File: src\data_preprocessing\indicators.py
import pandas as pd
from ta.volatility import BollingerBands, AverageTrueRange, KeltnerChannel, UlcerIndex
from ta.trend import EMAIndicator, SMAIndicator, MACD, ADXIndicator
from ta.momentum import RSIIndicator, StochasticOscillator
from ta.volume import OnBalanceVolumeIndicator, AccDistIndexIndicator, ChaikinMoneyFlowIndicator, EaseOfMovementIndicator, VolumePriceTrendIndicator
from loguru import logger

def add_technical_indicators(df):
    try:
        df_list = []
        for tf in df['timeframe'].unique():
            df_tf = df[df['timeframe'] == tf].copy()
            if df_tf.isnull().values.any():
                logger.warning(f"NaN values found in data for timeframe {tf} before adding technical indicators. Dropping NaNs.")
                df_tf.dropna(inplace=True)

            # Ensure there is enough data for indicator calculation
            min_required_length = 30  # Adjust based on the maximum window size required by indicators
            if len(df_tf) < min_required_length:
                logger.warning(f"Insufficient data for timeframe {tf}. Skipping indicator calculation.")
                continue

            logger.debug(f"Adding technical indicators for timeframe {tf}...")

            # Add EMA
            df_tf['ema_14'] = EMAIndicator(close=df_tf['close'], window=14).ema_indicator()

            # Add SMA
            df_tf['sma_14'] = SMAIndicator(close=df_tf['close'], window=14).sma_indicator()

            # Add RSI
            df_tf['rsi_14'] = RSIIndicator(close=df_tf['close'], window=14).rsi()

            # Add Stochastic Oscillator
            stoch = StochasticOscillator(high=df_tf['high'], low=df_tf['low'], close=df_tf['close'], window=14)
            df_tf['stoch_k'] = stoch.stoch()
            df_tf['stoch_d'] = stoch.stoch_signal()

            # Add Bollinger Bands
            bollinger = BollingerBands(close=df_tf['close'], window=20, window_dev=2)
            df_tf['bb_mavg'] = bollinger.bollinger_mavg()
            df_tf['bb_hband'] = bollinger.bollinger_hband()
            df_tf['bb_lband'] = bollinger.bollinger_lband()
            df_tf['bb_pband'] = bollinger.bollinger_pband()
            df_tf['bb_wband'] = bollinger.bollinger_wband()

            # Add Keltner Channel
            keltner = KeltnerChannel(high=df_tf['high'], low=df_tf['low'], close=df_tf['close'], window=20)
            df_tf['kc_hband'] = keltner.keltner_channel_hband()
            df_tf['kc_lband'] = keltner.keltner_channel_lband()
            df_tf['kc_mband'] = keltner.keltner_channel_mband()
            df_tf['kc_pband'] = keltner.keltner_channel_pband()
            df_tf['kc_wband'] = keltner.keltner_channel_wband()

            # Add Average True Range (ATR)
            df_tf['atr_14'] = AverageTrueRange(high=df_tf['high'], low=df_tf['low'], close=df_tf['close'], window=14).average_true_range()

            # Add On-Balance Volume (OBV)
            df_tf['obv'] = OnBalanceVolumeIndicator(close=df_tf['close'], volume=df_tf['volume']).on_balance_volume()

            # Add MACD
            macd = MACD(close=df_tf['close'], window_slow=26, window_fast=12, window_sign=9)
            df_tf['macd'] = macd.macd()
            df_tf['macd_signal'] = macd.macd_signal()
            df_tf['macd_diff'] = macd.macd_diff()

            # Add ADX
            adx = ADXIndicator(high=df_tf['high'], low=df_tf['low'], close=df_tf['close'], window=14)
            df_tf['adx'] = adx.adx()
            df_tf['adx_pos'] = adx.adx_pos()
            df_tf['adx_neg'] = adx.adx_neg()

            # Add Ulcer Index (UI)
            df_tf['ulcer_index'] = UlcerIndex(close=df_tf['close'], window=14).ulcer_index()

            # Add Accumulation/Distribution Index (ADI)
            if len(df_tf) > 1:  # Ensure there is more than one row for volume-based indicators
                df_tf['adi'] = AccDistIndexIndicator(high=df_tf['high'], low=df_tf['low'], close=df_tf['close'], volume=df_tf['volume']).acc_dist_index()

                # Add Chaikin Money Flow (CMF)
                df_tf['cmf'] = ChaikinMoneyFlowIndicator(high=df_tf['high'], low=df_tf['low'], close=df_tf['close'], volume=df_tf['volume'], window=20).chaikin_money_flow()

                # Add Ease of Movement (EoM)
                eom = EaseOfMovementIndicator(high=df_tf['high'], low=df_tf['low'], volume=df_tf['volume'], window=14)
                df_tf['eom'] = eom.ease_of_movement()

                # Add Volume Price Trend (VPT)
                df_tf['vpt'] = VolumePriceTrendIndicator(close=df_tf['close'], volume=df_tf['volume']).volume_price_trend()

            if df_tf.empty:
                logger.warning(f"Dataframe for timeframe {tf} is empty after adding indicators. Skipping this timeframe.")
                continue

            df_list.append(df_tf)

        if not df_list:
            logger.error("No dataframes were processed successfully. Returning original dataframe.")
            return df

        df = pd.concat(df_list, axis=0)
        df.sort_index(inplace=True)  # Ensure the index is sorted after concatenation
        logger.info("Technical indicators added successfully.")
        return df
    except Exception as e:
        logger.error(f"Error adding technical indicators: {e}")
        return df


// File: src\data_preprocessing\preprocess.py
# src/data_preprocessing/preprocess.py
import os
import sys
import glob
import pandas as pd
from sklearn.preprocessing import StandardScaler
from concurrent.futures import ProcessPoolExecutor
import multiprocessing
from datetime import datetime
import yaml
from loguru import logger
from .indicators import add_technical_indicators

def setup_logging():
    log_dir = os.path.abspath('../../logs')
    os.makedirs(log_dir, exist_ok=True)
    log_filename = f"preprocess_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

    logger.remove()
    logger.add(os.path.join(log_dir, log_filename), rotation="10 MB", level="DEBUG",
               format="{time} {level} {message}")
    logger.add(sys.stderr, level="INFO",
               format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | {message}")

    return logger

logger = setup_logging()

def load_config():
    try:
        config_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '../config/config.yaml'))
        with open(config_path, 'r') as file:
            config = yaml.safe_load(file)
        logger.debug(f"Configuration loaded from {config_path}")
        return config
    except Exception as e:
        logger.error(f"Error loading configuration: {e}")
        raise

config = load_config()

DATA_DIR_RAW = os.path.abspath(os.path.join(os.path.dirname(__file__), config['data']['raw_dir']))
DATA_DIR_PROCESSED = os.path.abspath(os.path.join(os.path.dirname(__file__), config['data']['final_dir']))


def load_and_prepare_data(filename):
    try:
        logger.info(f"Processing file: {filename}")
        df = pd.read_csv(os.path.join(DATA_DIR_RAW, filename))

        # Ensure 'Open time' is parsed as datetime
        df['Open time'] = pd.to_datetime(df['Open time'], errors='coerce')
        df.set_index('Open time', inplace=True)
        df.sort_index(inplace=True)

        df.columns = [col.lower() for col in df.columns]
        df.rename(columns={'close time': 'close_time'}, inplace=True)

        columns_to_keep = ['open', 'high', 'low', 'close', 'volume']
        df = df[columns_to_keep]

        timeframe = '15m' if '15m' in filename else '1h' if '1h' in filename else '4h' if '4h' in filename else '1d'
        df['timeframe'] = timeframe

        # Convert all columns except 'timeframe' to numeric
        for col in df.columns:
            if col != 'timeframe':
                df[col] = pd.to_numeric(df[col], errors='coerce')

        # Drop rows with NaN values after conversion
        df.dropna(inplace=True)

        return df
    except Exception as e:
        logger.error(f"Error processing file {filename}: {e}")
        return None

def compute_returns(df):
    try:
        for tf in ['15m', '1h', '4h', '1d']:
            periods = {'15m': 1, '1h': 4, '4h': 16, '1d': 96}
            df[f'return_{tf}'] = df.groupby('timeframe')['close'].pct_change(periods[tf])
        return df
    except Exception as e:
        logger.error(f"Error computing returns: {e}")
        return df

def create_targets(df):
    try:
        for tf in ['15m', '1h', '4h', '1d']:
            periods = {'15m': 1, '1h': 4, '4h': 16, '1d': 96}
            df[f'future_price_{tf}'] = df.groupby('timeframe')['close'].shift(-periods[tf])
            df[f'target_{tf}'] = (df[f'future_price_{tf}'] > df['close']).astype(int)
            df.drop(f'future_price_{tf}', axis=1, inplace=True)
        return df
    except Exception as e:
        logger.error(f"Error creating target variables: {e}")
        return df

def process_data():
    try:
        all_files = glob.glob(os.path.join(DATA_DIR_RAW, "*.csv"))

        with ProcessPoolExecutor(max_workers=multiprocessing.cpu_count()) as executor:
            dfs = list(executor.map(load_and_prepare_data, [os.path.basename(f) for f in all_files]))

        df = pd.concat([d for d in dfs if d is not None], axis=0)
        df = df[~df.index.duplicated(keep='first')]
        df.sort_index(inplace=True)

        df = compute_returns(df)
        df = add_technical_indicators(df)
        df = create_targets(df)

        df.dropna(inplace=True)

        scaler = StandardScaler()
        features = df.columns.drop(['timeframe', 'target_15m', 'target_1h', 'target_4h', 'target_1d'])
        df[features] = scaler.fit_transform(df[features])

        os.makedirs(DATA_DIR_PROCESSED, exist_ok=True)
        for tf in df['timeframe'].unique():
            output_file = os.path.join(DATA_DIR_PROCESSED, f"processed_data_{tf}.csv")
            df[df['timeframe'] == tf].to_csv(output_file)
            logger.info(f"Processed data for {tf} timeframe saved to {output_file}")

        return df
    except Exception as e:
        logger.error(f"Error in process_data: {e}")
        raise

def main():
    try:
        logger.info("Starting data preprocessing pipeline...")
        df = process_data()
        logger.info("Data preprocessing pipeline completed successfully.")
    except Exception as e:
        logger.exception(f"Unhandled exception in main pipeline: {e}")

if __name__ == "__main__":
    main()


// File: src\data_preprocessing\utils.py
import pandas as pd
from loguru import logger

def categorize_profit(profit):
    try:
        if profit > 0.01:
            return 1
        elif profit < -0.01:
            return -1
        else:
            return 0
    except Exception as e:
        logger.error(f"Error categorizing profit {profit}: {e}")
        return 0

def categorize_profit_loss(df):
    try:
        logger.debug("Categorizing profit/loss into target categories...")

        timeframes = ['15m', '1h', '4h', '1d']
        for timeframe in timeframes:
            return_col = f'return_{timeframe}'
            if return_col in df.columns:
                df[f'target_category_{timeframe}'] = df[return_col].apply(categorize_profit)
            else:
                logger.warning(f"Return column {return_col} not found in DataFrame.")

        logger.debug("Categorized profit/loss successfully.")
        return df
    except Exception as e:
        logger.error(f"Error categorizing profit/loss: {e}")
        return df

def check_data_integrity(df, filename):
    try:
        logger.debug(f"Checking data integrity for file: {filename}")

        if df.index.duplicated().any():
            duplicates = df.index[df.index.duplicated()]
            logger.warning(f"Found {duplicates.sum()} duplicate timestamps in {filename}. Dropping duplicates.")
            df = df[~df.index.duplicated(keep='first')]

        critical_columns = ['open', 'high', 'low', 'close', 'volume']
        missing = df[critical_columns].isnull().sum()
        if missing.any():
            logger.warning(f"Missing values found in {filename}:\n{missing}")
            df.dropna(subset=critical_columns, inplace=True)
            logger.info(f"Dropped rows with missing critical values in {filename}.")

        logger.debug(f"Data integrity checks passed for {filename}.")
        return df
    except Exception as e:
        logger.error(f"Error checking data integrity for {filename}: {e}")
        return df

// File: src\data_preprocessing\__init__.py


// File: src\models\liquid_nn.py
# src/models/liquid_nn.py

import torch
import torch.nn as nn
import torch.nn.functional as F

class LiquidNN(nn.Module):
    """
    Enhanced Liquid Neural Network (LNN) with residual connections and normalization.
    """
    def __init__(self, input_size, hidden_size, num_classes, sequence_length):
        super(LiquidNN, self).__init__()
        self.hidden_size = hidden_size
        self.sequence_length = sequence_length

        # GRU Layer with bidirectionality for richer feature extraction
        self.gru = nn.GRU(input_size, hidden_size, batch_first=True, bidirectional=True)
        
        # Attention Layer
        self.attention = nn.Linear(hidden_size * 2, 1)
        
        # Fully Connected Layers with Residual Connection
        self.fc1 = nn.Linear(hidden_size * 2, 128)
        self.bn1 = nn.BatchNorm1d(128)
        self.fc2 = nn.Linear(128, num_classes)
        self.bn2 = nn.BatchNorm1d(num_classes)
    
    def forward(self, x):
        """
        Forward pass of the Liquid Neural Network.

        Parameters:
        - x (torch.Tensor): Input tensor of shape [batch_size, sequence_length, input_size]

        Returns:
        - torch.Tensor: Output logits of shape [batch_size, num_classes]
        """
        # GRU outputs
        gru_out, _ = self.gru(x)  # gru_out: [batch_size, sequence_length, hidden_size*2]
        
        # Attention mechanism
        attn_weights = F.softmax(self.attention(gru_out), dim=1)  # [batch_size, sequence_length, 1]
        context = torch.sum(attn_weights * gru_out, dim=1)  # [batch_size, hidden_size*2]
        
        # Fully Connected Layers with Residual Connection
        out = F.relu(self.bn1(self.fc1(context)))
        out = self.bn2(self.fc2(out))
        return out


// File: src\models\__init__.py


// File: src\training\train.py


// File: src\training\__init__.py


