**File Tree (Relevant Files Only)**
  .
    - main.py
    - setup.py
  src\config
    - config.py
    - features.py
    - hardware.py
    - model.py
    - risk.py
    - training.py
    - __init__.py
  src\data
    - features.py
    - indicators.py
    - processor.py
    - __init__.py
  src\data\raw
    - btc_usdt_1m_klines.csv
  src\env
    - bybit_env.py
    - __init__.py
  src\models
    - agent.py
    - appo.py
    - nlnn.py
    - reward.py
    - __init__.py
  src\tests
    - __init__.py
  src\training
    - trainer.py
  src\utils
    - logging.py
    - risk_manager.py
    - system_monitor.py
    - __init__.py
// File: main.py
import logging
from src.config import TradingConfig
from src.models.agent import Agent
from src.data.features import FeatureExtractor
from src.env.bybit_env import BybitFuturesEnv
from src.utils.risk_manager import RiskManager

def main():
    # Initialize logging
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    # Load configuration
    config = TradingConfig()
    
    # Initialize components
    feature_extractor = FeatureExtractor(config)
    risk_manager = RiskManager(config)
    env = BybitFuturesEnv(config)
    
    # Initialize agent
    agent = Agent(
        input_size=config.feature_dim,
        hidden_size=config.hidden_size,
        num_layers=config.num_layers
    )
    
    logger.info("Starting training...")
    
    # Training loop
    for episode in range(config.num_episodes):
        state, info = env.reset()
        hidden_states = agent.init_hidden()
        episode_reward = 0
        
        while True:
            # Get action from agent
            action, value, hidden_states = agent.act(state, hidden_states)
            
            # Validate action with risk manager
            is_valid, message = risk_manager.validate_action(
                action, env.current_price, env.account_balance
            )
            
            if not is_valid:
                logger.warning(f"Action rejected: {message}")
                action = 2  # Default to HOLD
            
            # Take action in environment
            next_state, reward, done, truncated, info = env.step(action)
            episode_reward += reward
            
            if done or truncated:
                break
            
            state = next_state
            
        logger.info(f"Episode {episode} completed with reward {episode_reward}")

if __name__ == "__main__":
    main()

// File: setup.py
import os
import subprocess
import sys
from pathlib import Path

def setup_environment():
    """Setup the development environment with optimized configurations."""
    # Create virtual environment
    subprocess.run([sys.executable, "-m", "venv", ".venv"], check=True)
    
    # Get the path to the virtual environment's Python interpreter
    if os.name == "nt":  # Windows
        python_path = Path(".venv/Scripts/python.exe")
        pip_path = Path(".venv/Scripts/pip.exe")
    else:  # Unix-like
        python_path = Path(".venv/bin/python")
        pip_path = Path(".venv/bin/pip")
    
    # Upgrade pip
    subprocess.run([str(pip_path), "install", "--upgrade", "pip"], check=True)
    
    # Install PyTorch with CUDA 11.8 support (optimized for NVIDIA 3070)
    subprocess.run([
        str(pip_path),
        "install",
        "torch",
        "torchvision",
        "torchaudio",
        "--index-url",
        "https://download.pytorch.org/whl/cu118"
    ], check=True)
    
    # Install other requirements
    subprocess.run([
        str(pip_path),
        "install",
        "-r",
        "requirements.txt"
    ], check=True)
    
    # Create necessary directories
    directories = [
        "logs",
        "data",
        "models",
        "configs",
        "tests"
    ]
    for directory in directories:
        Path(directory).mkdir(exist_ok=True)
    
    print("Environment setup completed successfully!")
    print("\nActivate your virtual environment:")
    if os.name == "nt":
        print(".venv\\Scripts\\activate")
    else:
        print("source .venv/bin/activate")

if __name__ == "__main__":
    setup_environment()

// File: src\config\config.py
# config.py

from pydantic import BaseModel, Field
from typing import List
from .hardware import HardwareConfig
from .model import ModelConfig
from .risk import RiskConfig
from .features import FeatureConfig
from .training import TrainingConfig

class TradingConfig(BaseModel):
    """Master configuration class.

    This class aggregates all configuration settings for the trading agent and its environment.

    Attributes:
        hardware (HardwareConfig): Configuration for hardware resources.
        model (ModelConfig): Configuration for the neural network model.
        risk (RiskConfig): Configuration for risk management parameters.
        feature (FeatureConfig): Configuration for feature extraction.
        training (TrainingConfig): Configuration for the training process.

    Methods:
        optimize_for_hardware(): Optimizes the configuration based on the detected hardware capabilities.

    """
    hardware: HardwareConfig = Field(default_factory=HardwareConfig)
    model: ModelConfig = Field(default_factory=ModelConfig)
    risk: RiskConfig = Field(default_factory=RiskConfig)
    feature: FeatureConfig = Field(default_factory=FeatureConfig)
    training: TrainingConfig = Field(default_factory=TrainingConfig)

    def optimize_for_hardware(self):
        """Optimizes configuration for current hardware.

        This method adjusts certain parameters (primarily batch size) based on available GPU memory.
        It also sets the number of threads used by PyTorch to optimize performance.
        """
        if self.hardware.cuda_available:
            gpu_mem = torch.cuda.get_device_properties(0).total_memory
            self.model.batch_size = min(self.model.batch_size, int(gpu_mem * 0.7 / (self.model.feature_dim * 4)))
        torch.set_num_threads(self.hardware.torch_num_threads)
        if self.hardware.cuda_available:
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
        return self

def create_default_config() -> TradingConfig:
    """Creates and returns a default configuration, optimized for the current hardware."""
    config = TradingConfig()
    return config.optimize_for_hardware()

import torch

# Example of how to save the default config to a file (config.pt)
config = create_default_config()
torch.save(config.model_dump(), "config.pt")

// File: src\config\features.py
from pydantic import BaseModel, Field
from typing import List

class FeatureConfig(BaseModel):
    """Feature extraction configuration.

    This class defines parameters for extracting features from market data.  These features are used as input to the trading agent's neural network.

    Attributes:
        rolling_window_size (int): The size of the rolling window used for calculating features.  A larger window considers more historical data but might make the features less responsive to recent changes.
        technical_indicators (List[str]): A list of technical indicators to calculate.  These indicators provide insights into market trends and momentum.  Supported indicators depend on your `ta` library version.
        market_features (List[str]): A list of raw market features to include. These are typically directly from the market data (e.g., price, volume).
        normalization_method (str): The method used for normalizing features.  Normalization is essential to prevent features with larger scales from dominating the model's learning.


    """
    rolling_window_size: int = Field(100, description="Size of rolling window for features")
    technical_indicators: List[str] = Field(default=["rsi", "macd", "bbands", "volatility"], description="List of technical indicators to use")
    market_features: List[str] = Field(default=["price", "volume", "spread", "depth", "funding_rate"], description="List of market features to use")
    normalization_method: str = Field("standard", description="Feature normalization method")

// File: src\config\hardware.py
from pydantic import BaseModel, Field
import psutil
import torch

class HardwareConfig(BaseModel):
    """Hardware-specific configuration.

    This class encapsulates settings related to the hardware resources used by the trading agent.
    It automatically detects certain hardware features and provides options for optimization.

    Attributes:
        num_cpu_threads (int): The number of CPU threads to utilize for parallel processing. 
                               Defaults to the number of logical CPU cores detected by `psutil`.
        cuda_available (bool): A boolean indicating whether CUDA (GPU acceleration) is available. 
                               Automatically detected by PyTorch.
        gpu_memory_limit (int): The maximum amount of GPU memory (in MB) to allocate for training. 
                               Defaults to 7168 MB (7GB), leaving 1GB for system use on an NVIDIA 3070.  Adjust this based on your GPU's VRAM.
        torch_num_threads (int): The number of threads PyTorch should use. Defaults to 12 (optimized for Ryzen 9 7900X). Adjust for your CPU.
        pin_memory (bool): A boolean flag indicating whether to use pinned memory for faster data transfer between CPU and GPU. Defaults to True.  Generally recommended for GPU usage.

    """
    num_cpu_threads: int = Field(default_factory=lambda: psutil.cpu_count(logical=True), description="Number of CPU threads to use")
    cuda_available: bool = Field(default_factory=lambda: torch.cuda.is_available(), description="Whether CUDA is available")
    gpu_memory_limit: int = Field(default=7168, description="GPU memory limit in MB")
    torch_num_threads: int = Field(default=12, description="Number of threads for PyTorch")
    pin_memory: bool = Field(default=True, description="Use pinned memory for faster GPU transfer")

// File: src\config\model.py
from pydantic import BaseModel, Field

class ModelConfig(BaseModel):
    """Neural network configuration.

    This class defines the architecture and training hyperparameters for the neural network model.

    Attributes:
        feature_dim (int): The dimensionality of the input feature vector.  This should match the output of your feature extractor.
        hidden_size (int): The number of neurons in each hidden layer of the neural network.
        num_layers (int): The number of hidden layers in the n-LNN (novel neural network) architecture.
        dropout (float): The dropout rate used for regularization during training (a value between 0 and 1).
        learning_rate (float): The learning rate used by the optimizer.  This controls the step size during gradient descent.
        batch_size (int): The size of the mini-batches used during training.  Larger batch sizes can improve training speed but require more memory.
        sequence_length (int): The length of the input sequences used for training the recurrent layers (if applicable).  This is relevant if your model has RNN components.
        gradient_clip (float): The maximum value for gradient clipping to prevent exploding gradients during training.  This helps stabilize training.

    """
    feature_dim: int = Field(64, description="Input feature dimension")
    hidden_size: int = Field(256, description="Hidden layer size")
    num_layers: int = Field(4, description="Number of n-LNN layers")
    dropout: float = Field(0.1, description="Dropout rate")
    learning_rate: float = Field(3e-4, description="Learning rate")
    batch_size: int = Field(512, description="Training batch size")
    sequence_length: int = Field(100, description="Sequence length for training")
    gradient_clip: float = Field(1.0, description="Gradient clipping value")

// File: src\config\risk.py
from pydantic import BaseModel, Field

class RiskConfig(BaseModel):
    """Risk management configuration.

    This class defines parameters to control risk during trading.  These parameters are crucial for preventing excessive losses.

    Attributes:
        max_position_size (float): The maximum fraction of the account balance that can be used for a single position.  A value between 0 and 1.  For example, 0.1 means a maximum of 10% of the account balance can be risked on any one trade.
        stop_loss_pct (float): The percentage loss at which an open position will be automatically closed (stop-loss order).  A value between 0 and 1.  For example, 0.02 means a stop-loss order will be triggered if the position incurs a 2% loss.
        take_profit_pct (float): The percentage profit at which an open position will be automatically closed (take-profit order). A value between 0 and 1. For example, 0.04 means a take-profit order will be triggered if the position achieves a 4% profit.
        max_leverage (float): The maximum leverage allowed for trades.  This magnifies both profits and losses.  Higher leverage increases risk.
        min_trade_interval (int): The minimum time (in seconds) between consecutive trades to avoid excessive trading frequency.  This helps to avoid over-trading and potential slippage.
        max_drawdown (float): The maximum acceptable drawdown (percentage loss from peak equity) before trading is stopped.  This parameter helps to prevent catastrophic losses.
        position_sizing_method (str): The method used for determining position sizes.  This could be 'kelly' (Kelly criterion), 'fixed', or another custom method.

    """
    max_position_size: float = Field(0.1, description="Maximum position size as fraction of capital")
    stop_loss_pct: float = Field(0.02, description="Stop loss percentage")
    take_profit_pct: float = Field(0.04, description="Take profit percentage")
    max_leverage: float = Field(5.0, description="Maximum allowed leverage")
    min_trade_interval: int = Field(5, description="Minimum time interval between trades (seconds)")
    max_drawdown: float = Field(0.2, description="Maximum allowed drawdown")
    position_sizing_method: str = Field("kelly", description="Position sizing method")

// File: src\config\training.py
from pydantic import BaseModel, Field

class TrainingConfig(BaseModel):
    """Training configuration.

    This class contains hyperparameters for the reinforcement learning training process.

    Attributes:
        num_episodes (int): The total number of training episodes.
        gamma (float): The discount factor used in the reinforcement learning algorithm.  This determines the importance of future rewards. A value closer to 1 emphasizes long-term rewards.
        gae_lambda (float): The lambda parameter for Generalized Advantage Estimation (GAE).  This parameter controls the bias-variance tradeoff in GAE.
        entropy_coef (float): The coefficient for the entropy bonus in the loss function.  This encourages exploration during training.
        value_loss_coef (float): The coefficient for the value function loss in the loss function.  This balances the policy loss and value function loss.
        max_grad_norm (float): The maximum norm for gradient clipping to prevent exploding gradients during training.  Gradient clipping helps stabilize training.
        update_interval (int): The number of steps between policy updates.  More frequent updates might lead to faster learning but can be more noisy.
        num_minibatches (int): The number of minibatches used for each policy update. This parameter is used in algorithms like PPO to reduce computational cost.
        warmup_steps (int): The number of initial steps during which the policy is not updated.  This allows the agent to gather some initial experience before updating the policy.

    """
    num_episodes: int = Field(10000, description="Number of training episodes")
    gamma: float = Field(0.99, description="Discount factor")
    gae_lambda: float = Field(0.95, description="GAE lambda parameter")
    entropy_coef: float = Field(0.01, description="Entropy coefficient")
    value_loss_coef: float = Field(0.5, description="Value loss coefficient")
    max_grad_norm: float = Field(0.5, description="Maximum gradient norm")
    update_interval: int = Field(2048, description="Steps between policy updates")
    num_minibatches: int = Field(4, description="Number of minibatches per update")
    warmup_steps: int = Field(1000, description="Number of warmup steps")

// File: src\config\__init__.py


// File: src\data\features.py
import numpy as np
import pandas as pd
import ta
from typing import List, Union
from dataclasses import dataclass

@dataclass
class IndicatorConfig:
    """Configuration for technical indicator calculations.

    Attributes:
        rsi_period (int): The period for Relative Strength Index (RSI) calculation.
        macd_fast (int): The fast period for Moving Average Convergence Divergence (MACD) calculation.
        macd_slow (int): The slow period for MACD calculation.
        macd_signal (int): The signal period for MACD calculation.
        bb_period (int): The period for Bollinger Bands calculation.
        bb_std (int): The standard deviation multiplier for Bollinger Bands calculation.

    """
    rsi_period: int = 14
    macd_fast: int = 12
    macd_slow: int = 26
    macd_signal: int = 9
    bb_period: int = 20
    bb_std: int = 2

class TechnicalIndicators:
    """Calculates various technical indicators from price data.

    This class uses the `TA-Lib` library to efficiently compute several common technical indicators.

    Args:
        config (IndicatorConfig, optional): Configuration object specifying parameters for indicator calculations. 
                                            Defaults to `IndicatorConfig()`.

    Methods:
        calculate_all(prices: Union[List[float], np.ndarray]) -> dict: Calculates all configured indicators.
        calculate_rsi(df: pd.DataFrame) -> float: Calculates the Relative Strength Index (RSI).
        calculate_macd(df: pd.DataFrame) -> dict: Calculates the Moving Average Convergence Divergence (MACD) and its signal and difference.
        calculate_bollinger_bands(df: pd.DataFrame) -> dict: Calculates the Bollinger Bands (upper, middle, lower).
        calculate_volatility(df: pd.DataFrame, window: int = 20) -> float: Calculates annualized volatility.

    """
    def __init__(self, config: IndicatorConfig = None):
        self.config = config or IndicatorConfig()

    def calculate_all(self, prices: Union[List[float], np.ndarray]) -> dict:
        """Calculates all configured technical indicators.

        Args:
            prices (Union[List[float], np.ndarray]): A list or NumPy array of closing prices.

        Returns:
            dict: A dictionary containing all calculated indicators.

        """
        df = pd.DataFrame({'close': prices})
        indicators = {}
        indicators['rsi'] = self.calculate_rsi(df)
        macd_data = self.calculate_macd(df)
        indicators.update(macd_data)
        bb_data = self.calculate_bollinger_bands(df)
        indicators.update(bb_data)
        indicators['volatility'] = self.calculate_volatility(df)
        return indicators

    def calculate_rsi(self, df: pd.DataFrame) -> float:
        """Calculates the Relative Strength Index (RSI).

        Args:
            df (pd.DataFrame): Pandas DataFrame with a 'close' column containing closing prices.

        Returns:
            float: The RSI value.

        """
        rsi = ta.momentum.RSIIndicator(close=df['close'], window=self.config.rsi_period)
        return rsi.rsi().iloc[-1]

    def calculate_macd(self, df: pd.DataFrame) -> dict:
        """Calculates the Moving Average Convergence Divergence (MACD) and its signal and difference.

        Args:
            df (pd.DataFrame): Pandas DataFrame with a 'close' column containing closing prices.

        Returns:
            dict: A dictionary containing the MACD, MACD signal, and MACD difference.

        """
        macd = ta.trend.MACD(close=df['close'], window_slow=self.config.macd_slow, window_fast=self.config.macd_fast, window_sign=self.config.macd_signal)
        return {'macd': macd.macd().iloc[-1], 'macd_signal': macd.macd_signal().iloc[-1], 'macd_diff': macd.macd_diff().iloc[-1]}

    def calculate_bollinger_bands(self, df: pd.DataFrame) -> dict:
        """Calculates the Bollinger Bands (upper, middle, lower).

        Args:
            df (pd.DataFrame): Pandas DataFrame with a 'close' column containing closing prices.

        Returns:
            dict: A dictionary containing the upper, middle, and lower Bollinger Bands.

        """
        bb = ta.volatility.BollingerBands(close=df['close'], window=self.config.bb_period, window_dev=self.config.bb_std)
        return {'bb_high': bb.bollinger_hband().iloc[-1], 'bb_mid': bb.bollinger_mavg().iloc[-1], 'bb_low': bb.bollinger_lband().iloc[-1]}

    def calculate_volatility(self, df: pd.DataFrame, window: int = 20) -> float:
        """Calculates annualized volatility.

        Args:
            df (pd.DataFrame): Pandas DataFrame with a 'close' column containing closing prices.
            window (int, optional): The rolling window size for volatility calculation. Defaults to 20.

        Returns:
            float: The annualized volatility.

        """
        returns = df['close'].pct_change()
        return returns.std() * np.sqrt(252)  # Annualized volatility

// File: src\data\indicators.py
import numpy as np
import pandas as pd
import ta
from typing import List, Union
from dataclasses import dataclass

@dataclass
class IndicatorConfig:
    rsi_period: int = 14
    macd_fast: int = 12
    macd_slow: int = 26
    macd_signal: int = 9
    bb_period: int = 20
    bb_std: int = 2
    
class TechnicalIndicators:
    def __init__(self, config: IndicatorConfig = None):
        self.config = config or IndicatorConfig()
        
    def calculate_all(self, prices: Union[List[float], np.ndarray]) -> dict:
        """Calculate all technical indicators"""
        df = pd.DataFrame({'close': prices})
        
        indicators = {}
        
        # RSI
        indicators['rsi'] = self.calculate_rsi(df)
        
        # MACD
        macd_data = self.calculate_macd(df)
        indicators.update(macd_data)
        
        # Bollinger Bands
        bb_data = self.calculate_bollinger_bands(df)
        indicators.update(bb_data)
        
        # Volatility
        indicators['volatility'] = self.calculate_volatility(df)
        
        return indicators
        
    def calculate_rsi(self, df: pd.DataFrame) -> float:
        """Calculate Relative Strength Index"""
        rsi = ta.momentum.RSIIndicator(
            close=df['close'],
            window=self.config.rsi_period
        )
        return rsi.rsi().iloc[-1]
        
    def calculate_macd(self, df: pd.DataFrame) -> dict:
        """Calculate MACD indicator"""
        macd = ta.trend.MACD(
            close=df['close'],
            window_slow=self.config.macd_slow,
            window_fast=self.config.macd_fast,
            window_sign=self.config.macd_signal
        )
        return {
            'macd': macd.macd().iloc[-1],
            'macd_signal': macd.macd_signal().iloc[-1],
            'macd_diff': macd.macd_diff().iloc[-1]
        }
        
    def calculate_bollinger_bands(self, df: pd.DataFrame) -> dict:
        """Calculate Bollinger Bands"""
        bb = ta.volatility.BollingerBands(
            close=df['close'],
            window=self.config.bb_period,
            window_dev=self.config.bb_std
        )
        return {
            'bb_high': bb.bollinger_hband().iloc[-1],
            'bb_mid': bb.bollinger_mavg().iloc[-1],
            'bb_low': bb.bollinger_lband().iloc[-1]
        }
        
    def calculate_volatility(self, df: pd.DataFrame, window: int = 20) -> float:
        """Calculate price volatility"""
        returns = df['close'].pct_change()
        return returns.std() * np.sqrt(252)  # Annualized volatility

// File: src\data\processor.py
from typing import Dict, List
import pandas as pd
import numpy as np

class DataProcessor:
    def __init__(self, config):
        self.config = config
        
    def preprocess_market_data(self, raw_data: Dict) -> Dict:
        """Process raw market data into standardized format"""
        processed_data = {
            'close_price': float(raw_data['last_price']),
            'volume': float(raw_data['volume_24h']),
            'bid_ask_spread': float(raw_data['best_ask']) - float(raw_data['best_bid']),
            'funding_rate': float(raw_data['funding_rate']),
            'open_interest': float(raw_data['open_interest']),
            'leverage_ratio': float(raw_data['leverage_ratio']),
            'market_depth_ratio': self._calculate_depth_ratio(raw_data),
            'taker_buy_ratio': float(raw_data['taker_buy_volume']) / 
                              (float(raw_data['taker_buy_volume']) + 
                               float(raw_data['taker_sell_volume']))
        }
        return processed_data

// File: src\data\__init__.py
from .processor import DataProcessor
from .features import FeatureCalculator

__all__ = ['DataProcessor', 'FeatureCalculator']

// File: src\data\raw\btc_usdt_1m_klines.csv
// Snippet:
                open_time    open    high     low   close  volume                       close_time  quote_asset_volume  number_of_trades  taker_buy_base_asset_volume  taker_buy_quote_asset_volume  ignore
2020-01-01 00:00:00+00:00 7189.43 7190.52 7177.00 7182.44 246.092 2020-01-01 00:00:59.999000+00:00        1.767430e+06               336                       46.630                  334813.19820       0
2020-01-01 00:01:00+00:00 7182.43 7182.44 7178.75 7179.01  70.909 2020-01-01 00:01:59.999000+00:00        5.091458e+05               140                       32.597                  234063.27884       0
2020-01-01 00:02:00+00:00 7179.01 7179.01 7175.25 7177.93  99.420 2020-01-01 00:02:59.999000+00:00        7.135396e+05               148                       16.311                  117066.92118       0
2020-01-01 00:03:00+00:00 7177.77 7182.60 7177.00 7181.11  69.330 2020-01-01 00:03:59.999000+00:00        4.977934e+05               104                       43.723                  313920.02981       0
2020-01-01 00:04:00+00:00 7179.10 7179.10 7172.94 7175.25  97.368 2020-01-01 00:04:59.999000+00:00        6.986274e+05               193                       36.616                  262734.68999       0

// File: src\env\bybit_env.py
import asyncio
import aiowebsocket
import json
import hmac
import hashlib
import time
from loguru import logger
from src.data.features import EnhancedFeatureExtractor
# ... other imports ...

BYBIT_WEBSOCKET_URL = "wss://stream-testnet.bybit.com/v5/public/linear" #Change to mainnet if needed

class BybitFuturesEnv(gym.Env):
    def __init__(self, config, api_key=None, api_secret=None):
        # ... other initializations ...
        self.config = config
        self.feature_extractor = EnhancedFeatureExtractor(config.feature)
        self.api_key = api_key
        self.api_secret = api_secret
        self.websocket = None
        self.market_data_queue = asyncio.Queue()
        self.last_ping_time = 0

    async def connect_websocket(self):
        """Establishes a websocket connection to Bybit, handling authentication if needed."""
        if self.api_key and self.api_secret:
            auth_data = self._generate_auth_data()
            url = f"{BYBIT_WEBSOCKET_URL}?max_active_time=1m" # Customize alive time
            async with aiowebsocket.connect(url) as ws:
                self.websocket = ws
                await self.websocket.send(json.dumps(auth_data))
                auth_response = await self.websocket.receive()
                if not self._check_auth_success(json.loads(auth_response)):
                    logger.error("Authentication failed!")
                    return
                await self.subscribe_to_channels()
                await self._websocket_loop()

        else:
            async with aiowebsocket.connect(BYBIT_WEBSOCKET_URL) as ws:
                self.websocket = ws
                await self.subscribe_to_channels()
                await self._websocket_loop()


    def _generate_auth_data(self):
      """Generates authentication data for private channels."""
      expires = int((time.time() + 60) * 1000)  # Expires in 60 seconds
      message = f"GET/realtime{expires}".encode('utf-8')
      signature = hmac.new(self.api_secret.encode('utf-8'), message, hashlib.sha256).hexdigest()
      return {
          "op": "auth",
          "args": [self.api_key, expires, signature]
      }

    def _check_auth_success(self, response):
        """Checks the authentication response from Bybit."""
        return response.get('retCode') == 0

    async def subscribe_to_channels(self):
        """Subscribes to the necessary Bybit websocket channels."""
        await self.websocket.send(json.dumps({"op": "subscribe", "args": ["instrument_info.100ms.BTCUSDT"]})) #Subscribe to BTCUSDT
        # Add other subscriptions as needed (e.g., trades, klines)

    async def _websocket_loop(self):
        """Main loop for receiving and processing websocket messages."""
        while True:
            try:
                message = await self.websocket.receive()
                await self.process_websocket_message(message)
                if time.time() - self.last_ping_time > 20:  #Send ping every 20 seconds
                  self.last_ping_time = time.time()
                  await self.websocket.send(json.dumps({"op": "ping"}))
                  pong_response = await asyncio.wait_for(self.websocket.receive(), timeout=5) #wait for pong
                  if pong_response != '"pong"': #check for successful pong
                    logger.warning("Ping timeout, attempting reconnect...")
                    await self.websocket.close()
                    await self.connect_websocket() #Attempt to reconnect
                    break

            except asyncio.TimeoutError:
                logger.warning("Websocket receive timeout")
                await self.websocket.close()
                await self.connect_websocket()
                break

            except aiowebsocket.exceptions.WebSocketError as e:
                logger.error(f"Websocket error: {e}")
                await asyncio.sleep(5)
                await self.websocket.close()
                await self.connect_websocket()
                break
            except Exception as e:
                logger.exception(f"An unexpected error occurred: {e}")
                await asyncio.sleep(5)
                await self.websocket.close()
                await self.connect_websocket()
                break



    async def process_websocket_message(self, message):
        """Processes a single message received from the Bybit websocket."""
        try:
            data = json.loads(message)
            if data['topic'] == 'instrument_info.100ms.BTCUSDT':
                await self.market_data_queue.put(self._parse_market_data(data['data']))

        except json.JSONDecodeError:
            logger.error("Invalid JSON received from websocket")
        except KeyError as e:
            logger.error(f"Missing key in websocket message: {e}")
        except Exception as e:
            logger.exception(f"Error processing websocket message: {e}")


    def _parse_market_data(self, raw_data):
        """Parses and processes a single market data point from Bybit's websocket.

        Args:
          raw_data (dict): A dictionary containing raw market data from Bybit's websocket.

        Returns:
          dict: A dictionary containing processed market data ready for feature extraction. Returns an empty dictionary if parsing fails.
        """
        try:
            processed_data = {
                'close_price': float(raw_data['last_price']),
                'volume': float(raw_data['volume']),
                'bid_ask_spread': float(raw_data['best_ask_price']) - float(raw_data['best_bid_price']),
                'funding_rate': float(raw_data['funding_rate']),
                'market_depth_ratio': self._calculate_depth_ratio(raw_data),  # Implement this function
                'taker_buy_ratio': float(raw_data.get('taker_buy_sell_ratio', 0.5)) # Handle missing data gracefully
            }
            return processed_data
        except (KeyError, ValueError, TypeError) as e:
            logger.warning(f"Error parsing market data: {e}, Raw Data: {raw_data}")
            return {}

    # ... rest of your BybitFuturesEnv class ...

    # ... (rest of your code) ...

// File: src\env\__init__.py


// File: src\models\agent.py


// File: src\models\appo.py
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.cuda.amp import autocast, GradScaler
from typing import Dict, List, Optional, Tuple
import numpy as np
from dataclasses import dataclass
from .nlnn import NLNN
from .reward import CompositeReward, RewardConfig
import threading
from queue import Queue
import time

@dataclass
class APPOConfig:
    """Configuration for APPO algorithm"""
    # Network architecture
    input_dim: int = 64
    hidden_dim: int = 256
    num_layers: int = 4
    
    # APPO parameters
    num_workers: int = 12  # Optimized for Ryzen 9 7900X
    batch_size: int = 512  # Optimized for RTX 3070
    clip_ratio: float = 0.2
    value_loss_coef: float = 0.5
    entropy_coef: float = 0.01
    max_grad_norm: float = 0.5
    
    # Learning rates
    actor_lr: float = 3e-4
    critic_lr: float = 1e-3
    
    # Optimization
    gamma: float = 0.99
    gae_lambda: float = 0.95
    update_epochs: int = 4
    mini_batch_size: int = 64
    
    # Experience replay
    buffer_size: int = 10000
    priority_alpha: float = 0.6
    priority_beta: float = 0.4
    
    # Auxiliary tasks
    use_auxiliary: bool = True
    aux_weight: float = 0.1

class AsyncPPOAgent(nn.Module):
    """Asynchronous Proximal Policy Optimization with hardware optimizations"""
    
    def __init__(self, config: APPOConfig):
        super().__init__()
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Initialize networks
        self.backbone = NLNN(
            input_size=config.input_dim,
            hidden_size=config.hidden_dim,
            num_layers=config.num_layers
        ).to(self.device)
        
        self.actor = nn.Sequential(
            nn.Linear(config.hidden_dim, config.hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(config.hidden_dim // 2, 3)  # Buy, Sell, Hold
        ).to(self.device)
        
        self.critic = nn.Sequential(
            nn.Linear(config.hidden_dim, config.hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(config.hidden_dim // 2, 1)
        ).to(self.device)
        
        if config.use_auxiliary:
            self.auxiliary_head = nn.Sequential(
                nn.Linear(config.hidden_dim, config.hidden_dim // 2),
                nn.ReLU(),
                nn.Linear(config.hidden_dim // 2, config.input_dim)
            ).to(self.device)
        
        # Initialize optimizers
        self.actor_optimizer = torch.optim.Adam(
            list(self.backbone.parameters()) + list(self.actor.parameters()),
            lr=config.actor_lr
        )
        self.critic_optimizer = torch.optim.Adam(
            list(self.backbone.parameters()) + list(self.critic.parameters()),
            lr=config.critic_lr
        )
        
        # Initialize mixed precision training
        self.scaler = GradScaler()
        
        # Initialize experience replay
        self.replay_buffer = PrioritizedReplayBuffer(
            config.buffer_size,
            config.priority_alpha,
            config.priority_beta
        )
        
        # Initialize worker threads
        self.workers = [
            PPOWorker(i, self, Queue())
            for i in range(config.num_workers)
        ]
        
        # Compile model for faster execution
        self.trace_model()
    
    def trace_model(self):
        """JIT compile the model for faster execution"""
        self.backbone = torch.compile(self.backbone)
        self.actor = torch.compile(self.actor)
        self.critic = torch.compile(self.critic)
        if self.config.use_auxiliary:
            self.auxiliary_head = torch.compile(self.auxiliary_head)
    
    @torch.compile
    def forward(self, state: torch.Tensor,
                hidden_states: Optional[List[torch.Tensor]] = None
                ) -> Tuple[torch.Tensor, torch.Tensor, List[torch.Tensor]]:
        """Forward pass with mixed precision"""
        with autocast():
            # Get features from backbone
            features, new_hidden = self.backbone(state, hidden_states)
            
            # Get action logits and value estimate
            action_logits = self.actor(features)
            value = self.critic(features)
            
            # Get auxiliary prediction if enabled
            if self.config.use_auxiliary:
                aux_pred = self.auxiliary_head(features)
            else:
                aux_pred = None
            
            return action_logits, value, new_hidden, aux_pred
    
    def select_action(self, state: torch.Tensor,
                     hidden_states: List[torch.Tensor],
                     deterministic: bool = False) -> Tuple[torch.Tensor, float, List[torch.Tensor]]:
        """Select action using the current policy"""
        with torch.no_grad():
            action_logits, value, new_hidden, _ = self(state, hidden_states)
            
            if deterministic:
                action = torch.argmax(action_logits, dim=-1)
            else:
                action_probs = F.softmax(action_logits, dim=-1)
                action = torch

    def update_minibatch(self, states: torch.Tensor,
                        actions: torch.Tensor,
                        returns: torch.Tensor,
                        advantages: torch.Tensor):
        """Update policy and value function on a single minibatch"""
        with autocast(enabled=self.config.use_amp):
            # Get current policy and value predictions
            new_log_probs, new_values, entropy = self.evaluate_actions(states, actions)
            
            # Calculate policy loss with clipping
            ratio = torch.exp(new_log_probs)
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1 - self.config.clip_ratio, 1 + self.config.clip_ratio) * advantages
            policy_loss = -torch.min(surr1, surr2).mean()
            
            # Calculate value loss with clipping
            value_pred_clipped = values + torch.clamp(
                new_values - values,
                -self.config.clip_ratio,
                self.config.clip_ratio
            )
            value_losses = (new_values - returns).pow(2)
            value_losses_clipped = (value_pred_clipped - returns).pow(2)
            value_loss = torch.max(value_losses, value_losses_clipped).mean()
            
            # Calculate auxiliary loss if enabled
            if self.config.use_auxiliary:
                aux_pred = self.auxiliary_head(features)
                aux_loss = F.mse_loss(aux_pred, next_states)
            else:
                aux_loss = 0
            
            # Combine losses
            total_loss = (
                policy_loss
                - self.config.entropy_coef * entropy
                + self.config.value_loss_coef * value_loss
                + self.config.aux_weight * aux_loss
            )
            
            # Optimize using mixed precision
            if self.config.use_amp:
                self.scaler.scale(total_loss).backward()
                
                # Clip gradients
                if self.config.max_grad_norm > 0:
                    for param_group in [
                        list(self.backbone.parameters()),
                        list(self.actor.parameters()),
                        list(self.critic.parameters())
                    ]:
                        self.scaler.unscale_(self.actor_optimizer)
                        torch.nn.utils.clip_grad_norm_(param_group, self.config.max_grad_norm)
                
                self.scaler.step(self.actor_optimizer)
                self.scaler.step(self.critic_optimizer)
                self.scaler.update()
            else:
                total_loss.backward()
                if self.config.max_grad_norm > 0:
                    for param_group in [
                        list(self.backbone.parameters()),
                        list(self.actor.parameters()),
                        list(self.critic.parameters())
                    ]:
                        torch.nn.utils.clip_grad_norm_(param_group, self.config.max_grad_norm)
                
                self.actor_optimizer.step()
                self.critic_optimizer.step()
            
            self.actor_optimizer.zero_grad()
            self.critic_optimizer.zero_grad()
            
            return {
                'policy_loss': policy_loss.item(),
                'value_loss': value_loss.item(),
                'entropy': entropy.item(),
                'aux_loss': aux_loss.item() if isinstance(aux_loss, torch.Tensor) else aux_loss
            }
    
    def train(self, num_steps: int):
        """Main training loop"""
        # Start worker threads
        for worker in self.workers:
            worker.start()
        
        total_steps = 0
        episode_rewards = []
        metrics_history = []
        
        try:
            while total_steps < num_steps:
                # Collect experiences from workers
                experiences = []
                while len(experiences) < self.config.batch_size:
                    if not self.experience_queue.empty():
                        exp = self.experience_queue.get()
                        experiences.append(exp)
                        total_steps += 1
                
                # Update policy
                metrics = self.update(experiences)
                metrics_history.append(metrics)
                
                # Log progress
                if total_steps % 1000 == 0:
                    avg_metrics = {
                        k: np.mean([m[k] for m in metrics_history[-1000:]])
                        for k in metrics_history[0].keys()
                    }
                    logger.info(f"Step {total_steps}/{num_steps} - Metrics: {avg_metrics}")
        
        finally:
            # Clean up workers
            for worker in self.workers:
                worker.join()
    
    def save_checkpoint(self, path: str):
        """Save model checkpoint"""
        torch.save({
            'backbone_state_dict': self.backbone.state_dict(),
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'auxiliary_state_dict': self.auxiliary_head.state_dict() if self.config.use_auxiliary else None,
            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
            'config': self.config,
            'scaler_state_dict': self.scaler.state_dict() if self.scaler else None
        }, path)
    
    def load_checkpoint(self, path: str):
        """Load model checkpoint"""
        checkpoint = torch.load(path, map_location=self.device)
        
        self.backbone.load_state_dict(checkpoint['backbone_state_dict'])
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])
        
        if self.config.use_auxiliary and checkpoint['auxiliary_state_dict']:
            self.auxiliary_head.load_state_dict(checkpoint['auxiliary_state_dict'])
        
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])
        
        if self.scaler and checkpoint['scaler_state_dict']:
            self.scaler.load_state_dict(checkpoint['scaler_state_dict'])

if __name__ == "__main__":
    # Example usage
    config = APPOConfig(
        input_dim=64,
        hidden_dim=256,
        num_layers=4,
        num_workers=12,  # Using all cores of Ryzen 9 7900X
        batch_size=512,  # Optimized for RTX 3070 8GB VRAM
        use_amp=True,    # Enable automatic mixed precision
        tensor_parallel=True  # Enable tensor parallelization
    )
    
    agent = AsyncPPOAgent(config)
    
    # Train the agent
    agent.train(num_steps=1000000)
    
    # Save checkpoint
    agent.save_checkpoint("appo_checkpoint.pt")


// File: src\models\nlnn.py
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import gymnasium as gym
import numpy as np
from dataclasses import dataclass
from typing import Tuple, Dict, List
import logging
import time
from collections import deque

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

@dataclass
class TradingConfig:
    """Configuration for trading parameters and risk management"""
    max_position_size: float = 0.1  # Maximum position size as fraction of capital
    stop_loss_pct: float = 0.02     # Stop loss percentage
    take_profit_pct: float = 0.04   # Take profit percentage
    max_leverage: float = 5.0       # Maximum allowed leverage
    min_trade_interval: int = 5     # Minimum intervals between trades
    rolling_window_size: int = 100  # Size of rolling window for feature calculation

class FeatureExtractor:
    """Extracts and normalizes trading features"""
    
    def __init__(self, config: TradingConfig):
        self.config = config
        self.price_history = deque(maxlen=config.rolling_window_size)
        self.volume_history = deque(maxlen=config.rolling_window_size)
        
    def calculate_features(self, market_data: Dict) -> np.ndarray:
        """Calculate normalized feature vector from market data"""
        # Add new data points
        self.price_history.append(market_data['close_price'])
        self.volume_history.append(market_data['volume'])
        
        if len(self.price_history) < 2:
            return np.zeros(10)  # Return zero features if insufficient history
            
        # Calculate technical features
        returns = np.diff(self.price_history) / np.array(list(self.price_history)[:-1])
        volatility = np.std(returns[-20:]) if len(returns) >= 20 else 0
        rsi = self._calculate_rsi(list(self.price_history))
        
        # Normalize and combine features
        features = np.array([
            self._normalize(market_data['close_price'], self.price_history),
            self._normalize(market_data['volume'], self.volume_history),
            volatility,
            rsi / 100.0,  # RSI is already 0-100
            market_data['bid_ask_spread'] / market_data['close_price'],
            market_data['funding_rate'],
            market_data['open_interest'] / np.mean(self.volume_history),
            market_data['leverage_ratio'] / self.config.max_leverage,
            market_data['market_depth_ratio'],
            market_data['taker_buy_ratio']
        ])
        
        return np.clip(features, -1, 1)  # Ensure all features are in [-1, 1]
    
    def _normalize(self, value: float, history: deque) -> float:
        """Min-max normalization using recent history"""
        if len(history) < 2:
            return 0
        min_val = min(history)
        max_val = max(history)
        if min_val == max_val:
            return 0
        return (value - min_val) / (max_val - min_val) * 2 - 1
    
    def _calculate_rsi(self, prices: List[float], periods: int = 14) -> float:
        """Calculate Relative Strength Index"""
        if len(prices) < periods + 1:
            return 50.0
            
        deltas = np.diff(prices)
        gains = np.clip(deltas, 0, None)
        losses = -np.clip(deltas, None, 0)
        
        avg_gain = np.mean(gains[-periods:])
        avg_loss = np.mean(losses[-periods:])
        
        if avg_loss == 0:
            return 100.0
            
        rs = avg_gain / avg_loss
        rsi = 100 - (100 / (1 + rs))
        return rsi

class RiskManager:
    """Handles position sizing and risk management"""
    
    def __init__(self, config: TradingConfig):
        self.config = config
        self.current_position = 0
        self.last_trade_time = 0
        
    def validate_action(self, action: int, current_price: float, 
                       account_balance: float) -> Tuple[bool, str]:
        """Validate if an action can be executed based on risk parameters"""
        current_time = time.time()
        
        # Check trading frequency
        if current_time - self.last_trade_time < self.config.min_trade_interval:
            return False, "Trading too frequently"
            
        # Check position sizes
        new_position = self._calculate_position_size(action, current_price, account_balance)
        if abs(new_position) > self.config.max_position_size * account_balance:
            return False, "Position size exceeds maximum"
            
        return True, ""
        
    def _calculate_position_size(self, action: int, current_price: float,
                               account_balance: float) -> float:
        """Calculate appropriate position size based on risk parameters"""
        base_size = account_balance * self.config.max_position_size
        
        # Scale position size based on market volatility (simplified)
        # In practice, you'd want more sophisticated volatility adjustment
        volatility_scalar = 0.5  # Could be dynamically calculated
        position_size = base_size * volatility_scalar
        
        # Adjust for action type (0: Buy, 1: Sell, 2: Hold)
        if action == 2:  # Hold
            return self.current_position
        elif action == 1:  # Sell
            return -position_size
        else:  # Buy
            return position_size

class N_LNN(nn.Module):
    """Enhanced n-LNN implementation with additional features"""
    
    def __init__(self, input_size: int, hidden_size: int, num_layers: int = 1):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # Input transformation
        self.input_transform = nn.Linear(input_size, hidden_size)
        
        # Learnable parameters
        self.W_i = nn.ParameterList([
            nn.Parameter(torch.randn(hidden_size, hidden_size)) 
            for _ in range(num_layers)
        ])
        self.W_r = nn.ParameterList([
            nn.Parameter(torch.randn(hidden_size, hidden_size))
            for _ in range(num_layers)
        ])
        
        # Eigen learning rates and scaling factors
        self.lambda_i = nn.ParameterList([
            nn.Parameter(torch.ones(hidden_size))
            for _ in range(num_layers)
        ])
        self.lambda_r = nn.ParameterList([
            nn.Parameter(torch.ones(hidden_size))
            for _ in range(num_layers)
        ])
        
        self.scaling_i = nn.Parameter(torch.ones(1))
        self.scaling_r = nn.Parameter(torch.ones(1))
        
        self.dropout = nn.Dropout(0.1)
        self.layer_norm = nn.LayerNorm(hidden_size)
        
    def normalize(self, v: torch.Tensor, epsilon: float = 1e-8) -> torch.Tensor:
        """Hypersphere normalization with numerical stability"""
        norm = torch.norm(v, dim=-1, keepdim=True)
        return v / (norm + epsilon)
    
    def slerp(self, h_t: torch.Tensor, h_new: torch.Tensor,
              alpha: float = 0.5) -> torch.Tensor:
        """Spherical linear interpolation with safety checks"""
        # Ensure inputs are normalized
        h_t = self.normalize(h_t)
        h_new = self.normalize(h_new)
        
        # Calculate cos_theta with numerical stability
        cos_theta = torch.sum(h_t * h_new, dim=-1, keepdim=True).clamp(-1 + 1e-7, 1 - 1e-7)
        theta = torch.acos(cos_theta)
        
        # Handle edge cases
        sin_theta = torch.sin(theta)
        mask = sin_theta.abs() > 1e-7
        
        result = torch.where(
            mask,
            ((torch.sin((1 - alpha) * theta) / sin_theta) * h_t) + 
            ((torch.sin(alpha * theta) / sin_theta) * h_new),
            h_t
        )
        
        return self.normalize(result)
    
    def forward(self, inputs: torch.Tensor, 
                hidden_states: List[torch.Tensor]) -> Tuple[torch.Tensor, List[torch.Tensor]]:
        """Forward pass with multi-layer support and residual connections"""
        # Input projection
        x = self.input_transform(inputs)
        x = self.layer_norm(x)
        
        new_hidden_states = []
        for layer in range(self.num_layers):
            h_t = hidden_states[layer]
            
            # Apply transformations
            i_transform = torch.matmul(x, self.W_i[layer]) * self.lambda_i[layer]
            r_transform = torch.matmul(h_t, self.W_r[layer]) * self.lambda_r[layer]
            
            # Combine transformations
            h_new = self.normalize(
                self.scaling_i * i_transform + 
                self.scaling_r * r_transform
            )
            
            # Apply SLERP
            h_new = self.slerp(h_t, h_new)
            
            # Residual connection if dimensions match
            if x.shape == h_new.shape:
                h_new = h_new + x
                
            h_new = self.layer_norm(h_new)
            h_new = self.dropout(h_new)
            
            new_hidden_states.append(h_new)
            x = h_new  # Use as input for next layer
            
        return x, new_hidden_states

class Agent(nn.Module):
    """Trading agent with actor-critic architecture"""
    
    def __init__(self, input_size: int, hidden_size: int, num_layers: int = 1):
        super().__init__()
        self.n_lnn = N_LNN(input_size, hidden_size, num_layers)
        
        # Actor network (policy)
        self.actor = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Linear(hidden_size // 2, 3),  # 3 actions: Buy, Sell, Hold
        )
        
        # Critic network (value function)
        self.critic = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Linear(hidden_size // 2, 1)
        )
        
        # Initialize weights
        self.apply(self._init_weights)
        
    def _init_weights(self, module):
        """Initialize network weights"""
        if isinstance(module, nn.Linear):
            nn.init.orthogonal_(module.weight.data, gain=np.sqrt(2))
            module.bias.data.zero_()
            
    def forward(self, state: torch.Tensor, hidden_states: List[torch.Tensor]
                ) -> Tuple[torch.Tensor, torch.Tensor, List[torch.Tensor]]:
        """Forward pass returning action probabilities and value estimate"""
        features, new_hidden = self.n_lnn(state, hidden_states)
        
        # Get action probabilities and value estimate
        action_logits = self.actor(features)
        value = self.critic(features)
        
        # Apply action masking if needed (e.g., prevent buying when already long)
        # action_mask = self._get_action_mask(state)
        # action_logits = action_logits.masked_fill(~action_mask, float('-inf'))
        
        return F.softmax(action_logits, dim=-1), value, new_hidden
    
    def act(self, state: torch.Tensor, hidden_states: List[torch.Tensor]
            ) -> Tuple[int, float, List[torch.Tensor]]:
        """Select action based on current policy"""
        with torch.no_grad():
            action_probs, value, new_hidden = self(state, hidden_states)
            action = torch.multinomial(action_probs, 1).item()
            
        return action, value.item(), new_hidden

# Example usage:
if __name__ == "__main__":
    # Configuration
    config = TradingConfig()
    feature_extractor = FeatureExtractor(config)
    risk_manager = RiskManager(config)
    
    # Initialize agent
    input_size = 10  # Number of features
    hidden_size = 64
    num_layers = 2
    agent = Agent(input_size, hidden_size, num_layers)
    optimizer = optim.Adam(agent.parameters(), lr=3e-4)
    
    # Initialize hidden states
    hidden_states = [
        torch.zeros(1, hidden_size) for _ in range(num_layers)
    ]
    
    # Example market data
    market_data = {
        'close_price': 50000.0,
        'volume': 100.0,
        'bid_ask_spread': 1.0,
        'funding_rate': 0.0001,
        'open_interest': 1000.0,
        'leverage_ratio': 2.0,
        'market_depth_ratio': 0.5,
        'taker_buy_ratio': 0.6
    }
    
    # Extract features
    features = feature_extractor.calculate_features(market_data)
    state = torch.FloatTensor(features).unsqueeze(0)
    
    # Get action
    action, value, new_hidden = agent.act(state, hidden_states)
    
    # Validate action
    is_valid, message = risk_manager.validate_action(
        action, market_data['close_price'], account_balance=10000.0
    )
    
    if is_valid:
        logging.info(f"Taking action {action} with value estimate {value}")
    else:
        logging.warning(f"Action rejected: {message}")

// File: src\models\reward.py
import torch
import numpy as np
from typing import Dict, List, Optional, Union
from dataclasses import dataclass
from abc import ABC, abstractmethod
import torch.nn.functional as F
from torch.cuda.amp import autocast

@dataclass
class RewardConfig:
    """Configuration for reward calculation"""
    # PnL Rewards
    pnl_scale: float = 1.0
    realized_pnl_weight: float = 0.7
    unrealized_pnl_weight: float = 0.3
    
    # Risk-Adjusted Rewards
    use_sharpe: bool = True
    sharpe_window: int = 100
    risk_free_rate: float = 0.0
    
    # Trading Behavior Rewards
    timing_scale: float = 0.3
    position_scale: float = 0.2
    spread_scale: float = 0.1
    
    # Penalties
    oversizing_penalty: float = -0.5
    overtrading_penalty: float = -0.3
    drawdown_penalty: float = -0.4
    
    # Exploration Rewards
    curiosity_scale: float = 0.1
    novelty_threshold: float = 0.1

class BaseReward(ABC):
    """Abstract base class for reward functions"""
    
    def __init__(self, config: RewardConfig):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    @abstractmethod
    def calculate(self, state: Dict[str, torch.Tensor], 
                 action: torch.Tensor, 
                 next_state: Dict[str, torch.Tensor]) -> torch.Tensor:
        pass

class PnLReward(BaseReward):
    """Profit and Loss based reward"""
    
    @torch.compile
    def calculate(self, state: Dict[str, torch.Tensor],
                 action: torch.Tensor,
                 next_state: Dict[str, torch.Tensor]) -> torch.Tensor:
        with autocast():
            realized_pnl = next_state['realized_pnl']
            unrealized_pnl = next_state['unrealized_pnl']
            
            pnl_reward = (
                self.config.realized_pnl_weight * realized_pnl +
                self.config.unrealized_pnl_weight * unrealized_pnl
            ) * self.config.pnl_scale
            
            return pnl_reward

class SharpeReward(BaseReward):
    """Sharpe ratio based reward"""
    
    def __init__(self, config: RewardConfig):
        super().__init__(config)
        self.returns_history = torch.zeros(
            (config.sharpe_window,),
            device=self.device
        )
        self.current_idx = 0
    
    @torch.compile
    def calculate(self, state: Dict[str, torch.Tensor],
                 action: torch.Tensor,
                 next_state: Dict[str, torch.Tensor]) -> torch.Tensor:
        with autocast():
            # Update returns history
            current_return = (
                next_state['total_pnl'] - state['total_pnl']
            ) / state['account_balance']
            
            self.returns_history[self.current_idx] = current_return
            self.current_idx = (self.current_idx + 1) % self.config.sharpe_window
            
            # Calculate Sharpe ratio
            if self.current_idx >= self.config.sharpe_window:
                excess_returns = self.returns_history - self.config.risk_free_rate
                sharpe = (
                    torch.mean(excess_returns) /
                    (torch.std(excess_returns) + 1e-7)
                ) * torch.sqrt(torch.tensor(252.0, device=self.device))  # Annualized
                
                return sharpe
            
            return torch.tensor(0.0, device=self.device)

class BehaviorReward(BaseReward):
    """Trading behavior based reward"""
    
    @torch.compile
    def calculate(self, state: Dict[str, torch.Tensor],
                 action: torch.Tensor,
                 next_state: Dict[str, torch.Tensor]) -> torch.Tensor:
        with autocast():
            # Timing reward based on price movement direction
            price_movement = next_state['close_price'] - state['close_price']
            timing_reward = torch.sign(price_movement) * torch.sign(action - 1)  # -1 for sell, 0 for hold, 1 for buy
            timing_reward *= self.config.timing_scale
            
            # Position sizing reward
            position_size = torch.abs(next_state['position_size'])
            optimal_size = state['account_balance'] * state['volatility']
            position_reward = -torch.abs(position_size - optimal_size) * self.config.position_scale
            
            # Spread cost penalty
            spread_cost = state['bid_ask_spread'] * torch.abs(action - 1)
            spread_penalty = -spread_cost * self.config.spread_scale
            
            return timing_reward + position_reward + spread_penalty

class RiskPenalty(BaseReward):
    """Risk-based penalties"""
    
    @torch.compile
    def calculate(self, state: Dict[str, torch.Tensor],
                 action: torch.Tensor,
                 next_state: Dict[str, torch.Tensor]) -> torch.Tensor:
        with autocast():
            penalties = torch.tensor(0.0, device=self.device)
            
            # Oversizing penalty
            max_position = state['account_balance'] * state['max_position_size']
            if torch.abs(next_state['position_size']) > max_position:
                penalties += self.config.oversizing_penalty
            
            # Overtrading penalty
            if state['trade_count'] > state['optimal_trade_count']:
                penalties += self.config.overtrading_penalty
            
            # Drawdown penalty
            current_drawdown = (state['peak_equity'] - next_state['account_balance']) / state['peak_equity']
            if current_drawdown > state['max_drawdown']:
                penalties += self.config.drawdown_penalty
            
            return penalties

class CuriosityReward(BaseReward):
    """Intrinsic motivation through curiosity"""
    
    def __init__(self, config: RewardConfig, state_dim: int):
        super().__init__(config)
        self.state_history = torch.zeros(
            (1000, state_dim),
            device=self.device
        )
        self.current_idx = 0
    
    @torch.compile
    def calculate(self, state: Dict[str, torch.Tensor],
                 action: torch.Tensor,
                 next_state: Dict[str, torch.Tensor]) -> torch.Tensor:
        with autocast():
            state_tensor = torch.cat([
                torch.tensor(list(state.values()), device=self.device)
            ])
            
            # Update state history
            self.state_history[self.current_idx] = state_tensor
            self.current_idx = (self.current_idx + 1) % 1000
            
            # Calculate novelty as distance to nearest neighbor
            distances = torch.norm(
                self.state_history - state_tensor.unsqueeze(0),
                dim=1
            )
            novelty = torch.min(distances[distances > 0])
            
            # Scale novelty reward
            novelty_reward = torch.where(
                novelty > self.config.novelty_threshold,
                self.config.curiosity_scale,
                torch.tensor(0.0, device=self.device)
            )
            
            return novelty_reward

class CompositeReward:
    """Combines multiple reward components"""
    
    def __init__(self, config: RewardConfig, state_dim: int):
        self.components = {
            'pnl': PnLReward(config),
            'sharpe': SharpeReward(config),
            'behavior': BehaviorReward(config),
            'risk': RiskPenalty(config),
            'curiosity': CuriosityReward(config, state_dim)
        }
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    @torch.compile
    def calculate_reward(self,
                        state: Dict[str, torch.Tensor],
                        action: torch.Tensor,
                        next_state: Dict[str, torch.Tensor]) -> torch.Tensor:
        """Calculate combined reward from all components"""
        with autocast():
            rewards = {}
            
            # Calculate rewards in parallel if on GPU
            if self.device.type == 'cuda':
                streams = [torch.cuda.Stream() for _ in self.components]
                
                for (name, component), stream in zip(self.components.items(), streams):
                    with torch.cuda.stream(stream):
                        rewards[name] = component.calculate(state, action, next_state)
                
                torch.cuda.synchronize()
            else:
                for name, component in self.components.items():
                    rewards[name] = component.calculate(state, action, next_state)
            
            # Combine rewards
            total_reward = sum(rewards.values())
            
            # Clip final reward for stability
            return torch.clamp(total_reward, -10.0, 10.0)

// File: src\models\__init__.py


// File: src\tests\__init__.py


// File: src\training\trainer.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import GradScaler, autocast
from torch.nn.utils import clip_grad_norm_
import numpy as np
from typing import Dict, List, Optional, Tuple
import time
from pathlib import Path
import wandb
from loguru import logger

from src.config.config import TradingConfig
from src.models.nlnn import ActorCritic
from src.data.features import FeatureExtractor
from src.env.bybit_env import BybitFuturesEnv
from src.utils.risk_manager import RiskManager

class Trainer:
    def __init__(self, config: TradingConfig):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Initialize components
        self.env = BybitFuturesEnv(config)
        self.feature_extractor = FeatureExtractor(config)
        self.risk_manager = RiskManager(config)
        
        # Initialize model and optimizer
        self.model = ActorCritic(
            input_size=config.model.feature_dim,
            hidden_size=config.model.hidden_size,
            num_layers=config.model.num_layers
        ).to(self.device)
        
        self.optimizer = optim.Adam(
            self.model.parameters(),
            lr=config.model.learning_rate
        )
        
        # Initialize mixed precision training
        self.scaler = GradScaler()
        
        # Initialize experience buffer
        self.buffer_size = config.training.update_interval
        self.clear_buffers()
        
        # Setup wandb logging
        wandb.init(
            project="nlnn-trading",
            config=config.dict(),
            name=f"train_{int(time.time())}"
        )
    
    def clear_buffers(self):
        """Reset experience buffers"""
        self.states = []
        self.actions = []
        self.rewards = []
        self.values = []
        self.log_probs = []
        self.hidden_states = []
        self.masks = []
    
    @torch.compile
    def compute_gae(self,
                    rewards: torch.Tensor,
                    values: torch.Tensor,
                    masks: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """Compute Generalized Advantage Estimation"""
        advantages = torch.zeros_like(rewards)
        returns = torch.zeros_like(rewards)
        running_return = 0
        running_advantage = 0
        
        for t in reversed(range(len(rewards))):
            if t < len(rewards) - 1:
                next_value = values[t + 1]
            else:
                next_value = 0
                
            running_return = rewards[t] + self.config.training.gamma * running_return * masks[t]
            running_tderror = (
                rewards[t] + 
                self.config.training.gamma * next_value * masks[t] - 
                values[t]
            )
            running_advantage = (
                running_tderror + 
                self.config.training.gamma * 
                self.config.training.gae_lambda * 
                running_advantage * 
                masks[t]
            )
            
            returns[t] = running_return
            advantages[t] = running_advantage
            
        return returns, advantages
    
    def train_step(self) -> Dict[str, float]:
        """Perform a single training step"""
        # Convert buffers to tensors
        states = torch.cat(self.states)
        actions = torch.tensor(self.actions, device=self.device)
        rewards = torch.tensor(self.rewards, device=self.device)
        old_values = torch.cat(self.values)
        old_log_probs = torch.cat(self.log_probs)
        masks = torch.tensor(self.masks, device=self.device)
        
        # Compute returns and advantages
        returns, advantages = self.compute_gae(rewards, old_values, masks)
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # Prepare batches
        batch_size = len(states) // self.config.training.num_minibatches
        indices = np.arange(len(states))
        
        # Training metrics
        metrics = {
            "value_loss": 0,
            "policy_loss": 0,
            "entropy_loss": 0,
            "total_loss": 0
        }
        
        # Perform multiple epochs of training
        for _ in range(self.config.training.num_epochs):
            np.random.shuffle(indices)
            
            for start in range(0, len(states), batch_size):
                end = start + batch_size
                batch_indices = indices[start:end]
                
                # Get batch data
                batch_states = states[batch_indices]
                batch_actions = actions[batch_indices]
                batch_returns = returns[batch_indices]
                batch_advantages = advantages[batch_indices]
                batch_old_values = old_values[batch_indices]
                batch_old_log_probs = old_log_probs[batch_indices]
                
                # Forwar

// File: src\utils\logging.py
from loguru import logger
import sys
from pathlib import Path
from datetime import datetime
import json
from typing import Dict, Any

class CustomLogger:
    def __init__(self, log_dir: str = "logs"):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)
        
        # Remove default logger and set up our custom configuration
        logger.remove()
        
        # Add file handler
        log_file = self.log_dir / f"trading_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        logger.add(
            log_file,
            format="{time:YYYY-MM-DD HH:mm:ss.SSS} | {level: <8} | {name}:{function}:{line} - {message}",
            level="INFO",
            rotation="1 day",
            compression="zip"
        )
        
        # Add console handler
        logger.add(
            sys.stdout,
            format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
            level="INFO"
        )
        
    def log_metrics(self, metrics: Dict[str, Any], step: int):
        """Log metrics to both file and wandb if configured"""
        metrics_str = json.dumps(metrics)
        logger.info(f"Step {step} metrics: {metrics_str}")

custom_logger = CustomLogger()  # Singleton instance

// File: src\utils\risk_manager.py
import numpy as np
from typing import Dict, Tuple, List
from dataclasses import dataclass
import threading
from concurrent.futures import ThreadPoolExecutor
import time
from collections import deque
from loguru import logger
import torch

@dataclass
class RiskConfig:
    max_position_size: float = 0.1
    stop_loss_pct: float = 0.02
    take_profit_pct: float = 0.04
    max_leverage: float = 5.0
    min_trade_interval: float = 1.0  # seconds
    max_drawdown: float = 0.2
    position_sizing_method: str = "adaptive_kelly"
    num_threads: int = 24  # Optimized for Ryzen 9 7900X
    risk_check_batch_size: int = 1000
    use_cuda: bool = True

class Position:
    def __init__(self, size: float, entry_price: float, entry_time: float):
        self.size = size
        self.entry_price = entry_price
        self.entry_time = entry_time
        self.stop_loss = None
        self.take_profit = None
        self.unrealized_pnl = 0.0
        self.realized_pnl = 0.0

class EnhancedRiskManager:
    """Hardware-optimized risk management system"""
    
    def __init__(self, config: RiskConfig):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() and config.use_cuda else "cpu")
        
        # Initialize thread pool for parallel processing
        self.thread_pool = ThreadPoolExecutor(max_workers=config.num_threads)
        
        # Position management
        self.positions: Dict[str, Position] = {}
        self.position_lock = threading.Lock()
        
        # Performance tracking
        self.trades_history = deque(maxlen=1000)
        self.pnl_history = deque(maxlen=1000)
        self.last_trade_time = 0
        
        # Risk metrics
        self.current_drawdown = 0.0
        self.peak_equity = 0.0
        self.win_rate = 0.5  # Initial estimate
        
        # Batch processing buffers
        self.pending_actions = []
        self.pending_prices = []
        self.pending_timestamps = []
        
        # CUDA streams for parallel computation
        self.streams = None
        if self.device.type == 'cuda':
            self.streams = [torch.cuda.Stream() for _ in range(2)]

    def validate_action_batch(self, 
                            actions: List[int],
                            prices: List[float],
                            account_balance: float) -> List[Tuple[bool, str]]:
        """Validate multiple actions in parallel"""
        if self.device.type == 'cuda':
            return self._validate_action_batch_gpu(actions, prices, account_balance)
        else:
            return self._validate_action_batch_cpu(actions, prices, account_balance)

    @torch.compile
    def _validate_action_batch_gpu(self,
                                 actions: List[int],
                                 prices: List[float],
                                 account_balance: float) -> List[Tuple[bool, str]]:
        """GPU-accelerated batch action validation"""
        with torch.cuda.amp.autocast():
            # Convert inputs to tensors
            actions_tensor = torch.tensor(actions, device=self.device)
            prices_tensor = torch.tensor(prices, device=self.device)
            
            # Calculate position sizes
            position_sizes = self._calculate_position_sizes_gpu(
                actions_tensor, prices_tensor, account_balance
            )
            
            # Check position limits
            position_valid = torch.abs(position_sizes) <= (
                self.config.max_position_size * account_balance
            )
            
            # Check leverage limits
            leverage = torch.abs(position_sizes * prices_tensor) / account_balance
            leverage_valid = leverage <= self.config.max_leverage
            
            # Check trading frequency
            current_time = time.time()
            time_valid = (current_time - self.last_trade_time) >= self.config.min_trade_interval
            
            # Combine all checks
            valid = position_valid & leverage_valid & torch.full_like(
                position_valid, time_valid, dtype=torch.bool
            )
            
            # Generate messages
            messages = []
            for i, (p_valid, l_valid, is_valid) in enumerate(
                zip(position_valid.cpu(), leverage_valid.cpu(), valid.cpu())
            ):
                if not is_valid:
                    if not time_valid:
                        messages.append("Trading too frequently")
                    elif not p_valid:
                        messages.append("Position size exceeds maximum")
                    elif not l_valid:
                        messages.append("Leverage exceeds maximum")
                else:
                    messages.append("")
            
            return list(zip(valid.cpu().tolist(), messages))

    def _validate_action_batch_cpu(self,
                                 actions: List[int],
                                 prices: List[float],
                                 account_balance: float) -> List[Tuple[bool, str]]:
        """CPU-based parallel action validation"""
        def validate_single(args):
            action, price = args
            return self._validate_single_action(action, price, account_balance)
        
        # Process validations in parallel
        with ThreadPoolExecutor(max_workers=self.config.num_threads) as executor:
            results = list(executor.map(validate_single, zip(actions, prices)))
        
        return results

    def _validate_single_action(self,
                              action: int,
                              current_price: float,
                              account_balance: float) -> Tuple[bool, str]:
        """Validate a single action with all risk checks"""
        current_time = time.time()
        
        # Check trading frequency
        if current_time - self.last_trade_time < self.config.min_trade_interval:
            return False, "Trading too frequently"
        
        # Calculate position size
        position_size = self._calculate_adaptive_position_size(
            action, current_price, account_balance
        )
        
        # Check position limits
        if abs(position_size) > self.config.max_position_size * account_balance:
            return False, "Position size exceeds maximum"
        
        # Check leverage
        leverage = abs(position_size * current_price) / account_balance
        if leverage > self.config.max_leverage:
            return False, "Leverage exceeds maximum"
        
        # Check drawdown
        if self.current_drawdown > self.config.max_drawdown:
            return False, "Maximum drawdown exceeded"
        
        return True, ""

    @torch.compile
    def _calculate_position_sizes_gpu(self,
                                    actions: torch.Tensor,
                                    prices: torch.Tensor,
                                    account_balance: float) -> torch.Tensor:
        """Calculate position sizes using GPU acceleration"""
        # Base position size
        base_size = account_balance * self.config.max_position_size
        
        # Calculate adaptive Kelly criterion
        kelly_fraction = self._calculate_kelly_fraction_gpu

// File: src\utils\system_monitor.py
import psutil
import GPUtil
from dataclasses import dataclass
from typing import List, Dict
import numpy as np
from loguru import logger

@dataclass
class SystemMetrics:
    cpu_percent: float
    memory_percent: float
    gpu_utilization: List[float]
    gpu_memory_utilization: List[float]

class SystemMonitor:
    def __init__(self):
        self.metrics_history: List[SystemMetrics] = []
        
    def get_metrics(self) -> SystemMetrics:
        """Get current system metrics"""
        try:
            # CPU and Memory
            cpu_percent = psutil.cpu_percent(interval=1)
            memory_percent = psutil.virtual_memory().percent
            
            # GPU metrics
            gpus = GPUtil.getGPUs()
            gpu_utilization = [gpu.load * 100 for gpu in gpus]
            gpu_memory_utilization = [gpu.memoryUtil * 100 for gpu in gpus]
            
            metrics = SystemMetrics(
                cpu_percent=cpu_percent,
                memory_percent=memory_percent,
                gpu_utilization=gpu_utilization,
                gpu_memory_utilization=gpu_memory_utilization
            )
            
            self.metrics_history.append(metrics)
            return metrics
            
        except Exception as e:
            logger.error(f"Error getting system metrics: {str(e)}")
            return SystemMetrics(0.0, 0.0, [], [])
            
    def get_metrics_summary(self) -> Dict[str, float]:
        """Get summary statistics of system metrics"""
        if not self.metrics_history:
            return {}
            
        metrics_array = np.array([
            [m.cpu_percent, m.memory_percent] +
            m.gpu_utilization +
            m.gpu_memory_utilization
            for m in self.metrics_history
        ])
        
        return {
            'cpu_mean': np.mean(metrics_array[:, 0]),
            'cpu_max': np.max(metrics_array[:, 0]),
            'memory_mean': np.mean(metrics_array[:, 1]),
            'memory_max': np.max(metrics_array[:, 1]),
            'gpu_util_mean': np.mean(metrics_array[:, 2:2+len(self.metrics_history[0].gpu_utilization)]),
            'gpu_memory_mean': np.mean(metrics_array[:, -len(self.metrics_history[0].gpu_memory_utilization):])
        }

// File: src\utils\__init__.py


