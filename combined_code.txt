// File: config\base_config.yaml
# File: config/base_config.yaml

defaults:
  - _self_
  - exchange: exchange_config
  - features: features_config
  - model: model_config
  - trading: trading_config
  - logging: logging_config
  - ray: ray_config

data:
  start_date: "2023-01-01"
  end_date: "2023-12-31"

agents:
  num_agents: 5

training:
  num_episodes: 1000

evaluation:
  num_episodes: 100

paths:
  model_save_path: "./models/saved/"


// File: config\exchange_config.yaml
# config/exchange_config.yaml
exchange:
  name: binance
  api_key: "${env:BINANCE_API_KEY}" # Load from environment variable
  api_secret: "${env:BINANCE_API_SECRET}" # Load from environment variable
  trading_pairs:
    - BTC/USDT
    - ETH/USDT
  timeframe: 1m # Default timeframe
  rate_limit: 1200

// File: config\features.yaml
# config/features.yaml

features:
  indicators:
    SMA:
      enabled: true
      timeperiod: 20
    EMA:
      enabled: true
      timeperiod: 20
    RSI:
      enabled: true
      timeperiod: 14
    MACD:
      enabled: true
      fastperiod: 12
      slowperiod: 26
      signalperiod: 9
    ATR:
      enabled: true
      timeperiod: 14
  custom_features:
    pct_change:
      enabled: true
      window: 1
    volatility:
      enabled: true
      window: 20
  feature_selection:
    enabled: true
    method: "SelectFromModel"  # Options: SelectFromModel, RFE, etc.
    threshold: 0.01
    max_features: 10


// File: config\model_config.yaml
# config/model_config.yaml

model:
  type: "LSTM"  # Options: LSTM, Transformer, etc.
  input_size: 10  # Number of input features
  hidden_size: 64
  num_layers: 2
  output_size: 3  # Actions: Hold, Buy, Sell
  learning_rate: 0.001
  optimizer: "Adam"  # Options: Adam, SGD, etc.
  loss_function: "MSE"  # Options: MSE, CrossEntropy, etc.
  batch_size: 32
  epochs: 50
  dropout: 0.2
  device: "cuda"  # Options: cuda, cpu


// File: config\secrets.env


// File: config\trading_config.yaml
# config/trading_config.yaml

trading:
  initial_balance: 10000.0  # Starting balance in USD
  max_trade_size: 0.1  # Maximum 10% of balance per trade
  risk_per_trade: 0.02  # 2% risk per trade
  transaction_fee: 0.001  # 0.1% fee per trade
  slippage: 0.0005  # 0.05% slippage
  leverage: 1  # No leverage by default
  stop_loss: 0.05  # 5% stop loss
  take_profit: 0.10  # 10% take profit
  trade_history_limit: 1000  # Number of past trades to keep


// File: docs\conf.py
# docs/conf.py

import os
import sys
sys.path.insert(0, os.path.abspath('../src'))

project = 'AI-Powered Crypto Trading Bot'
author = 'Your Name'
release = '0.1.0'

extensions = [
    'sphinx.ext.autodoc',
    'sphinx.ext.napoleon',
    'sphinx.ext.viewcode',
]

templates_path = ['_templates']
exclude_patterns = []

html_theme = 'sphinx_rtd_theme'
html_static_path = ['_static']


// File: docs\index.rst
.. AI-Powered Crypto Trading Bot documentation master file, created by
   sphinx-quickstart on Wed Apr 14 2021.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.

Welcome to AI-Powered Crypto Trading Bot's documentation!
==========================================================

Contents:

.. toctree::
   :maxdepth: 2
   :caption: Contents:

Introduction
Installation
Usage
Modules
API Reference

Indices and tables
==================

* :ref:`genindex`
* :ref:`modindex`
* :ref:`search`


// File: environments\crypto_trading_env.py
# File: environments/crypto_trading_env.py

import gym
from gym import spaces
import numpy as np
import pandas as pd
from typing import Dict, Any, Tuple
from rewards.reward_functions import RewardFunction

class CryptoTradingEnv(gym.Env):
    """Custom Gym environment for cryptocurrency trading."""

    def __init__(self, data: pd.DataFrame, processed_features: pd.DataFrame, reward_function: RewardFunction, **kwargs):
        super(CryptoTradingEnv, self).__init__()

        self.data = data
        self.processed_features = processed_features
        self.reward_function = reward_function
        self.initial_balance = kwargs.get('initial_balance', 10000)
        self.transaction_fee = kwargs.get('transaction_fee', 0.001)
        self.slippage = kwargs.get('slippage', 0.001)
        self.max_position = kwargs.get('max_position', 1.0)

        # Define action and observation space
        self.action_space = spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float32)
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(processed_features.shape[1] + 2,), dtype=np.float32)

        self.reset()

    def reset(self) -> np.ndarray:
        self.current_step = 0
        self.balance = self.initial_balance
        self.position = 0
        self.done = False
        return self._get_observation()

    def step(self, action: float) -> Tuple[np.ndarray, float, bool, Dict[str, Any]]:
        if self.done:
            raise RuntimeError("Episode is done")

        self.current_step += 1
        current_price = self._get_current_price()
        
        # Execute trade
        trade_amount = action * self.max_position
        if trade_amount > 0:  # Buy
            cost = trade_amount * current_price * (1 + self.slippage)
            if cost <= self.balance:
                self.balance -= cost * (1 + self.transaction_fee)
                self.position += trade_amount
        elif trade_amount < 0:  # Sell
            revenue = abs(trade_amount) * current_price * (1 - self.slippage)
            if abs(trade_amount) <= self.position:
                self.balance += revenue * (1 - self.transaction_fee)
                self.position -= abs(trade_amount)

        # Calculate reward
        next_price = self._get_next_price()
        reward = self.reward_function(
            state=self._get_observation(),
            action=action,
            reward=self._calculate_profit(),
            next_state=self._get_next_observation(),
            done=self.done
        )

        # Check if done
        self.done = self.current_step >= len(self.data) - 1

        return self._get_observation(), reward, self.done, self._get_info()

    def _get_observation(self) -> np.ndarray:
        features = self.processed_features.iloc[self.current_step].values
        return np.concatenate([features, [self.balance, self.position]])

    def _get_next_observation(self) -> np.ndarray:
        next_step = min(self.current_step + 1, len(self.data) - 1)
        features = self.processed_features.iloc[next_step].values
        return np.concatenate([features, [self.balance, self.position]])

    def _get_current_price(self) -> float:
        return self.data.iloc[self.current_step]['close']

    def _get_next_price(self) -> float:
        next_step = min(self.current_step + 1, len(self.data) - 1)
        return self.data.iloc[next_step]['close']

    def _calculate_profit(self) -> float:
        current_price = self._get_current_price()
        portfolio_value = self.balance + self.position * current_price
        return (portfolio_value - self.initial_balance) / self.initial_balance

    def _get_info(self) -> Dict[str, Any]:
        return {
            "balance": self.balance,
            "position": self.position,
            "current_price": self._get_current_price(),
            "profit": self._calculate_profit()
        }


// File: models\evaluator.py
# models/evaluator.py

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from typing import List

class Evaluator:
    """
    Evaluates trading strategies based on various performance metrics.
    """

    def __init__(self, trade_history: pd.DataFrame):
        """
        Initializes the evaluator with trade history data.

        Args:
            trade_history (pd.DataFrame): DataFrame containing trade details.
        """
        self.trade_history = trade_history

    def calculate_sharpe_ratio(self, risk_free_rate: float = 0.0) -> float:
        """
        Calculates the Sharpe Ratio of the trading strategy.

        Args:
            risk_free_rate (float): The risk-free rate. Defaults to 0.0.

        Returns:
            float: Sharpe Ratio.
        """
        returns = self.trade_history['returns']
        excess_returns = returns - risk_free_rate
        sharpe_ratio = excess_returns.mean() / excess_returns.std()
        return sharpe_ratio

    def calculate_max_drawdown(self) -> float:
        """
        Calculates the Maximum Drawdown of the trading strategy.

        Returns:
            float: Maximum Drawdown.
        """
        cumulative_returns = (1 + self.trade_history['returns']).cumprod()
        peak = cumulative_returns.expanding(min_periods=1).max()
        drawdown = (cumulative_returns - peak) / peak
        max_drawdown = drawdown.min()
        return max_drawdown

    def calculate_total_return(self) -> float:
        """
        Calculates the Total Return of the trading strategy.

        Returns:
            float: Total Return.
        """
        total_return = (self.trade_history['portfolio_value'].iloc[-1] / self.trade_history['portfolio_value'].iloc[0]) - 1
        return total_return

    def plot_equity_curve(self):
        """
        Plots the equity curve of the trading strategy.
        """
        cumulative_returns = (1 + self.trade_history['returns']).cumprod()
        plt.figure(figsize=(12, 6))
        plt.plot(cumulative_returns, label='Equity Curve')
        plt.xlabel('Trade Number')
        plt.ylabel('Cumulative Returns')
        plt.title('Equity Curve')
        plt.legend()
        plt.grid(True)
        plt.show()

    def plot_drawdown(self):
        """
        Plots the drawdown over time.
        """
        cumulative_returns = (1 + self.trade_history['returns']).cumprod()
        peak = cumulative_returns.expanding(min_periods=1).max()
        drawdown = (cumulative_returns - peak) / peak
        plt.figure(figsize=(12, 6))
        plt.plot(drawdown, label='Drawdown', color='red')
        plt.xlabel('Trade Number')
        plt.ylabel('Drawdown')
        plt.title('Drawdown Over Time')
        plt.legend()
        plt.grid(True)
        plt.show()

    def summary(self):
        """
        Prints a summary of key performance metrics.
        """
        sharpe = self.calculate_sharpe_ratio()
        max_dd = self.calculate_max_drawdown()
        total_ret = self.calculate_total_return()

        print(f"Sharpe Ratio: {sharpe:.2f}")
        print(f"Maximum Drawdown: {max_dd:.2%}")
        print(f"Total Return: {total_ret:.2%}")

# Example usage
# evaluator = Evaluator(trade_history=trade_history_df)
# evaluator.summary()
# evaluator.plot_equity_curve()
# evaluator.plot_drawdown()


// File: models\model.py
# File: models/model.py

import torch
import torch.nn as nn
import torch.nn.functional as F

class TradingModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):
        super(TradingModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, output_size)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        # Assuming x shape: (batch, seq_length, input_size)
        lstm_out, _ = self.lstm(x)
        # Taking the output of the last time step
        last_output = lstm_out[:, -1, :]
        out = self.dropout(last_output)
        out = self.fc(out)
        return out

    def get_action(self, state):
        with torch.no_grad():
            q_values = self(torch.FloatTensor(state).unsqueeze(0))
            return q_values.argmax().item()

    def update(self, state, action, reward, next_state, done):
        # Implement your update logic here (e.g., Q-learning updates)
        pass


// File: models\timesnet.py
import torch
import torch.nn as nn
import torch.nn.functional as F

class TimesBlock(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(TimesBlock, self).__init__()
        self.conv1 = nn.Conv1d(input_size, hidden_size, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)
        self.ln = nn.LayerNorm(hidden_size)

    def forward(self, x):
        residual = x
        x = self.conv1(x.transpose(1, 2)).transpose(1, 2)
        x = F.relu(x)
        x = self.conv2(x.transpose(1, 2)).transpose(1, 2)
        x = self.ln(x + residual)
        return x

class TimesNet(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(TimesNet, self).__init__()
        self.blocks = nn.ModuleList([TimesBlock(input_size if i == 0 else hidden_size, hidden_size) for i in range(num_layers)])
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        for block in self.blocks:
            x = block(x)
        x = x.mean(dim=1)  # Global average pooling
        return self.fc(x)

class TimesNetTradingModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(TimesNetTradingModel, self).__init__()
        self.timesnet = TimesNet(input_size, hidden_size, num_layers, output_size)

    def forward(self, x):
        return self.timesnet(x)

    def get_action(self, state):
        with torch.no_grad():
            q_values = self(torch.FloatTensor(state).unsqueeze(0))
            return q_values.argmax().item()

    def update(self, state, action, reward, next_state, done):
        # Implement your update logic here
        pass

// File: models\trading_model.py
# File: models/trading_model.py

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class TradingModel(nn.Module):
    def __init__(self, input_size: int, hidden_size: int, num_layers: int, output_size: int, dropout: float = 0.2):
        super(TradingModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, output_size)
        self.optimizer = torch.optim.Adam(self.parameters())
        self.criterion = nn.MSELoss()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        lstm_out, _ = self.lstm(x)
        return self.fc(lstm_out[:, -1, :])

    def get_action(self, state: np.ndarray) -> float:
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).unsqueeze(0)
            q_values = self(state_tensor)
            return q_values.squeeze().item()

    def update(self, state: np.ndarray, action: float, reward: float, next_state: np.ndarray, done: bool):
        state_tensor = torch.FloatTensor(state).unsqueeze(0).unsqueeze(0)
        next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0).unsqueeze(0)
        action_tensor = torch.FloatTensor([action])
        reward_tensor = torch.FloatTensor([reward])
        done_tensor = torch.FloatTensor([float(done)])

        q_values = self(state_tensor)
        next_q_values = self(next_state_tensor)

        target = reward_tensor + (1 - done_tensor) * 0.99 * torch.max(next_q_values)
        loss = self.criterion(q_values.squeeze(), target)

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        return loss.item()

// File: models\trainer.py
# models/trainer.py

import pytorch_lightning as pl
import torch
from torch.utils.data import DataLoader, Dataset
import pandas as pd  # Import pandas here
from src.data_loader import TradingDataset
from .model import TradingModel
from src.utils import get_logger
from src.feature_selection import FeatureSelector

logger = get_logger()

class TradingLitModel(pl.LightningModule):
    """
    PyTorch Lightning module for trading.
    """

    def __init__(self, model: TradingModel, learning_rate: float, loss_fn, optimizer_cls):
        super(TradingLitModel, self).__init__()
        self.model = model
        self.learning_rate = learning_rate
        self.loss_fn = loss_fn
        self.optimizer_cls = optimizer_cls

    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_idx):
        X, y = batch
        preds = self.forward(X)
        loss = self.loss_fn(preds, y)
        self.log('train_loss', loss)
        return loss

    def configure_optimizers(self):
        optimizer = self.optimizer_cls(self.model.parameters(), lr=self.learning_rate)
        return optimizer

def train_model(config, train_df: pd.DataFrame, target_df: pd.Series):
    """
    Trains the trading model using PyTorch Lightning.

    Args:
        config (dict): Configuration dictionary.
        train_df (pd.DataFrame): Training feature data.
        target_df (pd.Series): Training target data.
    """
    # Feature Selection
    feature_selector = FeatureSelector(threshold=config['feature_selection']['threshold'],
                                       max_features=config['feature_selection']['max_features'])
    X_selected = feature_selector.fit_transform(train_df, target_df)
    logger.info(f"Selected features: {X_selected.columns.tolist()}")

    # Dataset and DataLoader
    dataset = TradingDataset(X_selected, target_df)
    dataloader = DataLoader(dataset, batch_size=config['model']['batch_size'], shuffle=True)

    # Model
    model = TradingModel(input_size=X_selected.shape[1],
                         hidden_size=config['model']['hidden_size'],
                         output_size=config['model']['output_size'])
    
    # Lightning Module
    lit_model = TradingLitModel(model=model,
                                learning_rate=config['model']['learning_rate'],
                                loss_fn=torch.nn.MSELoss(),
                                optimizer_cls=torch.optim.Adam)

    # Trainer
    trainer = pl.Trainer(max_epochs=config['model']['epochs'],
                         gpus=1 if torch.cuda.is_available() else 0,
                         logger=True)

    # Train
    trainer.fit(lit_model, dataloader)

    # Save the trained model
    torch.save(model.state_dict(), config['model']['model_save_path'] + "trading_model.pth")
    logger.info("Model training completed and saved.")

# Example usage
# train_model(config, train_features, train_targets)


// File: models\__init__.py
# models/__init__.py

from .model import TradingModel
from .trainer import train_model
from .evaluator import Evaluator

__all__ = ['TradingModel', 'train_model', 'Evaluator']


// File: scripts\backtest.py
# scripts/backtest.py

import argparse
import pandas as pd
from src.data_acquisition import BinanceDataProvider
from src.feature_engineering import FeatureEngineer
from src.feature_selection import FeatureSelector
from src.trading import TradingExecutor
from models.evaluator import Evaluator
from src.rewards import ProfitReward, SharpeRatioReward
from src.utils import setup_logging, get_logger

def parse_args():
    parser = argparse.ArgumentParser(description="Backtest Trading Strategies")
    parser.add_argument('--symbol', type=str, default='BTC/USDT', help='Trading pair symbol')
    parser.add_argument('--timeframe', type=str, default='1h', help='Candlestick timeframe')
    parser.add_argument('--start_date', type=str, required=True, help='Start date (YYYY-MM-DD)')
    parser.add_argument('--end_date', type=str, required=True, help='End date (YYYY-MM-DD)')
    parser.add_argument('--reward_type', type=str, choices=['profit', 'sharpe'], default='profit', help='Type of reward function')
    return parser.parse_args()

def main():
    # Setup logging
    setup_logging()
    logger = get_logger()

    args = parse_args()

    # Initialize Data Provider
    data_provider = BinanceDataProvider(api_key='YOUR_API_KEY', api_secret='YOUR_API_SECRET')
    logger.info(f"Fetching historical data for {args.symbol} from {args.start_date} to {args.end_date}")
    df = data_provider.get_data(symbol=args.symbol, timeframe=args.timeframe, start_date=args.start_date, end_date=args.end_date)

    # Feature Engineering
    feature_engineer = FeatureEngineer()
    df = feature_engineer.add_technical_indicators(df)
    df = feature_engineer.add_custom_features(df)
    df = df.dropna()

    # Feature Selection
    feature_selector = FeatureSelector(threshold=0.01, max_features=10)
    target = (df['close'].shift(-1) > df['close']).astype(int).fillna(0).astype(int)
    X_selected = feature_selector.fit_transform(df[['SMA_20', 'EMA', 'RSI', 'MACD', 'ATR', 'pct_change', 'volatility']], target)

    # Initialize Reward Function
    if args.reward_type == 'profit':
        reward_function = ProfitReward()
    else:
        reward_function = SharpeRatioReward()

    # Initialize Trading Executor
    trading_executor = TradingExecutor(initial_balance=10000.0, transaction_fee=0.001)
    trade_history = trading_executor.execute_backtest(df, X_selected, target, reward_function)

    # Evaluate Performance
    evaluator = Evaluator(trade_history)
    evaluator.summary()
    evaluator.plot_equity_curve()
    evaluator.plot_drawdown()

if __name__ == "__main__":
    main()


// File: scripts\download_data.py
# scripts/download_data.py

import argparse
from src.data_acquisition import BinanceDataProvider
from src.feature_engineering import FeatureEngineer
import pandas as pd
from src.utils import setup_logging, get_logger

def parse_args():
    parser = argparse.ArgumentParser(description="Download Historical Data")
    parser.add_argument('--symbol', type=str, default='BTC/USDT', help='Trading pair symbol')
    parser.add_argument('--timeframe', type=str, default='1h', help='Candlestick timeframe')
    parser.add_argument('--start_date', type=str, required=True, help='Start date (YYYY-MM-DD)')
    parser.add_argument('--end_date', type=str, required=True, help='End date (YYYY-MM-DD)')
    parser.add_argument('--output', type=str, default='data/raw/', help='Output directory for raw data')
    return parser.parse_args()

def main():
    # Setup logging
    setup_logging()
    logger = get_logger()

    args = parse_args()

    # Initialize Data Provider
    data_provider = BinanceDataProvider(api_key='YOUR_API_KEY', api_secret='YOUR_API_SECRET')

    # Fetch data
    logger.info(f"Downloading data for {args.symbol} from {args.start_date} to {args.end_date}")
    df = data_provider.get_data(symbol=args.symbol, timeframe=args.timeframe, start_date=args.start_date, end_date=args.end_date)

    # Save raw data
    output_file = f"{args.output}{args.symbol.replace('/', '_')}_{args.timeframe}_{args.start_date}_{args.end_date}.csv"
    df.to_csv(output_file, index=False)
    logger.info(f"Raw data saved to {output_file}")

    # Feature Engineering
    feature_engineer = FeatureEngineer()
    df = feature_engineer.add_technical_indicators(df)
    df = feature_engineer.add_custom_features(df)
    df = df.dropna()

    # Save processed data
    processed_file = f"data/processed/{args.symbol.replace('/', '_')}_{args.timeframe}_{args.start_date}_{args.end_date}_processed.csv"
    df.to_csv(processed_file, index=False)
    logger.info(f"Processed data saved to {processed_file}")

if __name__ == "__main__":
    main()


// File: scripts\preprocess_data.py
# scripts/preprocess_data.py

import argparse
import pandas as pd
from src.feature_engineering import FeatureEngineer
from src.feature_selection import FeatureSelector
from src.utils import setup_logging, get_logger

def parse_args():
    parser = argparse.ArgumentParser(description="Preprocess Raw Data")
    parser.add_argument('--input', type=str, required=True, help='Input raw data CSV file')
    parser.add_argument('--output', type=str, default='data/processed/', help='Output directory for processed data')
    parser.add_argument('--reward_type', type=str, choices=['profit', 'sharpe'], default='profit', help='Type of reward function for feature selection')
    return parser.parse_args()

def main():
    # Setup logging
    setup_logging()
    logger = get_logger()

    args = parse_args()

    # Load raw data
    logger.info(f"Loading raw data from {args.input}")
    df = pd.read_csv(args.input)
    df['timestamp'] = pd.to_datetime(df['timestamp'])

    # Feature Engineering
    feature_engineer = FeatureEngineer()
    df = feature_engineer.add_technical_indicators(df)
    df = feature_engineer.add_custom_features(df)
    df = df.dropna()
    logger.info("Feature engineering completed.")

    # Feature Selection
    feature_selector = FeatureSelector(threshold=0.01, max_features=10)
    
    # Define target variable based on reward type
    if args.reward_type == 'profit':
        target = (df['close'].shift(-1) > df['close']).astype(int).fillna(0).astype(int)
    else:
        # For Sharpe ratio, define a continuous target based on returns
        target = df['close'].pct_change().fillna(0)

    X = df[['SMA_20', 'EMA', 'RSI', 'MACD', 'ATR', 'pct_change', 'volatility']]
    X_selected = feature_selector.fit_transform(X, target)
    logger.info(f"Selected features: {X_selected.columns.tolist()}")

    # Save processed data
    processed_file = f"{args.output}{args.input.split('/')[-1].replace('.csv', '_processed.csv')}"
    processed_df = X_selected.copy()
    processed_df['target'] = target
    processed_df.to_csv(processed_file, index=False)
    logger.info(f"Processed data saved to {processed_file}")

if __name__ == "__main__":
    main()


// File: scripts\run.py
# scripts/run.py

import hydra
from omegaconf import DictConfig
import logging

from src.utils import setup_logging, get_logger
from src.data_acquisition import BinanceDataProvider
from src.feature_engineering import FeatureEngineer
from src.feature_selection import FeatureSelector
from src.feature_store import FeatureStore
from src.trading import TradingExecutor
from src.rewards import ProfitReward, SharpeRatioReward
from models.trainer import TradingLitModel
from models.model import TradingModel  # Corrected import
from models.evaluator import Evaluator
import pandas as pd
import torch
from torch.utils.data import DataLoader
import pytorch_lightning as pl
from src.data_loader import TradingDataset



@hydra.main(config_path="../config", config_name="base_config", version_base=None)
def main(cfg: DictConfig):
    # Setup logging (using Hydra's logging)
    setup_logging(log_level=cfg.base.log_level, log_file=f"logs/trading_bot_{cfg.base.mode}.log")
    logger = get_logger()
    logger.info(f"Running in {cfg.base.mode} mode")

    # Access configurations
    api_key = cfg.exchange.api_key
    api_secret = cfg.exchange.api_secret
    timeframe = cfg.exchange.timeframe
    learning_rate = cfg.model.learning_rate
    # ... other config access ...

    # Initialize Data Provider
    data_provider = BinanceDataProvider(api_key=api_key, api_secret=api_secret)
    logger.info(f"Fetching historical data...")  # More general message

    # Fetch data (adapt for multiple pairs if needed)
    symbol = cfg.exchange.trading_pairs[0] #  Get the first trading pair
    df = data_provider.get_data(symbol=symbol, timeframe=timeframe, start_date='2023-01-01', end_date='2023-12-31')

    # Feature Engineering
    feature_engineer = FeatureEngineer()
    df = feature_engineer.add_technical_indicators(df)
    df = feature_engineer.add_custom_features(df)
    df = df.dropna()
    logger.info("Feature engineering completed.")


    # Feature Selection
    feature_selector = FeatureSelector(threshold=cfg.features.feature_selection.threshold,
                                       max_features=cfg.features.feature_selection.max_features)
    
    # Define your target variable (you'll need to adjust this based on your reward function and strategy)
    target = (df['close'].shift(-1) > df['close']).astype(int).fillna(0).astype(int)  # Example target: predict price increase/decrease
    
    # Select relevant features for feature selection (important: include the raw price data if your model needs it)
    feature_columns = ['open', 'high', 'low', 'close', 'volume', 'SMA_20', 'EMA', 'RSI', 'MACD', 'ATR', 'pct_change', 'volatility']
    X = df[feature_columns]  # These are the features you will use for training
    
    X_selected = feature_selector.fit_transform(X, target) # Perform feature selection
    logger.info(f"Selected features: {X_selected.columns.tolist()}")



    if cfg.base.mode == 'train':
        # === Training Logic ===

        # Initialize Model (Using config parameters)
        model = TradingModel(input_size=X_selected.shape[1],
                             hidden_size=cfg.model.hidden_size,
                             num_layers=cfg.model.num_layers,
                             output_size=cfg.model.output_size,
                             dropout=cfg.model.dropout)

        # Initialize Trainer
        trainer = pl.Trainer(max_epochs=cfg.model.epochs,
                             gpus=1 if torch.cuda.is_available() and cfg.model.device == 'cuda' else 0,
                             logger=True)

        # Prepare Dataset and DataLoader (using TradingDataset)
        dataset = TradingDataset(X_selected, target)
        dataloader = DataLoader(dataset, batch_size=cfg.model.batch_size, shuffle=True)

        # Initialize Lightning Module
        lit_model = TradingLitModel(model=model,
                                    learning_rate=cfg.model.learning_rate,
                                    loss_fn=torch.nn.MSELoss(),
                                    optimizer_cls=getattr(torch.optim, cfg.model.optimizer))

        # Train
        logger.info("Starting model training...")
        trainer.fit(lit_model, dataloader)
        logger.info("Model training completed.")

        # Save the trained model
        torch.save(model.state_dict(), cfg.base.model_save_path + "trading_model.pth")
        logger.info(f"Model saved to {cfg.base.model_save_path}trading_model.pth")



    elif cfg.base.mode == 'test':
        # === Testing Logic ===
        # Load trained model
        model = TradingModel(input_size=X_selected.shape[1],  # Use the same input size as training!
                             hidden_size=cfg.model.hidden_size,
                             num_layers=cfg.model.num_layers,
                             output_size=cfg.model.output_size,
                             dropout=cfg.model.dropout) # VERY IMPORTANT to include dropout here too!


        model.load_state_dict(torch.load(cfg.base.model_save_path + "trading_model.pth"))
        model.eval()  # Set the model to evaluation mode
        logger.info("Trained model loaded.")

        # Initialize Reward Function (Example: ProfitReward)
        reward_function = ProfitReward() # Or SharpeRatioReward(), etc.


        # Initialize Trading Executor
        trading_executor = TradingExecutor(initial_balance=cfg.trading.initial_balance,
                                           transaction_fee=cfg.trading.transaction_fee,
                                           slippage=cfg.trading.slippage)


        # Execute Backtesting (You'll likely want to use a separate dataset for testing)
        # Make sure 'df' below is your TEST data, not the training data used above.
        trade_history = trading_executor.execute_backtest(df, X_selected, target, reward_function)



        #Evaluate Performance
        evaluator = Evaluator(trade_history)
        evaluator.summary()
        evaluator.plot_equity_curve()
        evaluator.plot_drawdown()






if __name__ == "__main__":
    main()

// File: scripts\tune_hyperparameters.py
# scripts/tune_hyperparameters.py

import argparse
import optuna  # Make sure Optuna is installed: pip install optuna
from models.trainer import train_model
from src.data_acquisition import BinanceDataProvider
from src.feature_engineering import FeatureEngineer
from src.feature_selection import FeatureSelector
from src.trading import TradingExecutor
from src.rewards import ProfitReward, SharpeRatioReward
from models.evaluator import Evaluator
from src.utils import setup_logging, get_logger
import pandas as pd

# *** Add these imports ***
from models.model import TradingModel
from models.trainer import TradingLitModel
import torch
from torch.utils.data import DataLoader
import pytorch_lightning as pl


def parse_args():
    parser = argparse.ArgumentParser(description="Hyperparameter Tuning with Optuna")
    parser.add_argument('--symbol', type=str, default='BTC/USDT', help='Trading pair symbol')
    parser.add_argument('--timeframe', type=str, default='1h', help='Candlestick timeframe')
    parser.add_argument('--start_date', type=str, required=True, help='Start date (YYYY-MM-DD)')
    parser.add_argument('--end_date', type=str, required=True, help='End date (YYYY-MM-DD)')
    return parser.parse_args()

def objective(trial, cfg):
    # Initialize Data Provider
    data_provider = BinanceDataProvider(api_key=cfg.exchange.api_key, api_secret=cfg.exchange.api_secret)
    df = data_provider.get_data(symbol=cfg.exchange.trading_pairs[0], timeframe=cfg.exchange.timeframe,
                                start_date=cfg.start_date, end_date=cfg.end_date)

    # Feature Engineering
    feature_engineer = FeatureEngineer()
    df = feature_engineer.add_technical_indicators(df)
    df = feature_engineer.add_custom_features(df)
    df = df.dropna()

    # Feature Selection
    feature_selector = FeatureSelector(threshold=0.01, max_features=10)
    target = (df['close'].shift(-1) > df['close']).astype(int).fillna(0).astype(int)
    X = df[['SMA_20', 'EMA', 'RSI', 'MACD', 'ATR', 'pct_change', 'volatility']]
    X_selected = feature_selector.fit_transform(X, target)

    # Define hyperparameters to tune
    hidden_size = trial.suggest_int('hidden_size', 32, 256)
    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)
    num_layers = trial.suggest_int('num_layers', 1, 4)

    # Initialize Model
    model = TradingModel(input_size=X_selected.shape[1],
                         hidden_size=hidden_size,
                         output_size=3)

    # Initialize Lightning Module
    lit_model = TradingLitModel(model=model,
                                learning_rate=learning_rate,
                                loss_fn=torch.nn.MSELoss(),
                                optimizer_cls=torch.optim.Adam)

    # Prepare Dataset and DataLoader
    from models.trainer import TradingDataset
    dataset = TradingDataset(X_selected, target)
    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

    # Initialize Trainer
    trainer = pl.Trainer(max_epochs=10, gpus=1 if torch.cuda.is_available() else 0, logger=False)

    # Train
    trainer.fit(lit_model, dataloader)

    # Evaluate on validation set (split data accordingly)
    # For simplicity, using the same data
    preds = model(torch.tensor(X_selected.values, dtype=torch.float32))
    preds = torch.argmax(preds, dim=1).numpy()
    accuracy = (preds == target.values).mean()

    return accuracy

def main():
    # Setup logging
    setup_logging()
    logger = get_logger()

    args = parse_args()

    # Configuration dictionary (could be loaded from a file or defined here)
    cfg = {
        'exchange': {
            'api_key': 'YOUR_API_KEY',
            'api_secret': 'YOUR_API_SECRET',
            'trading_pairs': [args.symbol],
            'timeframe': args.timeframe
        },
        'start_date': args.start_date,
        'end_date': args.end_date
    }

    study = optuna.create_study(direction='maximize')
    study.optimize(lambda trial: objective(trial, cfg), n_trials=50)

    logger.info("Best trial:")
    trial = study.best_trial

    logger.info(f"  Value: {trial.value}")
    logger.info("  Params: ")
    for key, value in trial.params.items():
        logger.info(f"    {key}: {value}")

    # Save study results
    study.trials_dataframe().to_csv("optuna_study_results.csv")
    logger.info("Study results saved to optuna_study_results.csv")

if __name__ == "__main__":
    main()


// File: scripts\__init__.py
# scripts/__init__.py

# This file can be left empty or include package-level imports if necessary.


// File: src\agent_manager.py
# File: agents/agent_manager.py

import ray
from typing import List, Dict, Any
import numpy as np
from environments.crypto_trading_env import CryptoTradingEnv
from models.trading_model import TradingModel
from rewards.reward_functions import RewardFunction
from utils.logging_config import get_logger

logger = get_logger(__name__)

@ray.remote
class TradingAgent:
    def __init__(self, env: CryptoTradingEnv, model: TradingModel, reward_function: RewardFunction, agent_id: str):
        self.env = env
        self.model = model
        self.reward_function = reward_function
        self.agent_id = agent_id
        self.performance_history = []

    def train(self, num_episodes: int) -> Dict[str, Any]:
        """
        Train the agent for a specified number of episodes.

        Args:
            num_episodes (int): The number of training episodes.

        Returns:
            Dict[str, Any]: A dictionary containing the agent's ID, average reward, and performance history.
        """
        total_reward = 0
        for episode in range(num_episodes):
            state = self.env.reset()
            done = False
            episode_reward = 0

            while not done:
                action = self.model.get_action(state)
                next_state, reward, done, _ = self.env.step(action)
                adjusted_reward = self.reward_function(state, action, reward, next_state, done)
                self.model.update(state, action, adjusted_reward, next_state, done)
                episode_reward += adjusted_reward
                state = next_state

            self.performance_history.append(episode_reward)
            total_reward += episode_reward
            logger.info(f"Agent {self.agent_id} - Episode {episode + 1}/{num_episodes}, Reward: {episode_reward:.2f}")

        average_reward = total_reward / num_episodes
        return {"agent_id": self.agent_id, "average_reward": average_reward, "performance_history": self.performance_history}

    def act(self, state: np.ndarray) -> int:
        """Get an action for the given state."""
        return self.model.get_action(state)

    def evaluate(self, num_episodes: int) -> Dict[str, float]:
        """Evaluate the agent's performance."""
        total_reward = 0
        for _ in range(num_episodes):
            state = self.env.reset()
            done = False
            while not done:
                action = self.act(state)
                state, reward, done, _ = self.env.step(action)
                total_reward += reward
        return {"agent_id": self.agent_id, "average_reward": total_reward / num_episodes}

    def save(self, path: str) -> None:
        """Save the agent's model and performance history."""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'performance_history': self.performance_history
        }, f"{path}/{self.agent_id}.pth")

    def load(self, path: str) -> None:
        """Load the agent's model and performance history."""
        checkpoint = torch.load(f"{path}/{self.agent_id}.pth")
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.performance_history = checkpoint['performance_history']

class AgentManager:
    def __init__(self, environments: Dict[str, CryptoTradingEnv], agent_configs: List[Dict[str, Any]]):
        self.environments = environments
        self.agents = [
            TradingAgent.remote(
                env=environments[list(environments.keys())[i % len(environments)]],
                model=config['model'],
                reward_function=config['reward_function'],
                agent_id=config['agent_id']
            ) for i, config in enumerate(agent_configs)
        ]

    def train_agents(self, num_episodes: int) -> List[Dict[str, Any]]:
        """Train all agents for a specified number of episodes."""
        try:
            results = ray.get([agent.train.remote(num_episodes) for agent in self.agents])
            return results
        except Exception as e:
            logger.error(f"Error during agent training: {str(e)}")
            return []

    def evaluate_agents(self, num_episodes: int) -> List[Dict[str, float]]:
        """Evaluate all agents on unseen data."""
        try:
            results = ray.get([agent.evaluate.remote(num_episodes) for agent in self.agents])
            return results
        except Exception as e:
            logger.error(f"Error during agent evaluation: {str(e)}")
            return []

    def get_best_agent(self) -> str:
        """Determine the best agent based on average reward."""
        results = self.evaluate_agents(num_episodes=100)
        best_agent = max(results, key=lambda x: x['average_reward'])
        return best_agent['agent_id']

    def ensemble_prediction(self, state: np.ndarray) -> int:
        """Make a prediction using an ensemble of all agents."""
        predictions = ray.get([agent.act.remote(state) for agent in self.agents])
        return int(np.argmax(np.bincount(predictions)))

    def save_agents(self, directory: str) -> None:
        """Save all agents to the specified directory."""
        ray.get([agent.save.remote(directory) for agent in self.agents])

    def load_agents(self, directory: str) -> None:
        """Load all agents from the specified directory."""
        ray.get([agent.load.remote(directory) for agent in self.agents])

// File: src\data_acquisition.py
# File: src/data_acquisition.py

from abc import ABC, abstractmethod
import ccxt
import pandas as pd

class DataProvider(ABC):
    """Abstract base class for data providers."""

    @abstractmethod
    def get_data(self, symbol: str, timeframe: str, start_date: str, end_date: str) -> pd.DataFrame:
        """
        Retrieves historical data for a given symbol and timeframe.
        """
        pass

class BinanceDataProvider(DataProvider):
    """Data provider for Binance exchange."""

    def __init__(self, api_key: str, api_secret: str):
        self.exchange = ccxt.binance({
            'apiKey': api_key,
            'secret': api_secret,
        })

    def get_data(self, symbol: str, timeframe: str, start_date: str, end_date: str) -> pd.DataFrame:
        # Fetch data from Binance API
        data = self.exchange.fetch_ohlcv(symbol, timeframe, since=start_date, limit=1000)  # Adjust limit as needed

        # Convert to pandas DataFrame
        df = pd.DataFrame(data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')

        return df

# Example usage
# data_provider = BinanceDataProvider(api_key="YOUR_API_KEY", api_secret="YOUR_API_SECRET")
# df = data_provider.get_data(symbol="BTC/USDT", timeframe="1h", start_date="2023-01-01", end_date="2023-01-31")

// File: src\data_loader.py
# src/data_loader.py

import torch
from torch.utils.data import Dataset, DataLoader
import pandas as pd

class TradingDataset(Dataset):
    """
    PyTorch Dataset for trading data.
    """

    def __init__(self, features: pd.DataFrame, targets: pd.Series):
        self.X = torch.tensor(features.values, dtype=torch.float32)
        self.y = torch.tensor(targets.values, dtype=torch.float32)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# Example usage
# dataset = TradingDataset(X_selected, target)
# dataloader = DataLoader(dataset, batch_size=32, shuffle=True)


// File: src\experiment_manager.py
# src/experiment_manager.py

import mlflow
import optuna
from src.agent_manager import AgentManager, TradingAgent
from environments.crypto_trading_env import CryptoTradingEnv
from models.model import TradingModel
from src.rewards import ProfitReward, SharpeRatioReward
from src.data_acquisition import BinanceDataProvider
from src.utils import get_logger
import torch

logger = get_logger()

class ExperimentManager:
    """
    Manages experiment tracking and hyperparameter optimization.
    """

    def __init__(self, experiment_name: str):
        mlflow.set_experiment(experiment_name)

    def run_experiment(self, params: dict, trade_history: pd.DataFrame):
        with mlflow.start_run():
            # Log parameters
            mlflow.log_params(params)

            # Initialize components based on params
            data_provider = BinanceDataProvider(api_key=params['api_key'], api_secret=params['api_secret'])
            reward_function = ProfitReward() if params['reward_type'] == 'profit' else SharpeRatioReward()
            env = CryptoTradingEnv(data_provider=data_provider, reward_function=reward_function, initial_balance=params['initial_balance'])
            model = TradingModel(input_size=env.observation_space.shape[0],
                                 hidden_size=params['hidden_size'],
                                 output_size=env.action_space.n)
            model.load_state_dict(torch.load(params['model_path']))
            model.eval()

            # Initialize agents
            agents = [TradingAgent.remote(env, model, reward_function) for _ in range(params['num_agents'])]
            agent_manager = AgentManager(agents)

            # Train agents
            results = agent_manager.train_agents(num_episodes=params['num_episodes'])

            # Log metrics
            average_reward = sum(results) / len(results)
            mlflow.log_metric("average_reward", average_reward)

            logger.info(f"Experiment completed with average_reward: {average_reward}")

            return average_reward

    def optimize_hyperparameters(self, objective_function, num_trials: int):
        study = optuna.create_study(direction="maximize")
        study.optimize(objective_function, n_trials=num_trials)
        return study.best_params

# Example usage
# experiment_manager = ExperimentManager(experiment_name="Trading Experiment")
# best_params = experiment_manager.optimize_hyperparameters(objective_function, num_trials=50)


// File: src\feature_engineering.py
import pandas as pd
import talib

class FeatureEngineer:
    def add_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        df['SMA_20'] = talib.SMA(df['close'], timeperiod=20)
        df['EMA_20'] = talib.EMA(df['close'], timeperiod=20)
        df['RSI'] = talib.RSI(df['close'], timeperiod=14)
        df['MACD'], df['MACD_Signal'], df['MACD_Hist'] = talib.MACD(df['close'])
        df['ATR'] = talib.ATR(df['high'], df['low'], df['close'])
        df['BBANDS_Upper'], df['BBANDS_Middle'], df['BBANDS_Lower'] = talib.BBANDS(df['close'])
        df['OBV'] = talib.OBV(df['close'], df['volume'])
        df['ADX'] = talib.ADX(df['high'], df['low'], df['close'])
        return df

    def add_custom_features(self, df: pd.DataFrame) -> pd.DataFrame:
        df['pct_change'] = df['close'].pct_change()
        df['log_return'] = np.log(df['close'] / df['close'].shift(1))
        df['volatility'] = df['log_return'].rolling(window=20).std() * np.sqrt(252)
        df['momentum'] = df['close'] - df['close'].shift(10)
        df['price_volume'] = df['close'] * df['volume']
        return df

    def create_lagged_features(self, df: pd.DataFrame, lag: int = 5) -> pd.DataFrame:
        features = ['close', 'volume', 'SMA_20', 'RSI', 'MACD', 'ATR']
        for feature in features:
            for i in range(1, lag + 1):
                df[f'{feature}_lag_{i}'] = df[feature].shift(i)
        return df

    def process_features(self, df: pd.DataFrame) -> pd.DataFrame:
        df = self.add_technical_indicators(df)
        df = self.add_custom_features(df)
        df = self.create_lagged_features(df)
        return df.dropna()

// File: src\feature_selection.py
# File: src/feature_selection.py

from sklearn.feature_selection import SelectFromModel, RFE
from sklearn.linear_model import Lasso
from sklearn.ensemble import RandomForestClassifier
from typing import Optional
import pandas as pd

class FeatureSelector:
    """
    Handles feature selection using various methods.
    """

    def __init__(self, method: str = "SelectFromModel", threshold: Optional[float] = None, max_features: Optional[int] = None):
        """
        Initializes the FeatureSelector.

        Args:
            method (str): Feature selection method. Options: 'SelectFromModel', 'RFE'.
            threshold (Optional[float]): Threshold for SelectFromModel.
            max_features (Optional[int]): Number of features for RFE.
        """
        self.method = method
        self.threshold = threshold
        self.max_features = max_features
        self.selector = None

    def fit_transform(self, X: pd.DataFrame, y: pd.Series) -> pd.DataFrame:
        """
        Fits the feature selector and transforms the feature set.

        Args:
            X (pd.DataFrame): Feature set.
            y (pd.Series): Target variable.

        Returns:
            pd.DataFrame: Reduced feature set.
        """
        if self.method == "SelectFromModel":
            # Using Lasso for feature selection
            estimator = Lasso(alpha=0.01)
            self.selector = SelectFromModel(estimator, threshold=self.threshold)
        elif self.method == "RFE":
            # Using RandomForest for RFE
            estimator = RandomForestClassifier(n_estimators=100)
            self.selector = RFE(estimator, n_features_to_select=self.max_features)
        else:
            raise ValueError(f"Unsupported feature selection method: {self.method}")

        self.selector.fit(X, y)
        selected_features = X.columns[self.selector.get_support()]
        return X[selected_features]

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        Transforms the feature set based on the fitted selector.

        Args:
            X (pd.DataFrame): Feature set.

        Returns:
            pd.DataFrame: Reduced feature set.
        """
        if not self.selector:
            raise RuntimeError("FeatureSelector not fitted yet.")
        selected_features = X.columns[self.selector.get_support()]
        return X[selected_features]

    def get_selected_features(self) -> list:
        """
        Returns the list of selected feature names.

        Returns:
            list: Selected feature names.
        """
        if not self.selector:
            raise RuntimeError("FeatureSelector not fitted yet.")
        return list(self.selector.get_support(indices=True))


// File: src\feature_store.py
# File: src/feature_store.py

import pandas as pd
import os
from typing import Optional

class FeatureStore:
    """
    Manages the storage and retrieval of features.
    """

    def __init__(self, feature_save_path: str):
        """
        Initializes the FeatureStore.

        Args:
            feature_save_path (str): Path to save the features.
        """
        self.feature_save_path = feature_save_path
        os.makedirs(self.feature_save_path, exist_ok=True)

    def save_features(self, df: pd.DataFrame, filename: str):
        """
        Saves the feature DataFrame to a CSV file.

        Args:
            df (pd.DataFrame): Feature DataFrame.
            filename (str): Name of the file to save.
        """
        filepath = os.path.join(self.feature_save_path, filename)
        df.to_csv(filepath, index=False)

    def load_features(self, filename: str) -> pd.DataFrame:
        """
        Loads features from a CSV file.

        Args:
            filename (str): Name of the file to load.

        Returns:
            pd.DataFrame: Loaded feature DataFrame.
        """
        filepath = os.path.join(self.feature_save_path, filename)
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"Feature file not found: {filepath}")
        return pd.read_csv(filepath)

    def list_features(self) -> list:
        """
        Lists all saved feature files.

        Returns:
            list: List of feature filenames.
        """
        return os.listdir(self.feature_save_path)


// File: src\main.py
# File: main.py

import hydra
from omegaconf import DictConfig
import ray
from agents.agent_manager import AgentManager
from environments.crypto_trading_env import CryptoTradingEnv
from models.trading_model import TradingModel
from utils.logging_config import setup_logging, get_logger
from data.data_provider import BinanceDataProvider
from feature_engineering.feature_engineer import FeatureEngineer
from feature_engineering.feature_selector import FeatureSelector
from rewards.reward_functions import ProfitReward, SharpeRatioReward
import torch

@hydra.main(config_path="config", config_name="base_config")
def main(cfg: DictConfig):
    # Setup logging
    setup_logging(cfg.logging)
    logger = get_logger(__name__)

    # Initialize Ray
    ray.init(num_cpus=cfg.ray.num_cpus, num_gpus=cfg.ray.num_gpus)

    # Initialize Data Provider
    data_provider = BinanceDataProvider(api_key=cfg.exchange.api_key, api_secret=cfg.exchange.api_secret)
    
    # Fetch data for each trading pair
    data = {}
    for symbol in cfg.exchange.trading_pairs:
        df = data_provider.get_data(
            symbol=symbol,
            timeframe=cfg.exchange.timeframe,
            start_date=cfg.data.start_date,
            end_date=cfg.data.end_date
        )
        data[symbol] = df
        logger.info(f"Fetched {len(df)} rows for {symbol}")

    # Feature Engineering
    feature_engineer = FeatureEngineer(cfg.features)
    feature_selector = FeatureSelector(cfg.feature_selection)
    
    processed_data = {}
    for symbol, df in data.items():
        df = feature_engineer.process_features(df)
        X = df[cfg.features.feature_columns]
        y = (df['close'].shift(-1) > df['close']).astype(int).fillna(0)
        X_selected = feature_selector.fit_transform(X, y)
        processed_data[symbol] = (X_selected, y)
        logger.info(f"Processed features for {symbol}. Selected features: {X_selected.columns.tolist()}")

    # Initialize environments and agents
    environments = {
        symbol: CryptoTradingEnv(
            data=data,
            processed_features=X,
            reward_function=ProfitReward() if cfg.trading.reward_type == 'profit' else SharpeRatioReward(),
            **cfg.environment
        ) for symbol, (X, _) in processed_data.items()
    }

    agent_configs = [
        {
            "model": TradingModel(
                input_size=next(iter(processed_data.values()))[0].shape[1],
                **cfg.model
            ),
            "reward_function": ProfitReward() if cfg.trading.reward_type == 'profit' else SharpeRatioReward(),
            "agent_id": f"agent_{i}"
        } for i in range(cfg.agents.num_agents)
    ]

    agent_manager = AgentManager(environments, agent_configs)

    # Train agents
    results = agent_manager.train_agents(num_episodes=cfg.training.num_episodes)
    logger.info(f"Training completed. Results: {results}")

    # Evaluate agents
    evaluation_results = agent_manager.evaluate_agents(num_episodes=cfg.evaluation.num_episodes)
    logger.info(f"Evaluation completed. Results: {evaluation_results}")

    # Save trained agents
    agent_manager.save_agents(cfg.paths.model_save_path)

    ray.shutdown()

if __name__ == "__main__":
    main()

// File: src\rewards.py
import numpy as np
from typing import List

class RewardFunction:
    def calculate_reward(self, action: int, current_price: float, next_price: float, portfolio_value: float) -> float:
        pass

class ProfitReward(RewardFunction):
    def calculate_reward(self, action: int, current_price: float, next_price: float, portfolio_value: float) -> float:
        if action == 1:  # Buy
            return (next_price - current_price) / current_price
        elif action == 2:  # Sell
            return (current_price - next_price) / current_price
        else:  # Hold
            return 0

class SharpeRatioReward(RewardFunction):
    def __init__(self, risk_free_rate: float = 0.0, window_size: int = 20):
        self.risk_free_rate = risk_free_rate
        self.window_size = window_size
        self.returns = []

    def calculate_reward(self, action: int, current_price: float, next_price: float, portfolio_value: float) -> float:
        return_ = (next_price - current_price) / current_price
        self.returns.append(return_)

        if len(self.returns) < self.window_size:
            return 0

        returns_array = np.array(self.returns[-self.window_size:])
        excess_returns = returns_array - self.risk_free_rate
        sharpe_ratio = np.sqrt(252) * np.mean(excess_returns) / np.std(excess_returns)

        return sharpe_ratio

class CombinedReward(RewardFunction):
    def __init__(self, profit_weight: float = 0.5, sharpe_weight: float = 0.5):
        self.profit_reward = ProfitReward()
        self.sharpe_reward = SharpeRatioReward()
        self.profit_weight = profit_weight
        self.sharpe_weight = sharpe_weight

    def calculate_reward(self, action: int, current_price: float, next_price: float, portfolio_value: float) -> float:
        profit = self.profit_reward.calculate_reward(action, current_price, next_price, portfolio_value)
        sharpe = self.sharpe_reward.calculate_reward(action, current_price, next_price, portfolio_value)
        return self.profit_weight * profit + self.sharpe_weight * sharpe

// File: src\sentiment_analysis.py
# File: src/sentiment_analysis.py

from typing import List
import pandas as pd
from transformers import pipeline
from src.utils import get_logger

logger = get_logger()

class SentimentAnalyzer:
    """
    Analyzes sentiment from text data using pre-trained models.
    """

    def __init__(self, model_name: str = "nlptown/bert-base-multilingual-uncased-sentiment"):
        """
        Initializes the SentimentAnalyzer.

        Args:
            model_name (str): Name of the pre-trained sentiment analysis model.
        """
        self.pipeline = pipeline("sentiment-analysis", model=model_name)
        logger.info(f"Initialized SentimentAnalyzer with model: {model_name}")

    def analyze_sentiment(self, texts: List[str]) -> pd.Series:
        """
        Analyzes sentiment for a list of texts.

        Args:
            texts (List[str]): List of text strings.

        Returns:
            pd.Series: Sentiment scores.
        """
        results = self.pipeline(texts)
        sentiments = [result['score'] if result['label'] in ['POSITIVE', '5 stars'] else -result['score'] for result in results]
        return pd.Series(sentiments, index=range(len(sentiments)))

    def add_sentiment_to_df(self, df: pd.DataFrame, text_column: str = "news_headline", sentiment_column: str = "sentiment") -> pd.DataFrame:
        """
        Adds sentiment scores to the DataFrame based on a text column.

        Args:
            df (pd.DataFrame): Original DataFrame.
            text_column (str): Column containing text data.
            sentiment_column (str): Name of the new sentiment column.

        Returns:
            pd.DataFrame: DataFrame with added sentiment scores.
        """
        if text_column not in df.columns:
            logger.warning(f"Text column '{text_column}' not found in DataFrame.")
            df[sentiment_column] = 0
            return df

        texts = df[text_column].tolist()
        df[sentiment_column] = self.analyze_sentiment(texts)
        logger.info(f"Added sentiment scores to DataFrame as '{sentiment_column}'.")
        return df

# Example usage
# sentiment_analyzer = SentimentAnalyzer()
# df = sentiment_analyzer.add_sentiment_to_df(df, text_column="news_headline")


// File: src\trading.py
# File: src/trading.py

import pandas as pd
from typing import Optional
from src.rewards import RewardFunction
from src.utils import get_logger

logger = get_logger()

class TradingExecutor:
    """
    Executes trading strategies based on model predictions.
    """

    def __init__(self, initial_balance: float = 10000.0, transaction_fee: float = 0.001, slippage: float = 0.0005):
        """
        Initializes the TradingExecutor.

        Args:
            initial_balance (float): Starting balance in USD.
            transaction_fee (float): Fee per trade.
            slippage (float): Slippage per trade.
        """
        self.initial_balance = initial_balance
        self.transaction_fee = transaction_fee
        self.slippage = slippage

    def execute_backtest(self, df: pd.DataFrame, features: pd.DataFrame, target: pd.Series, reward_function: RewardFunction) -> pd.DataFrame:
        """
        Executes a backtest of the trading strategy.

        Args:
            df (pd.DataFrame): Original DataFrame with market data.
            features (pd.DataFrame): Feature DataFrame used for predictions.
            target (pd.Series): Target variable.
            reward_function (RewardFunction): Reward function to calculate rewards.

        Returns:
            pd.DataFrame: Trade history.
        """
        balance = self.initial_balance
        position = 0
        trade_history = []

        for i in range(len(features) - 1):
            current_features = features.iloc[i].values
            current_price = df.iloc[i]['close']
            next_price = df.iloc[i + 1]['close']

            # Dummy prediction logic (replace with actual model predictions)
            # For example, buy if RSI < 30, sell if RSI > 70
            rsi = features.iloc[i]['RSI']
            if rsi < 30 and balance > 0:
                # Buy
                amount_to_buy = balance * 0.1  # Buy 10% of balance
                position += amount_to_buy / current_price
                balance -= amount_to_buy * (1 + self.transaction_fee + self.slippage)
                action = 'Buy'
            elif rsi > 70 and position > 0:
                # Sell
                proceeds = position * current_price
                balance += proceeds * (1 - self.transaction_fee - self.slippage)
                position = 0
                action = 'Sell'
            else:
                action = 'Hold'

            # Calculate portfolio value
            portfolio_value = balance + position * current_price

            # Calculate reward
            reward = reward_function.calculate_reward(
                action=action,
                current_price=current_price,
                next_price=next_price,
                portfolio_value=portfolio_value
            )

            # Record trade
            trade_history.append({
                'step': i,
                'action': action,
                'balance': balance,
                'position': position,
                'portfolio_value': portfolio_value,
                'reward': reward
            })

        trade_history_df = pd.DataFrame(trade_history)
        logger.info("Backtest completed.")
        return trade_history_df

    def execute_live_trading(self, df: pd.DataFrame, features: pd.DataFrame, model, reward_function: RewardFunction) -> pd.DataFrame:
        """
        Executes live trading using the trained model.

        Args:
            df (pd.DataFrame): DataFrame with live market data.
            features (pd.DataFrame): Feature DataFrame used for predictions.
            model: Trained trading model.
            reward_function (RewardFunction): Reward function to calculate rewards.

        Returns:
            pd.DataFrame: Trade history.
        """
        balance = self.initial_balance
        position = 0
        trade_history = []

        model.eval()

        with torch.no_grad():
            for i in range(len(features) - 1):
                current_features = torch.tensor(features.iloc[i].values, dtype=torch.float32).unsqueeze(0)
                current_price = df.iloc[i]['close']
                next_price = df.iloc[i + 1]['close']

                # Get model prediction
                outputs = model(current_features)
                action = torch.argmax(outputs, dim=1).item()  # 0: Hold, 1: Buy, 2: Sell

                # Map action to string
                action_str = {0: 'Hold', 1: 'Buy', 2: 'Sell'}[action]

                if action_str == 'Buy' and balance > 0:
                    # Buy
                    amount_to_buy = balance * 0.1  # Buy 10% of balance
                    position += amount_to_buy / current_price
                    balance -= amount_to_buy * (1 + self.transaction_fee + self.slippage)
                elif action_str == 'Sell' and position > 0:
                    # Sell
                    proceeds = position * current_price
                    balance += proceeds * (1 - self.transaction_fee - self.slippage)
                    position = 0

                # Calculate portfolio value
                portfolio_value = balance + position * current_price

                # Calculate reward
                reward = reward_function.calculate_reward(
                    action=action,
                    current_price=current_price,
                    next_price=next_price,
                    portfolio_value=portfolio_value
                )

                # Record trade
                trade_history.append({
                    'step': i,
                    'action': action_str,
                    'balance': balance,
                    'position': position,
                    'portfolio_value': portfolio_value,
                    'reward': reward
                })

        trade_history_df = pd.DataFrame(trade_history)
        logger.info("Live trading execution completed.")
        return trade_history_df


// File: src\utils.py
# File: src/utils.py

import logging
import os
import sys

def setup_logging(log_level: str = "INFO", log_file: Optional[str] = None):
    """
    Sets up logging configuration.

    Args:
        log_level (str): Logging level.
        log_file (Optional[str]): File to log messages. If None, logs to stdout.
    """
    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    handlers = [logging.StreamHandler(sys.stdout)]
    if log_file:
        os.makedirs(os.path.dirname(log_file), exist_ok=True)
        handlers.append(logging.FileHandler(log_file))

    logging.basicConfig(
        level=getattr(logging, log_level.upper(), logging.INFO),
        format=log_format,
        handlers=handlers
    )

def get_logger(name: Optional[str] = None) -> logging.Logger:
    """
    Returns a logger instance.

    Args:
        name (Optional[str]): Name of the logger. If None, returns the root logger.

    Returns:
        logging.Logger: Logger instance.
    """
    return logging.getLogger(name)


// File: src\__init__.py
# File: src/__init__.py

from .data_acquisition import BinanceDataProvider
from .feature_engineering import FeatureEngineer
from .feature_selection import FeatureSelector
from .feature_store import FeatureStore
from .sentiment_analysis import SentimentAnalyzer
from .trading import TradingExecutor
from .utils import setup_logging, get_logger
from .data_loader import TradingDataset

__all__ = [
    'BinanceDataProvider',
    'FeatureEngineer',
    'FeatureSelector',
    'FeatureStore',
    'SentimentAnalyzer',
    'TradingExecutor',
    'setup_logging',
    'get_logger',
    'TradingDataset'
]


// File: tests\test_agent_manager.py
# File: tests/test_agent_manager.py

import unittest
from unittest.mock import MagicMock
from src.agent_manager import TradingAgent, AgentManager
from environments.crypto_trading_env import CryptoTradingEnv
from models.model import TradingModel
from src.rewards import ProfitReward

class TestAgentManager(unittest.TestCase):

    def setUp(self):
        # Mock environment, model, and reward function
        self.mock_env = MagicMock(spec=CryptoTradingEnv)
        self.mock_model = MagicMock(spec=TradingModel)
        self.mock_reward_function = MagicMock(spec=ProfitReward)

        # Create a TradingAgent instance
        self.agent = TradingAgent(self.mock_env, self.mock_model, self.mock_reward_function)

        # Initialize AgentManager with a list of agents
        self.agent_manager = AgentManager([self.agent])

    def test_train_agents(self):
        # Setup mock behavior
        self.mock_env.reset.return_value = [1,2,3]
        self.mock_env.step.return_value = ([4,5,6], 1.0, True, {})
        self.mock_model.return_value = MagicMock(argmax=MagicMock(return_value=1))

        # Execute training
        results = self.agent_manager.train_agents(num_episodes=1)

        # Assertions
        self.assertEqual(len(results), 1)
        self.assertEqual(results[0], 1.0)  # Assuming total_reward per episode is 1.0

if __name__ == '__main__':
    unittest.main()


// File: tests\test_data_acquisition.py
# File: tests/test_data_acquisition.py

import unittest
from unittest.mock import patch
from src.data_acquisition import BinanceDataProvider
import pandas as pd

class TestBinanceDataProvider(unittest.TestCase):

    @patch('ccxt.binance')
    def test_get_data(self, mock_binance):
        # Setup mock response
        mock_exchange = mock_binance.return_value
        mock_exchange.fetch_ohlcv.return_value = [
            [1609459200000, 29000, 29500, 28800, 29400, 350],
            [1609462800000, 29400, 29600, 29300, 29500, 200]
        ]

        provider = BinanceDataProvider(api_key="test_key", api_secret="test_secret")
        df = provider.get_data(symbol="BTC/USDT", timeframe="1h", start_date="2021-01-01", end_date="2021-01-02")

        # Assertions
        self.assertEqual(len(df), 2)
        self.assertListEqual(list(df.columns), ['timestamp', 'open', 'high', 'low', 'close', 'volume'])
        self.assertEqual(df.iloc[0]['close'], 29400)
        self.assertEqual(df.iloc[1]['volume'], 200)

if __name__ == '__main__':
    unittest.main()


// File: tests\test_environment.py
# File: tests/test_environment.py

import unittest
from unittest.mock import MagicMock
from environments.crypto_trading_env import CryptoTradingEnv
from src.data_acquisition import DataProvider
from src.rewards import RewardFunction
import numpy as np

class TestCryptoTradingEnv(unittest.TestCase):

    def setUp(self):
        # Mock DataProvider and RewardFunction
        self.mock_data_provider = MagicMock(spec=DataProvider)
        self.mock_reward_function = MagicMock(spec=RewardFunction)
        self.mock_data_provider.get_data.return_value = MagicMock(
            iloc=MagicMock(side_effect=[
                pd.Series({'close': 100, 'SMA_20': 95, 'RSI': 30, 'MACD': 1, 'ATR': 0.5, 'pct_change': 0.01, 'volatility': 0.02}),
                pd.Series({'close': 105, 'SMA_20': 96, 'RSI': 32, 'MACD': 1.2, 'ATR': 0.55, 'pct_change': 0.02, 'volatility': 0.025}),
            ]),
            __len__=MagicMock(return_value=2)
        )
        self.env = CryptoTradingEnv(
            data_provider=self.mock_data_provider,
            reward_function=self.mock_reward_function,
            initial_balance=1000
        )

    def test_reset(self):
        state = self.env.reset()
        self.assertIsInstance(state, np.ndarray)
        self.assertEqual(state.shape[0], 10)

    def test_step_buy(self):
        self.mock_reward_function.calculate_reward.return_value = 0.05
        self.env.reset()
        observation, reward, done, info = self.env.step(1)  # Buy
        self.assertEqual(observation.shape[0], 10)
        self.assertEqual(reward, 0.05)
        self.assertFalse(done)

    def test_step_sell(self):
        self.mock_reward_function.calculate_reward.return_value = -0.03
        self.env.reset()
        self.env.step(1)  # Buy
        observation, reward, done, info = self.env.step(2)  # Sell
        self.assertEqual(reward, -0.03)
        self.assertTrue(done)

if __name__ == '__main__':
    unittest.main()


// File: tests\test_feature_engineering.py
# File: tests/test_feature_engineering.py

import unittest
import pandas as pd
from src.feature_engineering import FeatureEngineer

class TestFeatureEngineer(unittest.TestCase):

    def setUp(self):
        # Sample data
        data = {
            'timestamp': pd.date_range(start='2021-01-01', periods=5, freq='H'),
            'open': [100, 102, 101, 103, 104],
            'high': [105, 106, 103, 107, 108],
            'low': [99, 101, 100, 102, 103],
            'close': [104, 105, 102, 106, 107],
            'volume': [1000, 1500, 1200, 1300, 1400]
        }
        self.df = pd.DataFrame(data)

    def test_add_technical_indicators(self):
        feature_engineer = FeatureEngineer()
        df = feature_engineer.add_technical_indicators(self.df)
        self.assertIn('SMA_20', df.columns)
        self.assertIn('RSI', df.columns)
        self.assertIn('MACD', df.columns)
        self.assertIn('ATR', df.columns)
        self.assertTrue(df['SMA_20'].isnull().all())

    def test_add_custom_features(self):
        feature_engineer = FeatureEngineer()
        df = feature_engineer.add_custom_features(self.df)
        self.assertIn('pct_change', df.columns)
        self.assertIn('volatility', df.columns)
        self.assertEqual(df['pct_change'].iloc[1], 0.02)
        self.assertEqual(df['volatility'].iloc[2], 0.02)

if __name__ == '__main__':
    unittest.main()


// File: tests\test_feature_selection.py
# File: tests/test_feature_selection.py

import unittest
import pandas as pd
from src.feature_selection import FeatureSelector

class TestFeatureSelector(unittest.TestCase):

    def setUp(self):
        # Sample data
        self.X = pd.DataFrame({
            'SMA_20': [95, 96, 97, 98, 99],
            'EMA': [90, 91, 92, 93, 94],
            'RSI': [30, 32, 31, 33, 34],
            'MACD': [1, 1.2, 1.1, 1.3, 1.4],
            'ATR': [0.5, 0.55, 0.6, 0.65, 0.7],
            'pct_change': [0.01, 0.02, -0.01, 0.03, 0.04],
            'volatility': [0.02, 0.025, 0.03, 0.035, 0.04]
        })
        self.y = pd.Series([0, 1, 0, 1, 0])

    def test_selectfrommodel(self):
        selector = FeatureSelector(method="SelectFromModel", threshold=0.1)
        X_selected = selector.fit_transform(self.X, self.y)
        self.assertTrue(len(X_selected.columns) <= 10)

    def test_rfe(self):
        selector = FeatureSelector(method="RFE", max_features=3)
        X_selected = selector.fit_transform(self.X, self.y)
        self.assertEqual(len(X_selected.columns), 3)

    def test_invalid_method(self):
        selector = FeatureSelector(method="InvalidMethod")
        with self.assertRaises(ValueError):
            selector.fit_transform(self.X, self.y)

if __name__ == '__main__':
    unittest.main()


// File: tests\test_model.py
# File: tests/test_model.py

import unittest
import torch
from models.model import TradingModel

class TestTradingModel(unittest.TestCase):

    def setUp(self):
        self.model = TradingModel(input_size=7, hidden_size=64, num_layers=2, output_size=3, dropout=0.2)

    def test_forward(self):
        # Create a dummy input tensor
        input_tensor = torch.randn(1, 7)
        output = self.model(input_tensor)
        self.assertEqual(output.shape, (1, 3))

    def test_parameters(self):
        # Check if model parameters are correctly initialized
        self.assertEqual(self.model.hidden_size, 64)
        self.assertEqual(self.model.num_layers, 2)
        self.assertEqual(self.model.output_size, 3)

if __name__ == '__main__':
    unittest.main()


// File: tests\test_trading.py
# File: tests/test_trading.py

import unittest
import pandas as pd
from src.trading import TradingExecutor
from src.rewards import ProfitReward

class TestTradingExecutor(unittest.TestCase):

    def setUp(self):
        # Sample data
        data = {
            'close': [100, 105, 102, 106, 107],
            'SMA_20': [95, 96, 97, 98, 99],
            'EMA': [90, 91, 92, 93, 94],
            'RSI': [30, 32, 31, 33, 34],
            'MACD': [1, 1.2, 1.1, 1.3, 1.4],
            'ATR': [0.5, 0.55, 0.6, 0.65, 0.7],
            'pct_change': [0.01, 0.02, -0.01, 0.03, 0.04],
            'volatility': [0.02, 0.025, 0.03, 0.035, 0.04]
        }
        self.df = pd.DataFrame(data)
        self.features = self.df[['SMA_20', 'EMA', 'RSI', 'MACD', 'ATR', 'pct_change', 'volatility']]
        self.target = pd.Series([0, 1, 0, 1, 0])
        self.reward_function = ProfitReward()
        self.executor = TradingExecutor(initial_balance=1000, transaction_fee=0.001, slippage=0.0005)

    def test_execute_backtest(self):
        trade_history = self.executor.execute_backtest(self.df, self.features, self.target, self.reward_function)
        self.assertEqual(len(trade_history), len(self.features) - 1)
        self.assertIn('action', trade_history.columns)
        self.assertIn('balance', trade_history.columns)
        self.assertIn('position', trade_history.columns)
        self.assertIn('portfolio_value', trade_history.columns)
        self.assertIn('reward', trade_history.columns)

    def test_execute_live_trading(self):
        # Mock model's eval and prediction
        class MockModel:
            def eval(self):
                pass
            def __call__(self, x):
                # Always predict 'Hold'
                return torch.tensor([[0.0, 0.0, 1.0]])

        mock_model = MockModel()
        trade_history = self.executor.execute_live_trading(self.df, self.features, mock_model, self.reward_function)
        self.assertEqual(len(trade_history), len(self.features) - 1)
        self.assertTrue(all(trade_history['action'] == 'Hold'))

if __name__ == '__main__':
    unittest.main()


// File: tests\test_trainer.py
# File: tests/test_trainer.py

import unittest
from unittest.mock import MagicMock
import pandas as pd
from models.trainer import train_model
from models.model import TradingModel
from src.feature_selection import FeatureSelector

class TestTrainer(unittest.TestCase):

    def setUp(self):
        # Sample data
        self.train_df = pd.DataFrame({
            'SMA_20': [95, 96, 97, 98, 99],
            'EMA': [90, 91, 92, 93, 94],
            'RSI': [30, 32, 31, 33, 34],
            'MACD': [1, 1.2, 1.1, 1.3, 1.4],
            'ATR': [0.5, 0.55, 0.6, 0.65, 0.7],
            'pct_change': [0.01, 0.02, -0.01, 0.03, 0.04],
            'volatility': [0.02, 0.025, 0.03, 0.035, 0.04]
        })
        self.target_df = pd.Series([0, 1, 0, 1, 0])

    def test_train_model(self):
        # Mock config
        config = {
            'feature_selection': {
                'threshold': 0.01,
                'max_features': 10
            },
            'model': {
                'batch_size': 2,
                'hidden_size': 64,
                'output_size': 3,
                'learning_rate': 0.001,
                'epochs': 1,
                'model_save_path': './models/checkpoints/'
            }
        }

        # Mock FeatureSelector
        selector = FeatureSelector(threshold=0.01, max_features=10)
        selector.fit_transform = MagicMock(return_value=self.train_df)
        selector.get_selected_features = MagicMock(return_value=self.train_df.columns.tolist())

        with unittest.mock.patch('models.trainer.FeatureSelector', return_value=selector):
            # Mock TradingDataset and DataLoader
            with unittest.mock.patch('models.trainer.TradingDataset') as mock_dataset:
                mock_dataset.return_value = MagicMock()
                with unittest.mock.patch('torch.save') as mock_save:
                    # Mock model
                    with unittest.mock.patch('models.trainer.TradingModel', return_value=MagicMock()):
                        # Run training
                        train_model(config, self.train_df, self.target_df)
                        mock_save.assert_called()

if __name__ == '__main__':
    unittest.main()


// File: tests\test_utils.py
# File: tests/test_utils.py

import unittest
import logging
from src.utils import setup_logging, get_logger
import os

class TestUtils(unittest.TestCase):

    def test_setup_logging_console(self):
        setup_logging(log_level="DEBUG")
        logger = get_logger()
        self.assertEqual(logger.level, logging.DEBUG)

    def test_setup_logging_file(self):
        log_file = 'logs/test.log'
        if os.path.exists(log_file):
            os.remove(log_file)
        setup_logging(log_level="INFO", log_file=log_file)
        logger = get_logger()
        logger.info("Test log message.")
        self.assertTrue(os.path.exists(log_file))
        with open(log_file, 'r') as f:
            content = f.read()
            self.assertIn("Test log message.", content)
        os.remove(log_file)

if __name__ == '__main__':
    unittest.main()


// File: tests\__init__.py
# File: tests/__init__.py

# This file can be left empty or include package-level imports if necessary.


