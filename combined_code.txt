**File Tree (Relevant Files Only)**
  .
    - agent.py
    - config.py
    - conftest.py
    - networks.py
    - replay_buffer.py
    - reward.py
    - train.py
  env
    - environment.py
  tests
    - test_agent.py
// File: agent.py
# File: agent.py

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import logging
from typing import Dict, List, Tuple, Callable
from torch.optim import AdamW
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.utils.tensorboard import SummaryWriter

from config import EnvironmentConfig
from env.environment import HistoricalEnvironment
from networks import (
    MetaSACActorEnhanced, 
    MetaSACCriticEnhanced, 
    MetaController, 
    PolicyDistillerEnsemble, 
    MarketModeClassifier, 
    HighLevelPolicy,
    MarketGraphModel
)
from replay_buffer import ReplayBuffer
from reward import calculate_reward

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
formatter = logging.Formatter('[%(asctime)s] %(levelname)s:%(name)s:%(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)

class SACLoss(nn.Module):
    """Soft Actor-Critic Loss Function."""
    def __init__(self, alpha: float = 0.2):
        super().__init__()
        self.alpha = alpha

    def forward(self, q_pred: torch.Tensor, q_target: torch.Tensor, mu: torch.Tensor, log_sigma: torch.Tensor, log_prob: torch.Tensor):
        # Critic Loss: MSE between predicted Q and target Q
        critic_loss = F.mse_loss(q_pred, q_target)

        # Actor Loss: minimize alpha * log_prob - Q
        actor_loss = (self.alpha * log_prob - q_pred).mean()

        # Entropy Loss: maximize entropy (minimize negative entropy)
        entropy_loss = -self.alpha * log_prob.mean()

        total_loss = critic_loss + actor_loss + entropy_loss
        return total_loss, critic_loss, actor_loss, entropy_loss

class TradingLoss(nn.Module):
    """Specialized loss function tailored for trading tasks."""
    def __init__(self):
        super().__init__()

    def forward(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """
        Args:
            predictions: Predicted returns or actions
            targets: Actual returns or actions
        Returns:
            Computed loss
        """
        # Example: Mean Squared Error with additional penalty for volatility
        mse_loss = F.mse_loss(predictions, targets)
        volatility_penalty = torch.mean(torch.var(predictions, dim=1))
        loss = mse_loss + 0.1 * volatility_penalty
        return loss

class MetaSACAgent(nn.Module):
    """Meta Soft Actor-Critic (SAC) agent with advanced enhancements."""
    def __init__(self, config: EnvironmentConfig, env: HistoricalEnvironment):
        super().__init__()
        self.config = config
        self.device = torch.device(config.device)
        self.env = env

        # Graph Neural Network to model relationships between market entities
        self.market_gnn = MarketGraphModel(config).to(self.device)

        # Actor & Critics with Enhancements (Recurrent Layers, Transformer Layers, Residual Connections, etc.)
        self.actor = MetaSACActorEnhanced(config).to(self.device)
        self.critic1 = MetaSACCriticEnhanced(config).to(self.device)
        self.critic2 = MetaSACCriticEnhanced(config).to(self.device)
        self.critic_target1 = MetaSACCriticEnhanced(config).to(self.device)
        self.critic_target2 = MetaSACCriticEnhanced(config).to(self.device)

        # Initialize target networks
        self.critic_target1.load_state_dict(self.critic1.state_dict())
        self.critic_target2.load_state_dict(self.critic2.state_dict())

        # Meta-Controller for dynamic hyperparameter adjustment
        self.meta_controller = MetaController(config).to(self.device)
        
        # Policy Distiller with Ensemble of Specialist Policies
        self.specialist_policies = [MetaSACActorEnhanced(config).to(self.device) for _ in range(config.ensemble_size)]
        self.policy_distiller = PolicyDistillerEnsemble(self.specialist_policies, config).to(self.device)

        # Market Mode Classifier & High-Level Policy for hierarchical decision-making
        self.market_mode_classifier = MarketModeClassifier(
            input_dim=config.state_dim, 
            hidden_dim=config.hidden_dim, 
            output_dim=config.num_market_modes
        ).to(self.device)
        self.high_level_policy = HighLevelPolicy(config.state_dim, config.hidden_dim).to(self.device)

        # Optimizers with Advanced Techniques (AdamW)
        self.actor_optimizer = AdamW(self.actor.parameters(), lr=config.lr, weight_decay=config.weight_decay)
        self.critic1_optimizer = AdamW(self.critic1.parameters(), lr=config.lr, weight_decay=config.weight_decay)
        self.critic2_optimizer = AdamW(self.critic2.parameters(), lr=config.lr, weight_decay=config.weight_decay)
        self.meta_optimizer = AdamW(self.meta_controller.parameters(), lr=config.meta_lr, weight_decay=config.weight_decay)
        self.distiller_optimizer = AdamW(self.policy_distiller.parameters(), lr=config.lr, weight_decay=config.weight_decay)
        self.market_mode_optimizer = AdamW(self.market_mode_classifier.parameters(), lr=config.lr, weight_decay=config.weight_decay)
        self.high_level_optimizer = AdamW(self.high_level_policy.parameters(), lr=config.lr, weight_decay=config.weight_decay)

        # Learning Rate Schedulers
        self.actor_scheduler = ReduceLROnPlateau(self.actor_optimizer, mode="min", factor=0.5, patience=5)
        self.critic1_scheduler = ReduceLROnPlateau(self.critic1_optimizer, mode="min", factor=0.5, patience=5)
        self.critic2_scheduler = ReduceLROnPlateau(self.critic2_optimizer, mode="min", factor=0.5, patience=5)
        self.high_level_scheduler = ReduceLROnPlateau(self.high_level_optimizer, mode="min", factor=0.5, patience=5)
        self.meta_scheduler = ReduceLROnPlateau(self.meta_optimizer, mode="min", factor=0.5, patience=5)
        self.distiller_scheduler = ReduceLROnPlateau(self.distiller_optimizer, mode="min", factor=0.5, patience=5)

        # Entropy coefficient alpha with automatic adjustment
        self.alpha = nn.Parameter(torch.tensor(config.alpha, dtype=torch.float32, device=self.device))
        self.target_entropy = -float(config.action_dim) * config.target_entropy_factor

        # Replay Buffer
        self.replay_buffer = ReplayBuffer(config.buffer_capacity)

        # Logging
        self.writer = SummaryWriter()
        self.train_steps = 0

        # Move all modules to device
        self.to(self.device)

    def select_action(self, state: np.ndarray, time_step: int, edge_index: torch.Tensor, graph_node_features: torch.Tensor, eval=False) -> np.ndarray:
        """
        Selects action using either the actor or policy distiller.

        Args:
            state (np.ndarray): Current state.
            time_step (int): Current time step.
            edge_index (torch.Tensor): Graph edges.
            graph_node_features (torch.Tensor): Graph node features.
            eval (bool): Whether to select action in evaluation mode.

        Returns:
            np.ndarray: Selected action.
        """
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)  # (1, seq_length, state_dim)
        time_tensor = torch.tensor([time_step], dtype=torch.float32).to(self.device)  # (1,)
        graph_node_features = graph_node_features.to(self.device)
        edge_index = edge_index.to(self.device)

        # Process graph data
        graph_embedding = self.market_gnn(graph_node_features, edge_index)  # (num_nodes, hidden_dim)
        graph_embedding = torch.mean(graph_embedding, dim=0, keepdim=True).repeat(state_tensor.size(0), 1, 1)  # (1, 1, hidden_dim)
        state_tensor = torch.cat([state_tensor, graph_embedding], dim=1)  # (1, seq_length +1, hidden_dim)

        if eval:
            with torch.no_grad():
                mu, log_sigma = self.actor(state_tensor, time_tensor)
                action = mu
        else:
            with torch.no_grad():
                mu, log_sigma = self.policy_distiller(state_tensor, time_tensor)
                sigma = torch.exp(log_sigma)
                dist = torch.distributions.Normal(mu, sigma)
                z = dist.rsample()
                action = torch.tanh(z)
        
        action = action.clamp(-1, 1)
        return action.cpu().numpy().squeeze(0)

    def compute_q_targets(self, 
                          rewards: torch.Tensor, 
                          next_states: torch.Tensor, 
                          time_steps: torch.Tensor, 
                          dones: torch.Tensor,
                          r_scaling: torch.Tensor,
                          market_mode_probs: torch.Tensor) -> torch.Tensor:
        """
        Computes the target Q-values using the target critic networks.

        Args:
            rewards (torch.Tensor): Rewards from the environment.
            next_states (torch.Tensor): Next states.
            time_steps (torch.Tensor): Time steps.
            dones (torch.Tensor): Done flags.
            r_scaling (torch.Tensor): Reward scaling factors.
            market_mode_probs (torch.Tensor): Market mode probabilities.

        Returns:
            torch.Tensor: Target Q-values.
        """
        with torch.no_grad():
            next_mu, next_log_sigma = self.actor(next_states, time_steps)
            sigma = torch.exp(next_log_sigma)
            dist = torch.distributions.Normal(next_mu, sigma)
            next_actions = torch.tanh(dist.rsample())
            log_probs = dist.log_prob(next_actions) - torch.log(1 - next_actions.pow(2) + self.config.epsilon)
            next_log_probs = log_probs.sum(-1, keepdim=True)

            q_target1 = self.critic_target1(next_states, next_actions, time_steps)
            q_target2 = self.critic_target2(next_states, next_actions, time_steps)
            q_target = torch.min(q_target1, q_target2)

            scaled_rewards = (
                r_scaling[:, 0:1]*rewards*market_mode_probs[:, 0:1] +
                r_scaling[:, 1:2]*rewards*market_mode_probs[:, 1:2] +
                r_scaling[:, 2:3]*rewards*market_mode_probs[:, 2:3]
            )
            y = scaled_rewards + (1.0 - dones)*self.config.gamma*(q_target - self.alpha*next_log_probs)
            return y

    def update_critics(self, states, actions, time_steps, q_targets) -> Tuple[float, float]:
        """
        Updates the critic networks using Mean Squared Error loss.

        Args:
            states (torch.Tensor): Current states.
            actions (torch.Tensor): Actions taken.
            time_steps (torch.Tensor): Time steps.
            q_targets (torch.Tensor): Target Q-values.

        Returns:
            Tuple[float, float]: Critic1 and Critic2 losses.
        """
        q1 = self.critic1(states, actions, time_steps)
        q2 = self.critic2(states, actions, time_steps)
        critic1_loss = F.mse_loss(q1, q_targets)
        critic2_loss = F.mse_loss(q2, q_targets)

        self.critic1_optimizer.zero_grad()
        critic1_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.critic1.parameters(), self.config.max_grad_norm)
        self.critic1_optimizer.step()

        self.critic2_optimizer.zero_grad()
        critic2_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.critic2.parameters(), self.config.max_grad_norm)
        self.critic2_optimizer.step()

        return critic1_loss.item(), critic2_loss.item()

    def update_actor(self, states, time_steps) -> float:
        """
        Updates the actor network to maximize expected return.

        Args:
            states (torch.Tensor): Current states.
            time_steps (torch.Tensor): Time steps.

        Returns:
            float: Actor loss.
        """
        mu, log_sigma = self.policy_distiller(states, time_steps)
        sigma = torch.exp(log_sigma)
        dist = torch.distributions.Normal(mu, sigma)
        z = dist.rsample()
        actions = torch.tanh(z)
        log_probs = dist.log_prob(z) - torch.log(1 - actions.pow(2) + self.config.epsilon)
        log_probs = log_probs.sum(-1, keepdim=True)

        q_val1 = self.critic1(states, actions, time_steps)
        q_val2 = self.critic2(states, actions, time_steps)
        q_val = torch.min(q_val1, q_val2)
        actor_loss = (self.alpha * log_probs - q_val).mean()

        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.config.max_grad_norm)
        self.actor_optimizer.step()

        return actor_loss.item()

    def update_distiller(self, states, time_steps) -> float:
        """
        Updates the policy distiller network.

        Args:
            states (torch.Tensor): Current states.
            time_steps (torch.Tensor): Time steps.

        Returns:
            float: Distiller loss.
        """
        mu, log_sigma = self.policy_distiller(states, time_steps)
        sigma = torch.exp(log_sigma)
        dist = torch.distributions.Normal(mu, sigma)
        z = dist.rsample()
        actions = torch.tanh(z)
        log_probs = dist.log_prob(z) - torch.log(1 - actions.pow(2) + self.config.epsilon)
        log_probs = log_probs.sum(-1, keepdim=True)

        q_val1 = self.critic1(states, actions, time_steps)
        q_val2 = self.critic2(states, actions, time_steps)
        q_val = torch.min(q_val1, q_val2)
        distiller_loss = (self.alpha * log_probs - q_val).mean()

        self.distiller_optimizer.zero_grad()
        distiller_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.policy_distiller.parameters(), self.config.max_grad_norm)
        self.distiller_optimizer.step()

        return distiller_loss.item()

    def update_market_mode_classifier(self, states, market_modes) -> float:
        """
        Updates the market mode classifier network.

        Args:
            states (torch.Tensor): Current states.
            market_modes (torch.Tensor): Market mode labels.

        Returns:
            float: Market mode classification loss.
        """
        preds = self.market_mode_classifier(states)
        loss = F.cross_entropy(preds, market_modes)
        self.market_mode_optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.market_mode_classifier.parameters(), self.config.max_grad_norm)
        self.market_mode_optimizer.step()
        return loss.item()

    def update_meta_controller(self, meta_input_tensor, log_probs, rewards) -> Tuple[float, torch.Tensor]:
        """
        Updates the meta-controller network.

        Args:
            meta_input_tensor (torch.Tensor): Inputs for the meta-controller.
            log_probs (torch.Tensor): Log probabilities from the policy.
            rewards (torch.Tensor): Rewards from the environment.

        Returns:
            Tuple[float, torch.Tensor]: Meta-controller loss and reward scaling factors.
        """
        mean_r = torch.mean(rewards, dim=0, keepdim=True)
        var_r = torch.var(rewards, dim=0, keepdim=True)
        reward_stats = torch.cat([mean_r, var_r], dim=-1).to(self.device)

        batch_size = meta_input_tensor.size(0)
        reward_stats = reward_stats.repeat(batch_size, 1)

        out = self.meta_controller(meta_input_tensor, reward_stats)
        # Assuming out contains scaling factors at specific indices
        # Adjust indices based on actual implementation
        r_scaling = out[:, :self.config.num_hyperparams + 3]

        # Example meta-controller loss (placeholder)
        meta_loss = F.mse_loss(out, torch.ones_like(out))  # Placeholder loss

        self.meta_optimizer.zero_grad()
        meta_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.meta_controller.parameters(), self.config.max_grad_norm)
        self.meta_optimizer.step()

        return meta_loss.item(), r_scaling

    def update_high_level_policy(self, states, advantages) -> float:
        """
        Updates the high-level policy network.

        Args:
            states (torch.Tensor): Current states.
            advantages (torch.Tensor): Advantage estimates.

        Returns:
            float: High-level policy loss.
        """
        probs = self.high_level_policy(states)
        log_probs = torch.log(probs + 1e-10)
        policy_loss = -(log_probs * advantages).mean()

        self.high_level_optimizer.zero_grad()
        policy_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.high_level_policy.parameters(), self.config.max_grad_norm)
        self.high_level_optimizer.step()
        return policy_loss.item()

    def soft_update(self, target_net, source_net) -> None:
        """
        Performs a soft update of target network parameters.

        Args:
            target_net (nn.Module): Target network.
            source_net (nn.Module): Source network.
        """
        for target_param, source_param in zip(target_net.parameters(), source_net.parameters()):
            target_param.data.copy_(target_param.data*(1.0 - self.config.tau) + source_param.data*self.config.tau)

    def perform_best_of_n_search(
        self,
        initial_state: torch.Tensor,
        num_samples: int,
        search_depth: int,
        time_memory: List[int],
        exploration_noise_std: float = 0.0
    ) -> List[List[Tuple[np.ndarray, np.ndarray, float, np.ndarray, bool, int]]]:
        """
        Samples trajectories using Best-of-N search.

        Args:
            initial_state (torch.Tensor): Initial state tensor.
            num_samples (int): Number of samples.
            search_depth (int): Depth of search.
            time_memory (List[int]): Historical time steps.
            exploration_noise_std (float): Standard deviation for exploration noise.

        Returns:
            List[List[Tuple]]: List of trajectories.
        """
        all_trajectories = []
        for _ in range(num_samples):
            state = initial_state.clone()
            trajectory = []
            time_idx = time_memory[-1]
            for _step in range(search_depth):
                with torch.no_grad():
                    mu, log_sigma = self.actor(state.unsqueeze(0), torch.tensor([time_idx], device=self.device))
                    sigma = torch.exp(log_sigma)
                    dist = torch.distributions.Normal(mu, sigma)
                    z = dist.rsample()
                    action = torch.tanh(z).squeeze(0)
                    if exploration_noise_std > 0.0:
                        noise = torch.randn_like(action) * exploration_noise_std
                        action = torch.clamp(action + noise, -1.0, 1.0)

                action_np = action.cpu().numpy()
                next_state_np, reward, done, _info = self.env.step(action_np, time_idx)
                next_state = torch.FloatTensor(next_state_np).to(self.device)
                trajectory.append((state.cpu().numpy(), action_np, reward, next_state_np, done, time_idx))

                state = next_state
                time_idx += 1
                if done:
                    break
            all_trajectories.append(trajectory)
        return all_trajectories

    def perform_beam_search(
        self,
        initial_state: torch.Tensor,
        beam_width: int,
        search_depth: int,
        time_memory: List[int],
        exploration_noise_std: float = 0.0
    ) -> List[Tuple[np.ndarray, np.ndarray, float, np.ndarray, bool, int]]:
        """
        Performs Beam Search to find the best trajectory.

        Args:
            initial_state (torch.Tensor): Initial state tensor.
            beam_width (int): Beam width.
            search_depth (int): Depth of search.
            time_memory (List[int]): Historical time steps.
            exploration_noise_std (float): Standard deviation for exploration noise.

        Returns:
            List[Tuple]: Best trajectory.
        """
        beam = [(initial_state.clone(), [], 0.0, time_memory[-1])]
        final_trajs = []

        for _ in range(search_depth):
            new_beam = []
            for current_state, partial_traj, cum_reward, t_step in beam:
                if partial_traj and partial_traj[-1][4] is True:
                    final_trajs.append((current_state, partial_traj, cum_reward, t_step))
                    continue
                with torch.no_grad():
                    mu, log_sigma = self.actor(current_state.unsqueeze(0), torch.tensor([t_step], device=self.device))
                    sigma = torch.exp(log_sigma)
                    dist = torch.distributions.Normal(mu, sigma)
                    candidate_actions = []
                    for _i in range(beam_width):
                        z = dist.rsample()
                        a = torch.tanh(z).squeeze(0)
                        if exploration_noise_std > 0.0:
                            noise = torch.randn_like(a) * exploration_noise_std
                            a = torch.clamp(a + noise, -1.0, 1.0)
                        candidate_actions.append(a)

                for a_tensor in candidate_actions:
                    a_np = a_tensor.cpu().numpy()
                    ns_np, r, d, _info = self.env.step(a_np, t_step)
                    ns = torch.FloatTensor(ns_np).to(self.device)
                    new_traj = partial_traj.copy()
                    new_traj.append((current_state.cpu().numpy(), a_np, r, ns_np, d, t_step))
                    new_beam.append((ns, new_traj, cum_reward + r, t_step + 1))

            new_beam.sort(key=lambda x: x[2], reverse=True)
            beam = new_beam[:beam_width]

        final_trajs.extend(beam)
        if final_trajs:
            best_traj = max(final_trajs, key=lambda x: x[2])[1]
            return best_traj
        else:
            return []

    def update_params_with_training_time_search(
        self,
        replay_buffer: ReplayBuffer,
        meta_input: np.ndarray,
        time_memory: List[int],
        update_steps: int =1,
        search_algorithm: str = "best-of-n",
        num_samples: int = 4,
        beam_width: int = 3,
        search_depth: int = 5,
        use_d_search: bool = False,
        exploration_noise_std_fn: Callable[[int], float] = lambda step: 0.0
    ) -> Dict[str, float]:
        """
        Updates parameters using search-generated data.

        Args:
            replay_buffer (ReplayBuffer): Experience replay buffer.
            meta_input (np.ndarray): Input for the meta-controller.
            time_memory (List[int]): Historical time steps.
            update_steps (int): Number of update iterations.
            search_algorithm (str): Search algorithm type ("best-of-n" or "beam-search").
            num_samples (int): Number of samples for best-of-n search.
            beam_width (int): Beam width for beam search.
            search_depth (int): Depth of search.
            use_d_search (bool): Whether to use depth-based search.
            exploration_noise_std_fn (Callable[[int], float]): Function to determine exploration noise.

        Returns:
            Dict[str, float]: Dictionary of loss values for logging.
        """
        final_info = {}
        for _ in range(update_steps):
            if len(replay_buffer) < self.config.batch_size:
                logger.warning("Not enough samples in replay buffer.")
                continue

            batch = replay_buffer.sample(self.config.batch_size)
            sampled_states = torch.FloatTensor([b[0] for b in batch]).to(self.device)

            search_trajectories = []
            for state in sampled_states:
                exploration_noise_std = exploration_noise_std_fn(self.train_steps)
                if search_algorithm == "best-of-n":
                    all_trajectories = self.perform_best_of_n_search(
                        state, num_samples, search_depth, time_memory, exploration_noise_std
                    )
                    if use_d_search:
                        for traj in all_trajectories:
                            search_trajectories.extend(traj)
                    else:
                        if all_trajectories:
                            best_traj = max(all_trajectories, key=lambda t: sum(step[2] for step in t))
                            search_trajectories.extend(best_traj)
                elif search_algorithm == "beam-search":
                    best_traj = self.perform_beam_search(
                        state, beam_width, search_depth, time_memory, exploration_noise_std
                    )
                    search_trajectories.extend(best_traj)
                else:
                    logger.error(f"Unknown search algorithm: {search_algorithm}")
                    continue

            if not search_trajectories:
                logger.warning("No trajectories found during search.")
                continue

            # Extract search trajectory data
            states_search = torch.FloatTensor([traj[0] for traj in search_trajectories]).to(self.device)
            actions_search = torch.FloatTensor([traj[1] for traj in search_trajectories]).to(self.device)
            rewards_search = torch.FloatTensor([traj[2] for traj in search_trajectories]).unsqueeze(1).to(self.device)
            next_states_search = torch.FloatTensor([traj[3] for traj in search_trajectories]).to(self.device)
            dones_search = torch.FloatTensor([traj[4] for traj in search_trajectories]).unsqueeze(1).to(self.device)
            time_steps_search = torch.FloatTensor([traj[5] for traj in search_trajectories]).to(self.device)

            # Market mode classification (using existing classifier)
            random_modes = torch.randint(0, self.config.num_market_modes, (states_search.shape[0],)).to(self.device)
            market_mode_loss = self.update_market_mode_classifier(states_search, random_modes)

            # Meta-Controller update
            meta_input_tensor = torch.FloatTensor(meta_input).to(self.device)
            with torch.no_grad():
                mu, log_sigma = self.actor(states_search, time_steps_search)
                sigma = torch.exp(log_sigma)
                dist = torch.distributions.Normal(mu, sigma)
                z = dist.rsample()
                actions_ = torch.tanh(z)
                log_probs = dist.log_prob(z) - torch.log(1 - actions_.pow(2) + self.config.epsilon)
                log_probs = log_probs.sum(-1, keepdim=True)

            meta_loss_val, r_scaling = self.update_meta_controller(meta_input_tensor, log_probs, rewards_search)
            market_mode_probs = self.market_mode_classifier(states_search)

            # Compute Q targets
            q_targets = self.compute_q_targets(rewards_search, next_states_search, time_steps_search, dones_search, r_scaling, market_mode_probs)

            # Update Critic Networks
            critic1_loss, critic2_loss = self.update_critics(states_search, actions_search, time_steps_search, q_targets)

            # Update Actor Network
            actor_loss_val = self.update_actor(states_search, time_steps_search)

            # Update Policy Distiller
            distiller_loss_val = self.update_distiller(states_search, time_steps_search)

            # Update High-Level Policy
            advantages = q_targets - torch.mean(q_targets, dim=0, keepdim=True)
            high_level_loss_val = self.update_high_level_policy(states_search, advantages)

            # Soft-update target critics
            self.soft_update(self.critic_target1, self.critic1)
            self.soft_update(self.critic_target2, self.critic2)

            # Update Learning Rate Schedulers
            self.actor_scheduler.step(actor_loss_val)
            self.critic1_scheduler.step(critic1_loss)
            self.critic2_scheduler.step(critic2_loss)
            self.high_level_scheduler.step(high_level_loss_val)
            self.meta_scheduler.step(meta_loss_val)
            self.distiller_scheduler.step(distiller_loss_val)

            self.train_steps += 1

            # Logging
            self.writer.add_scalar("Loss/actor", actor_loss_val, self.train_steps)
            self.writer.add_scalar("Loss/critic1", critic1_loss, self.train_steps)
            self.writer.add_scalar("Loss/critic2", critic2_loss, self.train_steps)
            self.writer.add_scalar("Loss/meta", meta_loss_val, self.train_steps)
            self.writer.add_scalar("Loss/distiller", distiller_loss_val, self.train_steps)
            self.writer.add_scalar("Loss/market_mode", market_mode_loss, self.train_steps)
            self.writer.add_scalar("Loss/high_level_policy", high_level_loss_val, self.train_steps)
            self.writer.add_scalar("Params/alpha", self.alpha.item(), self.train_steps)

            final_info = {
                "actor_loss": actor_loss_val,
                "critic1_loss": critic1_loss,
                "critic2_loss": critic2_loss,
                "meta_loss": meta_loss_val,
                "distiller_loss": distiller_loss_val,
                "market_mode_loss": market_mode_loss,
                "high_level_loss": high_level_loss_val,
                "alpha": self.alpha.item()
            }

        return final_info

    def update_params(self, replay_buffer: ReplayBuffer, meta_input: np.ndarray, time_memory: List[int], update_steps: int =1) -> Dict[str, float]:
        """
        Updates parameters without using search.

        Args:
            replay_buffer (ReplayBuffer): Experience replay buffer.
            meta_input (np.ndarray): Input for the meta-controller.
            time_memory (List[int]): Historical time steps.
            update_steps (int): Number of update iterations.

        Returns:
            Dict[str, float]: Dictionary of loss values for logging.
        """
        final_info = {}
        for _ in range(update_steps):
            if len(replay_buffer) < self.config.batch_size:
                logger.warning("Not enough samples in replay buffer.")
                continue

            batch = replay_buffer.sample(self.config.batch_size)
            states = torch.FloatTensor([b[0] for b in batch]).to(self.device)
            actions = torch.FloatTensor([b[1] for b in batch]).to(self.device)
            rewards = torch.FloatTensor([b[2] for b in batch]).unsqueeze(1).to(self.device)
            next_states = torch.FloatTensor([b[3] for b in batch]).to(self.device)
            dones = torch.FloatTensor([b[4] for b in batch]).unsqueeze(1).to(self.device)
            time_steps = torch.FloatTensor([b[5] for b in batch]).to(self.device)

            # Market mode classification (using existing classifier)
            random_modes = torch.randint(0, self.config.num_market_modes, (states.shape[0],)).to(self.device)
            market_mode_loss = self.update_market_mode_classifier(states, random_modes)

            # Meta-Controller update
            meta_input_tensor = torch.FloatTensor(meta_input).to(self.device)
            with torch.no_grad():
                mu, log_sigma = self.actor(states, time_steps)
                sigma = torch.exp(log_sigma)
                dist = torch.distributions.Normal(mu, sigma)
                z = dist.rsample()
                actions_ = torch.tanh(z)
                log_probs = dist.log_prob(z) - torch.log(1 - actions_.pow(2) + self.config.epsilon)
                log_probs = log_probs.sum(-1, keepdim=True)

            meta_loss_val, r_scaling = self.update_meta_controller(meta_input_tensor, log_probs, rewards)
            market_mode_probs = self.market_mode_classifier(states)

            # Compute Q targets
            q_targets = self.compute_q_targets(rewards, next_states, time_steps, dones, r_scaling, market_mode_probs)

            # Update Critic Networks
            critic1_loss, critic2_loss = self.update_critics(states, actions, time_steps, q_targets)

            # Update Actor Network
            actor_loss_val = self.update_actor(states, time_steps)

            # Update Policy Distiller
            distiller_loss_val = self.update_distiller(states, time_steps)

            # Update High-Level Policy
            advantages = q_targets - torch.mean(q_targets, dim=0, keepdim=True)
            high_level_loss_val = self.update_high_level_policy(states, advantages)

            # Soft-update target critics
            self.soft_update(self.critic_target1, self.critic1)
            self.soft_update(self.critic_target2, self.critic2)

            # Update Learning Rate Schedulers
            self.actor_scheduler.step(actor_loss_val)
            self.critic1_scheduler.step(critic1_loss)
            self.critic2_scheduler.step(critic2_loss)
            self.high_level_scheduler.step(high_level_loss_val)
            self.meta_scheduler.step(meta_loss_val)
            self.distiller_scheduler.step(distiller_loss_val)

            self.train_steps += 1

            # Logging
            self.writer.add_scalar("Loss/actor", actor_loss_val, self.train_steps)
            self.writer.add_scalar("Loss/critic1", critic1_loss, self.train_steps)
            self.writer.add_scalar("Loss/critic2", critic2_loss, self.train_steps)
            self.writer.add_scalar("Loss/meta", meta_loss_val, self.train_steps)
            self.writer.add_scalar("Loss/distiller", distiller_loss_val, self.train_steps)
            self.writer.add_scalar("Loss/market_mode", market_mode_loss, self.train_steps)
            self.writer.add_scalar("Loss/high_level_policy", high_level_loss_val, self.train_steps)
            self.writer.add_scalar("Params/alpha", self.alpha.item(), self.train_steps)

            final_info = {
                "actor_loss": actor_loss_val,
                "critic1_loss": critic1_loss,
                "critic2_loss": critic2_loss,
                "meta_loss": meta_loss_val,
                "distiller_loss": distiller_loss_val,
                "market_mode_loss": market_mode_loss,
                "high_level_loss": high_level_loss_val,
                "alpha": self.alpha.item()
            }

        return final_info

    def save(self, path: str) -> None:
        """
        Saves the agent's state dictionaries.

        Args:
            path (str): Path to save the model.
        """
        try:
            torch.save({
                "actor": self.actor.state_dict(),
                "critic1": self.critic1.state_dict(),
                "critic2": self.critic2.state_dict(),
                "critic_target1": self.critic_target1.state_dict(),
                "critic_target2": self.critic_target2.state_dict(),
                "meta_controller": self.meta_controller.state_dict(),
                "policy_distiller": self.policy_distiller.state_dict(),
                "market_mode_classifier": self.market_mode_classifier.state_dict(),
                "high_level_policy": self.high_level_policy.state_dict(),
                "alpha": self.alpha.detach().cpu().numpy(),
                "train_steps": self.train_steps
            }, path)
            logger.info(f"Model saved to {path}")
        except Exception as e:
            logger.error(f"Failed to save model: {e}")
            raise

    def load(self, path: str) -> None:
        """
        Loads the agent's state dictionaries.

        Args:
            path (str): Path from which to load the model.
        """
        try:
            ckpt = torch.load(path, map_location=self.device)
            self.actor.load_state_dict(ckpt["actor"])
            self.critic1.load_state_dict(ckpt["critic1"])
            self.critic2.load_state_dict(ckpt["critic2"])
            self.critic_target1.load_state_dict(ckpt["critic_target1"])
            self.critic_target2.load_state_dict(ckpt["critic_target2"])
            self.meta_controller.load_state_dict(ckpt["meta_controller"])
            self.policy_distiller.load_state_dict(ckpt["policy_distiller"])
            self.market_mode_classifier.load_state_dict(ckpt["market_mode_classifier"])
            self.high_level_policy.load_state_dict(ckpt["high_level_policy"])
            self.alpha.data.copy_(torch.tensor(ckpt["alpha"], dtype=torch.float32, device=self.device))
            self.train_steps = ckpt["train_steps"]
            logger.info(f"Model loaded from {path}")
        except Exception as e:
            logger.error(f"Failed to load model: {e}")
            raise


// File: config.py
# File: config.py

import torch  # Added import
from dataclasses import dataclass
from typing import List, Optional

@dataclass
class EnvironmentConfig:
    """Configuration for environment, training, and hyperparameters."""
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    action_dim: int = 5  # Updated to match sample_batch
    state_dim: int = 50  # Updated from 8 to 50 for consistency
    hidden_dim: int = 256
    attention_dim: int = 64
    num_mlp_layers: int = 3
    dropout_rate: float = 0.1
    time_encoding_dim: int = 16
    custom_layers: Optional[List[str]] = None
    window_size: int = 20
    num_hyperparams: int = 10  # Updated for testing consistency
    num_market_modes: int = 3
    graph_input_dim: int = 10
    graph_hidden_dim: int = 32
    num_graph_layers: int = 2
    ensemble_size: int = 3
    lr: float = 3e-4
    meta_lr: float = 1e-4
    gamma: float = 0.99
    tau: float = 0.005
    alpha: float = 0.2
    target_entropy_factor: float = 1.0
    epsilon: float = 1e-6
    max_grad_norm: float = 5.0
    buffer_capacity: int = 100000
    batch_size: int = 64
    meta_input_dim: int = 10
    weight_decay: float = 1e-5  # Added weight_decay for optimizers


// File: conftest.py
# File: conftest.py

import sys
import pytest
import torch
import numpy as np
from config import EnvironmentConfig
from env.environment import HistoricalEnvironment
from agent import MetaSACAgent

# Add project root to sys.path
sys.path.append(".")

@pytest.fixture
def agent():
    """
    Fixture to initialize the MetaSACAgent with mock data.
    """
    config = EnvironmentConfig(
        state_dim=50,
        action_dim=5,
        hidden_dim=128,
        attention_dim=64,
        num_mlp_layers=3,
        dropout_rate=0.1,
        time_encoding_dim=16,
        custom_layers=["KLinePatternLayer", "VolatilityTrackingLayer", "FractalDimensionLayer"],
        window_size=20,
        num_hyperparams=10,
        graph_input_dim=10,
        graph_hidden_dim=32,
        num_graph_layers=2,
        ensemble_size=3,
        weight_decay=1e-5
    )
    mock_data = np.random.randn(2000, config.state_dim).astype(np.float32)  # Increased data points for better simulation
    env = HistoricalEnvironment(mock_data)
    agent = MetaSACAgent(config, env)
    # Populate replay buffer with random data
    for _ in range(config.buffer_capacity // 10):  # Add a fraction to avoid filling it up
        state = env.reset()
        for step in range(10):
            action = np.random.uniform(-1, 1, config.action_dim)
            next_state, reward, done, _ = env.step(action, step)
            agent.replay_buffer.add(state, action, reward, next_state, done, step)
            if done:
                break
            state = next_state
    return agent

@pytest.fixture
def sample_batch():
    """
    Fixture to provide a sample batch of data for testing.
    """
    return {
        'states': np.random.randn(32, 20, 50).astype(np.float32),  # (batch_size, seq_length, state_dim)
        'actions': np.random.randn(32, 5).astype(np.float32),     # (batch_size, action_dim)
        'rewards': np.random.randn(32, 1).astype(np.float32),     # (batch_size, 1)
        'next_states': np.random.randn(32, 20, 50).astype(np.float32),  # (batch_size, seq_length, state_dim)
        'dones': np.random.randint(0, 2, (32, 1)).astype(np.float32),    # (batch_size, 1)
        'time_steps': np.random.randint(0, 1000, (32,)),                 # (batch_size,)
        'edge_index': torch.tensor([[0, 1, 2], [1, 2, 0]], dtype=torch.long),  # Example edge index
        'graph_node_features': torch.randn(3, 10)                        # (num_nodes, graph_input_dim)
    }


// File: networks.py
# File: networks.py

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple, List, Optional
from torch_geometric.nn import GCNConv

from config import EnvironmentConfig

# ============================
# Custom Activation Functions and Layers
# ============================

class APELU(nn.Module):
    def __init__(self, alpha_init: float = 0.01, beta_init: float = 1.0):
        super().__init__()
        self.alpha = nn.Parameter(torch.tensor(alpha_init, dtype=torch.float32))
        self.beta = nn.Parameter(torch.tensor(beta_init, dtype=torch.float32))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        out = torch.where(x >= 0, x, self.alpha * x * torch.exp(self.beta * x))
        out = torch.nan_to_num(out)
        return out

class MomentumActivation(nn.Module):
    def __init__(self, momentum_sensitivity: float = 1.0):
        super().__init__()
        self.momentum_sensitivity = nn.Parameter(torch.tensor(momentum_sensitivity, dtype=torch.float32))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        out = x * (1 + self.momentum_sensitivity * torch.tanh(x))
        out = torch.nan_to_num(out)
        return out

class VolatilityAdaptiveActivation(nn.Module):
    def __init__(self, initial_scale: float = 1.0):
        super().__init__()
        self.scale = nn.Parameter(torch.tensor(initial_scale, dtype=torch.float32))

    def forward(self, x: torch.Tensor, volatility: torch.Tensor) -> torch.Tensor:
        volatility = torch.nan_to_num(volatility, nan=0.0)
        out = x * (1 + self.scale * torch.tanh(volatility))
        out = torch.nan_to_num(out)
        return out

# ============================
# Specialized Layers for Financial Data
# ============================

class KLinePatternLayer(nn.Module):
    def __init__(self, hidden_dim: int):
        super().__init__()
        self.linear = nn.Linear(5, hidden_dim)  # Bullish, Bearish, Doji, Hammer, Inverted Hammer

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        patterns = self.detect_patterns(x)
        patterns = torch.nan_to_num(patterns)
        return F.relu(self.linear(patterns))

    def detect_patterns(self, x: torch.Tensor) -> torch.Tensor:
        batch_size, seq_length, _ = x.shape
        patterns = torch.zeros((batch_size, seq_length, 5), device=x.device)

        if batch_size > 1 and seq_length > 1:
            open_prices = x[:, :-1, 0]
            high_prices = x[:, :-1, 1]
            low_prices = x[:, :-1, 2]
            close_prices = x[:, :-1, 3]
            prev_open = x[:, 1:, 0]
            prev_close = x[:, 1:, 3]

            # Bullish Engulfing
            bullish = (close_prices > open_prices) & (prev_close < prev_open) & \
                      (open_prices < prev_close) & (close_prices > prev_open)

            # Bearish Engulfing
            bearish = (close_prices < open_prices) & (prev_close > prev_open) & \
                      (open_prices > prev_close) & (close_prices < prev_open)

            # Doji
            doji = torch.abs(open_prices - close_prices) < (0.05 * (high_prices - low_prices))

            # Hammer
            hammer = ((high_prices - torch.max(open_prices, close_prices)) < (0.1 * (high_prices - low_prices))) & \
                     ((torch.min(open_prices, close_prices) - low_prices) > (0.7 * (high_prices - low_prices)))

            # Inverted Hammer
            inv_hammer = ((high_prices - torch.max(open_prices, close_prices)) > (0.7 * (high_prices - low_prices))) & \
                         ((torch.min(open_prices, close_prices) - low_prices) < (0.1 * (high_prices - low_prices)))

            patterns[:, 1:, 0] = bullish.float()
            patterns[:, 1:, 1] = bearish.float()
            patterns[:, 1:, 2] = doji.float()
            patterns[:, 1:, 3] = hammer.float()
            patterns[:, 1:, 4] = inv_hammer.float()

        return patterns

class VolatilityTrackingLayer(nn.Module):
    def __init__(self, hidden_dim: int, window_size: int):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.window_size = window_size
        self.linear = nn.Linear(3, hidden_dim)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        close_prices = x[:, :, 3].unsqueeze(-1)  # (batch_size, seq_length, 1)
        volatility_measures = self.calculate_volatility_measures(close_prices, x)
        volatility_measures = torch.nan_to_num(volatility_measures)
        return F.relu(self.linear(volatility_measures))

    def calculate_volatility_measures(self, close_prices: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
        batch_size, seq_length, _ = close_prices.shape
        measures = torch.zeros((batch_size, seq_length, 3), device=close_prices.device)

        for i in range(batch_size):
            for t in range(self.window_size, seq_length):
                window = close_prices[i, t - self.window_size:t, 0]
                log_returns = torch.log(window[1:] / window[:-1] + 1e-8)
                std_dev = torch.std(log_returns)

                high = x[i, t, 1]
                low = x[i, t, 2]
                log_hl = torch.log(high / low + 1e-8)
                log_cc = log_returns
                garman_klass = torch.sqrt(torch.mean(0.5 * log_hl**2 - (2 * torch.log(torch.tensor(2.0)) - 1) * log_cc**2))
                parkinson = torch.sqrt(torch.mean(log_hl**2) / (4 * torch.log(torch.tensor(2.0))))

                measures[i, t, 0] = std_dev
                measures[i, t, 1] = garman_klass
                measures[i, t, 2] = parkinson

        return measures

class TimeWarpLayer(nn.Module):
    def __init__(self, hidden_dim: int, window_size: int):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.window_size = window_size
        self.linear = nn.Linear(hidden_dim, hidden_dim)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Implement time warping logic
        # Placeholder implementation
        return F.relu(self.linear(x))

class ExponentialMovingAverageLayer(nn.Module):
    def __init__(self, window_size: int, hidden_dim: int):
        super().__init__()
        self.window_size = window_size
        self.hidden_dim = hidden_dim
        self.linear = nn.Linear(hidden_dim, hidden_dim)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Implement EMA logic
        # Placeholder implementation
        return F.relu(self.linear(x))

class FractalDimensionLayer(nn.Module):
    def __init__(self, hidden_dim: int, max_k: int = 10, buffer_size: int = 50):
        super().__init__()
        self.linear = nn.Linear(1, hidden_dim)
        self.max_k = max_k
        self.buffer_size = buffer_size
        self.register_buffer('values_buffer', torch.zeros(buffer_size))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        close_prices = x[:, :, 3]  # (batch_size, seq_length)
        batch_size, seq_length = close_prices.shape
        hfd_values = torch.zeros((batch_size, seq_length, 1), device=x.device)

        for i in range(batch_size):
            for t in range(seq_length):
                # Update buffer: shift left and append new price
                self.values_buffer = torch.roll(self.values_buffer, shifts=-1)
                self.values_buffer[-1] = close_prices[i, t]

                if t >= self.max_k:
                    window = self.values_buffer[-self.max_k:]
                    hfd_values[i, t, 0] = self.calculate_hfd_optimized(window)

        return F.relu(self.linear(hfd_values))

    def calculate_hfd_optimized(self, arr: torch.Tensor) -> float:
        n = len(arr)
        if n < self.max_k + 1:
            return 0.0

        lk_values = torch.zeros(self.max_k, device=arr.device)
        for k in range(1, self.max_k + 1):
            for m in range(k):
                idxs = torch.arange(m, n, k, device=arr.device)
                if len(idxs) >= 2:
                    lengths = torch.abs(arr[idxs[1:]] - arr[idxs[:-1]])
                    lk_values[k - 1] += torch.sum(lengths) * (n - 1) / (len(idxs) * k)
            lk_values[k - 1] /= k

        valid_k_values = lk_values > 0
        if torch.sum(valid_k_values) > 1:
            k_arr = torch.arange(1, self.max_k + 1, device=arr.device)[valid_k_values]
            log_k = torch.log(k_arr.float())
            log_lk = torch.log(lk_values[valid_k_values])
            slope, _ = torch.polyfit(log_k, log_lk, 1)
            return -slope.item()
        else:
            return 0.0

# ============================
# Residual Block
# ============================

class ResidualBlock(nn.Module):
    """Residual block with two linear layers and a skip connection."""
    def __init__(self, input_dim: int, hidden_dim: int, dropout_rate: float = 0.1):
        super().__init__()
        self.linear1 = nn.Linear(input_dim, hidden_dim)
        self.activation = APELU()
        self.dropout = nn.Dropout(dropout_rate)
        self.linear2 = nn.Linear(hidden_dim, input_dim)
        self.norm = nn.LayerNorm(input_dim)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        residual = x
        out = self.linear1(x)
        out = self.activation(out)
        out = self.dropout(out)
        out = self.linear2(out)
        out = self.norm(out)
        out += residual
        out = self.activation(out)
        return out

# ============================
# Transformer and Attention Layers
# ============================

class TransformerEncoderLayerCustom(nn.Module):
    """Custom Transformer Encoder Layer with multi-headed attention and residual connections."""
    def __init__(self, embed_dim: int, num_heads: int, dim_feedforward: int = 2048, dropout: float = 0.1):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)
        # Implementation of Feedforward model
        self.linear1 = nn.Linear(embed_dim, dim_feedforward)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(dim_feedforward, embed_dim)

        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.activation = F.relu
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

    def forward(self, src: torch.Tensor, src_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Args:
            src: the sequence to the encoder (S, N, E)
            src_mask: the mask for the src sequence (S, S)
        """
        src2, _ = self.self_attn(src, src, src, attn_mask=src_mask)
        src = src + self.dropout1(src2)
        src = self.norm1(src)
        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
        src = src + self.dropout2(src2)
        src = self.norm2(src)
        return src

class TransformerEncoderCustom(nn.Module):
    """Custom Transformer Encoder with multiple layers."""
    def __init__(self, embed_dim: int, num_heads: int, num_layers: int, dim_feedforward: int = 2048, dropout: float = 0.1):
        super().__init__()
        self.layers = nn.ModuleList([
            TransformerEncoderLayerCustom(embed_dim, num_heads, dim_feedforward, dropout)
            for _ in range(num_layers)
        ])
        self.norm = nn.LayerNorm(embed_dim)

    def forward(self, src: torch.Tensor, src_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        for layer in self.layers:
            src = layer(src, src_mask)
        src = self.norm(src)
        return src

class MultiHeadAttentionCustom(nn.Module):
    """Multi-Head Attention Layer."""
    def __init__(self, embed_dim: int, num_heads: int, dropout: float = 0.1):
        super().__init__()
        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)
        self.norm = nn.LayerNorm(embed_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        attn_output, _ = self.multihead_attn(x, x, x, attn_mask=attn_mask)
        x = x + self.dropout(attn_output)
        x = self.norm(x)
        return x

# ============================
# MLP Classes with Enhancements
# ============================

class BaseMLP(nn.Module):
    """Base MLP with support for custom layers, normalization, and residual connections."""
    def __init__(
        self,
        input_dim: int,
        hidden_dim: int,
        output_dim: int,
        num_layers: int,
        dropout_rate: float,
        use_custom_layers: bool,
        window_size: int,
        custom_layers: Optional[List[str]] = None,
        use_instance_norm: bool = False,
        use_group_norm: bool = False,
        num_groups: int = 8,
        use_residual: bool = False,
    ):
        super().__init__()
        self.use_custom_layers = use_custom_layers
        self.use_instance_norm = use_instance_norm
        self.use_group_norm = use_group_norm
        self.use_residual = use_residual
        self.activation = APELU()
        self.dropout = nn.Dropout(dropout_rate)

        # Custom Layers
        self.custom_layers_list: List[nn.Module] = []
        if use_custom_layers:
            layer_mapping = {
                "KLinePatternLayer": lambda: KLinePatternLayer(hidden_dim),
                "VolatilityTrackingLayer": lambda: VolatilityTrackingLayer(hidden_dim, window_size),
                "TimeWarpLayer": lambda: TimeWarpLayer(hidden_dim, window_size),
                "ExponentialMovingAverageLayer": lambda: ExponentialMovingAverageLayer(window_size, hidden_dim),
                "FractalDimensionLayer": lambda: FractalDimensionLayer(hidden_dim)
            }
            if custom_layers is None:
                self.custom_layers_list = [
                    KLinePatternLayer(hidden_dim),
                    VolatilityTrackingLayer(hidden_dim, window_size),
                    TimeWarpLayer(hidden_dim, window_size),
                    ExponentialMovingAverageLayer(window_size, hidden_dim),
                    FractalDimensionLayer(hidden_dim)
                ]
            else:
                for layer_name in custom_layers:
                    if layer_name in layer_mapping:
                        self.custom_layers_list.append(layer_mapping[layer_name]())
            in_features = hidden_dim * len(self.custom_layers_list)
        else:
            in_features = input_dim

        # MLP Layers
        self.layers = nn.ModuleList()
        self.norms = nn.ModuleList()
        self.residual_blocks = nn.ModuleList() if use_residual else None

        prev_features = in_features
        for i in range(num_layers):
            out_features = output_dim if i == num_layers - 1 else hidden_dim
            self.layers.append(nn.Linear(prev_features, out_features))
            if self.use_instance_norm:
                self.norms.append(nn.InstanceNorm1d(out_features))
            elif self.use_group_norm:
                self.norms.append(nn.GroupNorm(num_groups, out_features))
            else:
                self.norms.append(nn.LayerNorm(out_features))
            if self.use_residual and i < num_layers - 1:
                self.residual_blocks.append(ResidualBlock(out_features, hidden_dim, dropout_rate))
            prev_features = out_features

    def forward(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor:
        """Forward pass with optional custom layers."""
        if self.use_custom_layers:
            outputs = []
            for cl in self.custom_layers_list:
                out = cl(x)
                outputs.append(out)
            x = torch.cat(outputs, dim=-1)

        for i, layer in enumerate(self.layers):
            x = layer(x)
            if i < len(self.norms):
                if self.use_instance_norm:
                    x = self.norms[i](x.unsqueeze(2)).squeeze(2)
                elif self.use_group_norm:
                    x = self.norms[i](x.unsqueeze(2)).squeeze(2)
                else:
                    x = self.norms[i](x)
                x = self.activation(x)
                x = self.dropout(x)
                if self.use_residual and i < len(self.residual_blocks):
                    x = self.residual_blocks[i](x)
        return x

class AdaptiveModulationMLP(BaseMLP):
    """MLP with time-aware modulation, residual connections, and advanced normalization."""
    def __init__(
        self,
        input_dim: int,
        hidden_dim: int,
        output_dim: int,
        num_layers: int,
        dropout_rate: float,
        time_encoding_dim: int,
        use_custom_layers: bool,
        window_size: int,
        custom_layers: Optional[List[str]] = None,
        use_instance_norm: bool = False,
        use_group_norm: bool = False,
        num_groups: int = 8,
        use_residual: bool = False,
    ):
        super().__init__(
            input_dim,
            hidden_dim,
            output_dim,
            num_layers,
            dropout_rate,
            use_custom_layers,
            window_size,
            custom_layers,
            use_instance_norm,
            use_group_norm,
            num_groups,
            use_residual
        )
        self.sinusoidal_encoding = SinusoidalTimeEncoding(time_encoding_dim)
        self.time_biases = nn.ModuleList([
            TimeAwareBias(hidden_dim, time_encoding_dim, hidden_dim) for _ in range(num_layers - 1)
        ])
        self.modulations = nn.ParameterList([
            nn.Parameter(torch.ones(hidden_dim)) for _ in range(num_layers - 1)
        ])

    def forward(self, x: torch.Tensor, time_step: torch.Tensor) -> torch.Tensor:
        """Forward pass with time-aware modulation."""
        time_encoding = self.sinusoidal_encoding(time_step)

        if self.use_custom_layers:
            outputs = []
            for cl in self.custom_layers_list:
                out = cl(x)
                outputs.append(out)
            x = torch.cat(outputs, dim=-1)

        for i, layer in enumerate(self.layers):
            x = layer(x)
            if i < len(self.norms):
                mod_factor = self.modulations[i] + self.time_biases[i](time_encoding)
                x = x * mod_factor
                if self.use_instance_norm:
                    x = self.norms[i](x.unsqueeze(2)).squeeze(2)
                elif self.use_group_norm:
                    x = self.norms[i](x.unsqueeze(2)).squeeze(2)
                else:
                    x = self.norms[i](x)
                x = self.activation(x)
                x = self.dropout(x)
                if self.use_residual and i < len(self.residual_blocks):
                    x = self.residual_blocks[i](x)
        return x

# ============================
# Sinusoidal Time Encoding
# ============================

class SinusoidalTimeEncoding(nn.Module):
    """Encodes time using sinusoidal functions."""
    def __init__(self, time_encoding_dim: int):
        super().__init__()
        self.time_encoding_dim = time_encoding_dim
        self.frequencies = 10**(torch.arange(0, time_encoding_dim//2) * (-2/(time_encoding_dim//2)))

    def forward(self, time_step: torch.Tensor) -> torch.Tensor:
        """Applies sinusoidal time encoding."""
        time_step = time_step.float().unsqueeze(-1)  # (batch_size, 1)
        scaled_time = time_step * self.frequencies.to(time_step.device)  # (batch_size, time_encoding_dim//2)
        sin_enc = torch.sin(scaled_time)
        cos_enc = torch.cos(scaled_time)
        if self.time_encoding_dim % 2 == 0:
            encoding = torch.cat([sin_enc, cos_enc], dim=-1)  # (batch_size, time_encoding_dim)
        else:
            zero_pad = torch.zeros_like(cos_enc[:, :1], device=cos_enc.device)
            encoding = torch.cat([sin_enc, cos_enc, zero_pad], dim=-1)
        return encoding

# ============================
# Time-Aware Bias
# ============================

class TimeAwareBias(nn.Module):
    """Learns a bias that is a function of time encoding."""
    def __init__(self, input_dim: int, time_encoding_dim: int, hidden_dim: int):
        super().__init__()
        self.time_embedding = nn.Linear(time_encoding_dim, hidden_dim)
        self.time_projection = nn.Linear(hidden_dim, input_dim)
        self.activation = APELU()

    def forward(self, time_encoding: torch.Tensor) -> torch.Tensor:
        """Applies time-aware bias."""
        x = self.time_embedding(time_encoding)
        x = self.activation(x)
        return self.time_projection(x)

# ============================
# Policy Distiller with Ensemble Methods
# ============================

class PolicyDistillerEnsemble(nn.Module):
    """Combines outputs from multiple specialist policies using an ensemble approach."""
    def __init__(self, specialist_policies: List[nn.Module], config: EnvironmentConfig):
        super().__init__()
        self.specialists = nn.ModuleList(specialist_policies)
        self.ensemble_size = config.ensemble_size
        self.mlp = nn.Linear(config.action_dim * self.ensemble_size * 2, config.action_dim * 2)  # For mu and log_sigma

    def forward(self, state: torch.Tensor, time_step: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            state: (batch_size, seq_length, state_dim)
            time_step: (batch_size, )
        Returns:
            mu: (batch_size, action_dim)
            log_sigma: (batch_size, action_dim)
        """
        outputs = [spec(state, time_step) for spec in self.specialists]  # List of (batch_size, 2 * action_dim)
        mus = torch.stack([o[:, :self.specialists[0].action_dim] for o in outputs], dim=-1)  # (batch_size, action_dim, ensemble_size)
        log_sigmas = torch.stack([o[:, self.specialists[0].action_dim:] for o in outputs], dim=-1)  # (batch_size, action_dim, ensemble_size)

        # Concatenate along the ensemble dimension
        mus = mus.view(mus.size(0), -1)  # (batch_size, action_dim * ensemble_size)
        log_sigmas = log_sigmas.view(log_sigmas.size(0), -1)  # (batch_size, action_dim * ensemble_size)

        # Pass through an MLP to aggregate
        aggregated = self.mlp(torch.cat([mus, log_sigmas], dim=-1))  # (batch_size, action_dim * 2)
        mu = torch.tanh(aggregated[:, :self.config.action_dim])
        log_sigma = torch.clamp(aggregated[:, self.config.action_dim:], min=-20, max=2)
        return mu, log_sigma

    def compute_log_prob(self, mu: torch.Tensor, log_sigma: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:
        """Computes the log probability of actions under the current policy."""
        sigma = torch.exp(log_sigma)
        dist = torch.distributions.Normal(mu, sigma)
        log_prob = dist.log_prob(actions).sum(dim=-1)
        return log_prob

# ============================
# High-Level Policy
# ============================

class HighLevelPolicy(nn.Module):
    """High-Level Policy Network for hierarchical decision-making."""
    def __init__(self, state_dim: int, hidden_dim: int):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Outputs probability of selecting a high-level action."""
        return self.mlp(x)

# ============================
# Market Mode Classifier
# ============================

class MarketModeClassifier(nn.Module):
    """Classifies the current market mode."""
    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Outputs market mode logits."""
        return self.mlp(x)

# ============================
# Meta Controller
# ============================

class MetaController(nn.Module):
    """Meta Controller for dynamic hyperparameter adjustment."""
    def __init__(self, config: EnvironmentConfig):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(config.state_dim + 2, config.hidden_dim),
            nn.ReLU(),
            nn.Linear(config.hidden_dim, config.num_hyperparams + 3),
            nn.Sigmoid()
        )
        self.num_hyperparams = config.num_hyperparams
        self.ema_smoothing = 0.9  # Smoothing factor

        self.register_buffer('ema_values', torch.zeros(config.num_hyperparams + 3))

    def forward(self, x: torch.Tensor, reward_stats: torch.Tensor) -> torch.Tensor:
        cat_input = torch.cat([x, reward_stats], dim=-1)
        out = self.mlp(cat_input)

        self.ema_values = self.ema_smoothing * self.ema_values + (1 - self.ema_smoothing) * out.detach()

        return self.ema_values


// File: replay_buffer.py
# File: replay_buffer.py

import random
from typing import List, Tuple, Any

class ReplayBuffer:
    """Stores transitions for training."""
    def __init__(self, capacity: int):
        self.capacity = capacity
        self.buffer: List[Tuple[Any, ...]] = []
        self.position: int = 0

    def add(self, state, action, reward, next_state, done, time_step):
        """Adds a transition to the buffer."""
        transition = (state, action, reward, next_state, done, time_step)
        if len(self.buffer) < self.capacity:
            self.buffer.append(transition)
        else:
            self.buffer[self.position] = transition
        self.position = (self.position + 1) % self.capacity

    def sample(self, batch_size: int) -> List[Tuple[Any, ...]]:
        """Samples a batch of transitions."""
        return random.sample(self.buffer, batch_size)

    def __len__(self) -> int:
        return len(self.buffer)


// File: reward.py
# File: reward.py

import numpy as np

def calculate_reward(current_price: float, previous_price: float, action: np.ndarray) -> float:
    """
    Calculates the reward based on the change in price and the agent's action.

    Args:
        current_price (float): The current price of the asset.
        previous_price (float): The previous price of the asset.
        action (np.ndarray): The action taken by the agent.

    Returns:
        float: Calculated reward.
    """
    price_change = current_price - previous_price
    # Assuming action[0] controls position size (e.g., buy/sell)
    position = action[0]
    # Reward is profit from position minus penalty for action magnitude
    profit = price_change * position
    penalty = np.linalg.norm(action) * 0.01  # Adjust penalty coefficient as needed
    return profit - penalty


// File: train.py
# File: train.py (Optimized Training Loop)

import numpy as np
import torch
from config import EnvironmentConfig
from env.environment import HistoricalEnvironment
from agent import MetaSACAgent
from reward import calculate_reward

from typing import Callable
import random
import logging

def get_noise_schedule(initial_noise: float, final_noise: float, decay_steps: int) -> Callable[[int], float]:
    """
    Creates a linear noise schedule.

    Args:
        initial_noise: Initial noise value.
        final_noise: Final noise value.
        decay_steps: Number of steps to decay noise over.

    Returns:
        A function that takes the current step and returns the noise value.
    """
    def noise_fn(step: int) -> float:
        if step > decay_steps:
            return final_noise
        return initial_noise - (initial_noise - final_noise) * (step / decay_steps)
    return noise_fn

def main():
    # Initialize logging
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger("MetaSACTrainer")

    # Initialize configuration
    config = EnvironmentConfig(
        state_dim=50,
        action_dim=5,
        hidden_dim=128,
        attention_dim=64,
        num_mlp_layers=3,
        dropout_rate=0.1,
        time_encoding_dim=16,
        custom_layers=["KLinePatternLayer", "VolatilityTrackingLayer", "FractalDimensionLayer"],
        window_size=20,
        num_hyperparams=10,
        graph_input_dim=10,
        graph_hidden_dim=32,
        num_graph_layers=2,
        ensemble_size=3,
        weight_decay=1e-5
    )

    # Set random seeds for reproducibility
    torch.manual_seed(42)
    np.random.seed(42)
    random.seed(42)

    # Create mock historical data
    mock_data = np.random.randn(2000, config.state_dim).astype(np.float32)

    # Initialize environment
    env = HistoricalEnvironment(mock_data)

    # Initialize agent
    agent = MetaSACAgent(config, env)

    # Warm-up replay buffer with random actions
    initial_steps = 200
    state = env.reset()
    for step in range(initial_steps):
        action = np.random.uniform(-1, 1, config.action_dim)
        next_state, reward, done, _info = env.step(action, step)
        agent.replay_buffer.add(state, action, reward, next_state, done, step)
        if done:
            state = env.reset()
        else:
            state = next_state

    # Define noise schedule
    noise_schedule = get_noise_schedule(initial_noise=0.2, final_noise=0.01, decay_steps=10000)

    # Training loop parameters
    num_epochs = 100
    updates_per_epoch = 100

    for epoch in range(num_epochs):
        epoch_losses = {}
        for _update in range(updates_per_epoch):
            # Example meta input: could be based on market indicators
            meta_input = np.random.randn(config.batch_size, config.meta_input_dim).astype(np.float32)
            time_memory = list(range(config.window_size))  # Example time memory

            losses = agent.update_params_with_training_time_search(
                replay_buffer=agent.replay_buffer,
                meta_input=meta_input,
                time_memory=time_memory,
                update_steps=1,
                search_algorithm="best-of-n",  # or "beam-search"
                num_samples=4,
                beam_width=3,
                search_depth=5,
                use_d_search=False,
                exploration_noise_std_fn=noise_schedule
            )

            # Aggregate losses for the epoch
            for key, value in losses.items():
                if key not in epoch_losses:
                    epoch_losses[key] = []
                epoch_losses[key].append(value)

        # Compute average losses for the epoch
        avg_losses = {k: np.mean(v) for k, v in epoch_losses.items()}
        logger.info(f"Epoch {epoch+1}/{num_epochs} completed, Total Steps={agent.train_steps}")
        for loss_name, loss_value in avg_losses.items():
            logger.info(f"  {loss_name}: {loss_value:.4f}")
            agent.writer.add_scalar(f"Epoch/Loss/{loss_name}", loss_value, epoch+1)

        # Save model checkpoints periodically
        if (epoch + 1) % 10 == 0:
            checkpoint_path = f"metasac_epoch_{epoch+1}.pth"
            agent.save(checkpoint_path)
            logger.info(f"Model checkpoint saved at {checkpoint_path}")

    # Final model save
    agent.save("metasac_final.pth")
    logger.info("Final model saved as metasac_final.pth")

    # Close the TensorBoard writer
    agent.writer.close()

if __name__ == "__main__":
    main()


// File: env\environment.py
# File: env/environment.py

import numpy as np
from typing import Tuple
from reward import calculate_reward

class HistoricalEnvironment:
    """Simulates stepping through historical data."""
    def __init__(self, historical_data: np.ndarray):
        self.historical_data = historical_data
        self.current_time = 0
        self.max_time = len(historical_data) - 1

    def reset(self) -> np.ndarray:
        """Resets environment to start."""
        self.current_time = 0
        return self.historical_data[self.current_time]

    def step(self, action: np.ndarray, current_time: int) -> Tuple[np.ndarray, float, bool, dict]:
        """Steps to next time, returns next_state, reward, done, info."""
        next_time = current_time + 1
        done = next_time >= self.max_time
        if not done:
            next_state = self.historical_data[next_time]
        else:
            next_state = np.zeros_like(self.historical_data[0])
        
        # Calculate reward using the reward function
        previous_price = self.historical_data[current_time, 3]  # Assuming close price is at index 3
        current_price = self.historical_data[next_time, 3] if not done else previous_price
        reward = calculate_reward(current_price, previous_price, action)
        
        return next_state, reward, done, {}


// File: tests\test_agent.py
# File: tests/test_agent.py

import pytest
import torch
import numpy as np
from config import EnvironmentConfig
from env.environment import HistoricalEnvironment
from agent import MetaSACAgent

@pytest.fixture
def agent_instance():
    """
    Fixture to initialize the MetaSACAgent with mock data.
    """
    config = EnvironmentConfig(
        state_dim=50,
        action_dim=5,
        hidden_dim=128,
        attention_dim=64,
        num_mlp_layers=3,
        dropout_rate=0.1,
        time_encoding_dim=16,
        custom_layers=["KLinePatternLayer", "VolatilityTrackingLayer", "FractalDimensionLayer"],
        window_size=20,
        num_hyperparams=10,
        graph_input_dim=10,
        graph_hidden_dim=32,
        num_graph_layers=2,
        ensemble_size=3,
        weight_decay=1e-5
    )
    mock_data = np.random.randn(1000, config.state_dim).astype(np.float32)
    env = HistoricalEnvironment(mock_data)
    agent = MetaSACAgent(config, env)
    # Populate replay buffer with random data
    for _ in range(config.buffer_capacity // 10):  # Add a fraction to avoid filling it up
        state = env.reset()
        for step in range(10):
            action = np.random.uniform(-1, 1, config.action_dim)
            next_state, reward, done, _ = env.step(action, step)
            agent.replay_buffer.add(state, action, reward, next_state, done, step)
            if done:
                break
            state = next_state
    return agent

@pytest.fixture
def sample_batch():
    """
    Fixture to provide a sample batch of data for testing.
    """
    return {
        'states': np.random.randn(32, 20, 50).astype(np.float32),  # (batch_size, seq_length, state_dim)
        'actions': np.random.randn(32, 5).astype(np.float32),     # (batch_size, action_dim)
        'rewards': np.random.randn(32, 1).astype(np.float32),     # (batch_size, 1)
        'next_states': np.random.randn(32, 20, 50).astype(np.float32),  # (batch_size, seq_length, state_dim)
        'dones': np.random.randint(0, 2, (32, 1)).astype(np.float32),    # (batch_size, 1)
        'time_steps': np.random.randint(0, 1000, (32,)),                 # (batch_size,)
        'edge_index': torch.tensor([[0, 1, 2], [1, 2, 0]], dtype=torch.long),  # Example edge index
        'graph_node_features': torch.randn(3, 10)                        # (num_nodes, graph_input_dim)
    }

def test_select_action(agent_instance):
    state = np.random.randn(20, 50).astype(np.float32)  # (seq_length, state_dim)
    time_step = 100
    edge_index = torch.tensor([[0, 1, 2], [1, 2, 0]], dtype=torch.long)
    graph_node_features = torch.randn(3, 10)  # (num_nodes, graph_input_dim)
    action = agent_instance.select_action(state, time_step, edge_index, graph_node_features, eval=True)
    assert action.shape == (5,), "Action shape mismatch"
    assert np.all(action >= -1) and np.all(action <= 1), "Action values out of bounds"

def test_update_params(agent_instance, sample_batch):
    states = torch.FloatTensor(sample_batch['states']).to(agent_instance.device)
    actions = torch.FloatTensor(sample_batch['actions']).to(agent_instance.device)
    rewards = torch.FloatTensor(sample_batch['rewards']).to(agent_instance.device)
    next_states = torch.FloatTensor(sample_batch['next_states']).to(agent_instance.device)
    dones = torch.FloatTensor(sample_batch['dones']).to(agent_instance.device)
    time_steps = torch.FloatTensor(sample_batch['time_steps']).to(agent_instance.device)

    # Add to replay buffer
    for i in range(states.shape[0]):
        agent_instance.replay_buffer.add(
            states[i].cpu().numpy(),
            actions[i].cpu().numpy(),
            rewards[i].cpu().numpy(),
            next_states[i].cpu().numpy(),
            dones[i].cpu().numpy(),
            int(time_steps[i].item())
        )

    meta_input = np.random.randn(agent_instance.config.batch_size, agent_instance.config.meta_input_dim).astype(np.float32)
    time_memory = list(range(agent_instance.config.window_size))

    losses = agent_instance.update_params_with_training_time_search(
        replay_buffer=agent_instance.replay_buffer,
        meta_input=meta_input,
        time_memory=time_memory,
        update_steps=1
    )

    assert 'actor_loss' in losses, "Missing actor_loss in losses"
    assert 'critic1_loss' in losses, "Missing critic1_loss in losses"
    assert 'critic2_loss' in losses, "Missing critic2_loss in losses"
    assert 'meta_loss' in losses, "Missing meta_loss in losses"
    assert 'distiller_loss' in losses, "Missing distiller_loss in losses"
    assert 'market_mode_loss' in losses, "Missing market_mode_loss in losses"
    assert 'high_level_loss' in losses, "Missing high_level_loss in losses"
    assert 'alpha' in losses, "Missing alpha in losses"

    # Additional Assertions
    assert isinstance(losses['actor_loss'], float), "actor_loss should be a float"
    assert isinstance(losses['critic1_loss'], float), "critic1_loss should be a float"
    assert isinstance(losses['critic2_loss'], float), "critic2_loss should be a float"
    assert isinstance(losses['meta_loss'], float), "meta_loss should be a float"
    assert isinstance(losses['distiller_loss'], float), "distiller_loss should be a float"
    assert isinstance(losses['market_mode_loss'], float), "market_mode_loss should be a float"
    assert isinstance(losses['high_level_loss'], float), "high_level_loss should be a float"
    assert isinstance(losses['alpha'], float), "alpha should be a float"

def test_model_save_load(agent_instance, tmp_path):
    """
    Test saving and loading of the agent model.
    """
    save_path = tmp_path / "metasac_test.pth"
    agent_instance.save(str(save_path))

    # Create a new agent instance and load the saved state
    config = EnvironmentConfig(
        state_dim=50,
        action_dim=5,
        hidden_dim=128,
        attention_dim=64,
        num_mlp_layers=3,
        dropout_rate=0.1,
        time_encoding_dim=16,
        custom_layers=["KLinePatternLayer", "VolatilityTrackingLayer", "FractalDimensionLayer"],
        window_size=20,
        num_hyperparams=10,
        graph_input_dim=10,
        graph_hidden_dim=32,
        num_graph_layers=2,
        ensemble_size=3,
        weight_decay=1e-5
    )
    mock_data = np.random.randn(1000, config.state_dim).astype(np.float32)
    env = HistoricalEnvironment(mock_data)
    new_agent = MetaSACAgent(config, env)

    # Load the saved state
    new_agent.load(str(save_path))

    # Verify that parameters match
    for param1, param2 in zip(agent_instance.parameters(), new_agent.parameters()):
        assert torch.allclose(param1, param2), "Model parameters do not match after loading."


