**File Tree (Relevant Files Only)**
  .
    - main.py
  config
    - config.py
    - __init__.py
  data
    - features.py
    - processor.py
    - __init__.py
  data\raw
    - btc_15m_data_2018_to_2024-2024-10-10.csv
    - btc_1d_data_2018_to_2024-2024-10-10.csv
    - btc_1h_data_2018_to_2024-2024-10-10.csv
    - btc_4h_data_2018_to_2024-2024-10-10.csv
  models
    - nlnn.py
    - __init__.py
  utils
    - activations.py
    - hardware_config.py
    - logging.py
    - monitoring.py
    - __init__.py
// File: main.py
import argparse
from pathlib import Path
import torch
from torch.cuda.amp import autocast
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

from config.config import NLNNConfig
from models.nlnn import NLNN
from training.trainer import NLNNTrainer
from utils.logging import get_logger
from utils.monitoring import ResourceMonitor
from utils.hardware_config import HardwareConfig, OptimizedDataPipeline, create_optimized_config

def parse_args():
    parser = argparse.ArgumentParser(description="Train a Hardware-Optimized Normalized Liquid Neural Network")
    parser.add_argument('--config', type=str, default='config/config.json', help='Path to the config JSON file')
    parser.add_argument('--data', type=str, default='data/raw/btc_15m_data_2018_to_2024-2024-10-10.csv', help='Path to the training data CSV file')
    parser.add_argument('--epochs', type=int, default=50, help='Number of training epochs')
    parser.add_argument('--save_dir', type=str, default='checkpoints', help='Directory to save model checkpoints')
    parser.add_argument('--log_dir', type=str, default='logs', help='Directory to save logs')
    return parser.parse_args()

def create_datasets(filepath: str, sequence_length: int, hw_config: HardwareConfig):
    df = pd.read_csv(filepath, parse_dates=['Open time'])
    df = df.sort_values('Open time')

    features = ['Open', 'High', 'Low', 'Close', 'Volume']
    data = df[features].values

    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(data)

    X, y = [], []
    for i in range(len(data_scaled) - sequence_length):
        X.append(data_scaled[i:i+sequence_length])
        y.append(data_scaled[i+sequence_length, 3])

    X = torch.tensor(X, dtype=torch.float32)
    y = torch.tensor(y, dtype=torch.float32)

    train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, shuffle=False)

    train_dataset = torch.utils.data.TensorDataset(
        hw_config.optimize_tensor(train_X),
        hw_config.optimize_tensor(train_y)
    )
    val_dataset = torch.utils.data.TensorDataset(
        hw_config.optimize_tensor(val_X),
        hw_config.optimize_tensor(val_y)
    )

    return train_dataset, val_dataset

def main():
    args = parse_args()
    logger = get_logger('n-LNN', log_dir=Path(args.log_dir))
    
    hw_config = create_optimized_config()
    logger.info(f"Using device: {hw_config.device}")
    logger.info(f"Batch size: {hw_config.batch_size} (effective: {hw_config.effective_batch_size})")
    
    config_path = Path(args.config)
    if config_path.exists():
        config = NLNNConfig.load(config_path)
    else:
        config = NLNNConfig()
        config.save(config_path)
    
    config.batch_size = hw_config.batch_size
    config.num_threads = hw_config.torch_threads
    config.use_mixed_precision = True
    
    logger.info("Creating datasets...")
    train_dataset, val_dataset = create_datasets(
        filepath=args.data,
        sequence_length=config.sequence_length,
        hw_config=hw_config
    )
    
    logger.info("Setting up data pipeline...")
    data_pipeline = OptimizedDataPipeline(hw_config)
    train_loader, val_loader = data_pipeline.create_dataloaders(
        train_dataset,
        val_dataset
    )
    
    logger.info("Initializing model...")
    model = NLNN(config)
    model = model.to(
        device=hw_config.device,
        memory_format=hw_config.memory_format
    )
    
    logger.info("Initializing trainer...")
    trainer = NLNNTrainer(
        model=model,
        config=config
    )
    
    logger.info(f"Starting training for {args.epochs} epochs...")
    trainer.train(
        train_dataloader=train_loader,
        num_epochs=args.epochs,
        val_dataloader=val_loader,
        save_dir=Path(args.save_dir)
    )
    
    logger.info(f"Final GPU memory stats: {hw_config.memory_stats()}")

if __name__ == "__main__":
    main()

// File: config\config.py
import argparse
import sys
from pathlib import Path
from loguru import logger
import torch
from typing import Dict, List, Optional, Tuple
import json

class ProcessorConfig:
    """Configuration for data processing"""
    def __init__(
        self,
        raw_dir: Path = Path("data/raw"),
        processed_dir: Path = Path("data/processed"),
        timeframes: List[str] = ['15m', '1h', '4h', '1d'],
        sequence_length: int = 128,
        batch_size: int = 512,
        train_ratio: float = 0.8,
        num_threads: int = 12,
        use_cuda: bool = torch.cuda.is_available()
    ):
        self.raw_dir = raw_dir
        self.processed_dir = processed_dir
        self.timeframes = timeframes
        self.sequence_length = sequence_length
        self.batch_size = batch_size
        self.train_ratio = train_ratio
        self.num_threads = num_threads
        self.use_cuda = use_cuda

def setup_logger():
    """Configure logging"""
    logger.remove()  # Remove default handler
    logger.add(
        sys.stdout,
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>"
    )
    logger.add(
        "logs/data_processor.log",
        rotation="500 MB",
        retention="10 days"
    )

def process_single_timeframe(processor, timeframe: str, config: ProcessorConfig) -> Dict:
    """Process a single timeframe and return stats"""
    try:
        logger.info(f"Processing timeframe: {timeframe}")
        
        start_time = time.time()
        X_train, y_train, X_val, y_val = processor.process_timeframe_distributed(timeframe)
        processing_time = time.time() - start_time
        
        stats = {
            'timeframe': timeframe,
            'train_samples': len(X_train),
            'val_samples': len(X_val),
            'feature_dim': X_train.shape[-1],
            'sequence_length': X_train.shape[1],
            'processing_time': processing_time,
            'memory_used': processor.get_memory_stats()
        }
        
        logger.info(f"Processed {timeframe} in {processing_time:.2f}s")
        logger.info(f"Train samples: {stats['train_samples']}, Val samples: {stats['val_samples']}")
        
        return stats
        
    except Exception as e:
        logger.error(f"Error processing {timeframe}: {str(e)}")
        raise

def process_all_timeframes(processor, config: ProcessorConfig) -> List[Dict]:
    """Process all timeframes and return stats"""
    stats = []
    
    for timeframe in config.timeframes:
        try:
            timeframe_stats = process_single_timeframe(processor, timeframe, config)
            stats.append(timeframe_stats)
        except Exception as e:
            logger.error(f"Skipping {timeframe} due to error: {str(e)}")
            continue
            
    return stats

def save_stats(stats: List[Dict], output_dir: Path):
    """Save processing statistics"""
    output_dir.mkdir(parents=True, exist_ok=True)
    output_file = output_dir / "processing_stats.json"
    
    with open(output_file, 'w') as f:
        json.dump(stats, f, indent=2)
    
    logger.info(f"Saved processing stats to {output_file}")

def create_data_visualizations(stats: List[Dict], output_dir: Path):
    """Create visualizations of the processed data"""
    try:
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        # Set up the plotting style
        plt.style.use('seaborn')
        
        # Processing time comparison
        plt.figure(figsize=(10, 6))
        times = [stat['processing_time'] for stat in stats]
        timeframes = [stat['timeframe'] for stat in stats]
        plt.bar(timeframes, times)
        plt.title('Processing Time by Timeframe')
        plt.xlabel('Timeframe')
        plt.ylabel('Time (seconds)')
        plt.savefig(output_dir / 'processing_times.png')
        plt.close()
        
        # Sample distribution
        plt.figure(figsize=(10, 6))
        train_samples = [stat['train_samples'] for stat in stats]
        val_samples = [stat['val_samples'] for stat in stats]
        x = np.arange(len(timeframes))
        width = 0.35
        
        plt.bar(x - width/2, train_samples, width, label='Train')
        plt.bar(x + width/2, val_samples, width, label='Validation')
        plt.xticks(x, timeframes)
        plt.title('Sample Distribution by Timeframe')
        plt.xlabel('Timeframe')
        plt.ylabel('Number of Samples')
        plt.legend()
        plt.savefig(output_dir / 'sample_distribution.png')
        plt.close()
        
        logger.info(f"Saved visualizations to {output_dir}")
        
    except Exception as e:
        logger.warning(f"Could not create visualizations: {str(e)}")

def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(description='Process cryptocurrency data across multiple timeframes')
    parser.add_argument('--raw-dir', type=str, default='data/raw', help='Directory containing raw data files')
    parser.add_argument('--processed-dir', type=str, default='data/processed', help='Directory for processed data')
    parser.add_argument('--timeframes', type=str, nargs='+', default=['15m', '1h', '4h', '1d'], help='Timeframes to process')
    parser.add_argument('--sequence-length', type=int, default=128, help='Sequence length for time series')
    parser.add_argument('--batch-size', type=int, default=512, help='Batch size for data loading')
    parser.add_argument('--train-ratio', type=float, default=0.8, help='Train/validation split ratio')
    parser.add_argument('--num-threads', type=int, default=12, help='Number of processing threads')
    parser.add_argument('--output-dir', type=str, default='outputs', help='Directory for outputs and visualizations')
    parser.add_argument('--debug', action='store_true', help='Enable debug mode')
    
    args = parser.parse_args()
    
    # Setup logging
    setup_logger()
    if args.debug:
        logger.level("DEBUG")
    
    # Create configuration
    config = ProcessorConfig(
        raw_dir=Path(args.raw_dir),
        processed_dir=Path(args.processed_dir),
        timeframes=args.timeframes,
        sequence_length=args.sequence_length,
        batch_size=args.batch_size,
        train_ratio=args.train_ratio,
        num_threads=args.num_threads
    )
    
    # Initialize processor
    try:
        logger.info("Initializing data processor...")
        from enhanced_processor import EnhancedDataProcessor  # Import your processor class
        processor = EnhancedDataProcessor(config)
        
        # Process timeframes
        logger.info("Starting data processing...")
        stats = process_all_timeframes(processor, config)
        
        # Save results
        output_dir = Path(args.output_dir)
        save_stats(stats, output_dir)
        create_data_visualizations(stats, output_dir)
        
        logger.info("Data processing completed successfully")
        
    except Exception as e:
        logger.error(f"Error during processing: {str(e)}")
        if args.debug:
            import traceback
            logger.debug(traceback.format_exc())
        sys.exit(1)
    finally:
        if 'processor' in locals():
            processor.cleanup()

if __name__ == "__main__":
    main()

// File: config\__init__.py


// File: data\features.py
import numpy as np
import pandas as pd
from typing import Dict, List, Optional
import ta
from dataclasses import dataclass
from numba import jit

@dataclass
class FeatureConfig:
    """Configuration for feature calculation"""
    window_sizes: List[int] = (5, 10, 20, 50, 100)
    rsi_period: int = 14
    bb_period: int = 20
    epsilon: float = 1e-8
    use_ta_lib: bool = True

@jit(nopython=True)
def calculate_returns(prices: np.ndarray, epsilon: float = 1e-8) -> np.ndarray:
    """Calculate log returns with Numba acceleration"""
    return np.log(np.maximum(prices[1:] / prices[:-1], epsilon))

class FeatureCalculator:
    """Unified feature calculator for price data"""
    
    def __init__(self, config: FeatureConfig):
        self.config = config
    
    def calculate_price_features(self, df: pd.DataFrame) -> Dict[str, np.ndarray]:
        """Calculate basic price-based features"""
        features = {}
        
        # Returns and volatility
        closes = df['Close'].values
        features['log_returns'] = np.pad(calculate_returns(closes), (1, 0))
        features['volatility'] = pd.Series(features['log_returns']).rolling(20).std().fillna(0).values
        
        # Price ratios
        features['high_low_ratio'] = (df['High'] / df['Low']).values
        features['close_open_ratio'] = (df['Close'] / df['Open']).values
        
        # Price position
        range_denominator = (df['High'] - df['Low']).values + self.config.epsilon
        features['price_position'] = ((df['Close'] - df['Low']) / range_denominator).values
        
        return features
    
    def calculate_volume_features(self, df: pd.DataFrame) -> Dict[str, np.ndarray]:
        """Calculate volume-based features"""
        features = {}
        
        # Volume momentum
        volumes = df['Volume'].values
        features['volume_momentum'] = np.pad(np.diff(volumes), (1, 0))
        
        # Volume intensity
        price_range = (df['High'] - df['Low']).values
        features['volume_intensity'] = volumes * price_range
        
        # Volume moving averages
        volume_series = pd.Series(volumes)
        for window in self.config.window_sizes:
            features[f'volume_ma_{window}'] = volume_series.rolling(window).mean().fillna(0).values
            features[f'volume_std_{window}'] = volume_series.rolling(window).std().fillna(0).values
        
        return features
    
    def calculate_technical_features(self, df: pd.DataFrame) -> Dict[str, np.ndarray]:
        """Calculate technical indicators"""
        features = {}
        
        if self.config.use_ta_lib:
            # Momentum indicators
            features['rsi'] = ta.momentum.rsi(df['Close'], window=self.config.rsi_period).fillna(0).values
            
            # Trend indicators
            for window in self.config.window_sizes:
                features[f'sma_{window}'] = ta.trend.sma_indicator(df['Close'], window=window).fillna(0).values
                features[f'ema_{window}'] = ta.trend.ema_indicator(df['Close'], window=window).fillna(0).values
            
            # Volatility indicators
            bb_high = ta.volatility.bollinger_hband(df['Close'], window=self.config.bb_period)
            bb_low = ta.volatility.bollinger_lband(df['Close'], window=self.config.bb_period)
            features['bb_width'] = ((bb_high - bb_low) / df['Close']).fillna(0).values
            
            features['atr'] = ta.volatility.average_true_range(
                df['High'], df['Low'], df['Close']
            ).fillna(0).values
        
        return features
    
    def calculate_all_features(self, df: pd.DataFrame) -> Dict[str, np.ndarray]:
        """Calculate all features"""
        features = {}
        
        # Calculate each feature group
        features.update(self.calculate_price_features(df))
        features.update(self.calculate_volume_features(df))
        features.update(self.calculate_technical_features(df))
        
        # Ensure all features are numpy arrays
        for key, value in features.items():
            if isinstance(value, pd.Series):
                features[key] = value.values
        
        return features
    
    def get_feature_names(self) -> List[str]:
        """Get list of all feature names"""
        # Create a small sample dataframe to get feature names
        sample_df = pd.DataFrame({
            'Open': [1] * 10,
            'High': [1] * 10,
            'Low': [1] * 10,
            'Close': [1] * 10,
            'Volume': [1] * 10
        })
        
        return list(self.calculate_all_features(sample_df).keys())

// File: data\processor.py
import modin.pandas as mpd
import cupy as cp
import dask.array as da
import numpy as np
import torch
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from sklearn.preprocessing import RobustScaler
import ray
from loguru import logger
from numba import jit, cuda
import psutil
from functools import partial
from concurrent.futures import ThreadPoolExecutor

@ray.remote
class DistributedFeatureCalculator:
    """Distributed feature calculation using Ray"""
    def __init__(self, config):
        self.config = config
        
    def calculate_features(self, chunk):
        return self.calculate_all_features(chunk)

class EnhancedDataProcessor:
    """Enhanced data processor with distributed computing and GPU acceleration"""
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Initialize Ray for distributed processing
        if not ray.is_initialized():
            ray.init(num_cpus=psutil.cpu_count())
        
        # Configure logging
        logger.add(
            f"logs/data_processor_{self.config.timeframes[0]}.log",
            rotation="500 MB"
        )
        
        # Initialize caches and buffers
        self._data_cache = {}
        self._feature_cache = {}
        self._scaler_cache = {}
        
        # Initialize GPU memory pool if available
        if torch.cuda.is_available():
            self.gpu_memory_pool = cp.cuda.MemoryPool()
            cp.cuda.set_allocator(self.gpu_memory_pool.malloc)
    
    def _load_timeframe_optimized(self, timeframe: str) -> mpd.DataFrame:
        """Load timeframe data using Modin for parallel processing"""
        if timeframe not in self._data_cache:
            file_path = self.config.raw_dir / f"btc_{timeframe}_data_2018_to_2024-2024-10-10.csv"
            
            # Define dtypes for efficient memory usage
            dtypes = {
                'Open': np.float32,
                'High': np.float32,
                'Low': np.float32,
                'Close': np.float32,
                'Volume': np.float32,
                'Quote asset volume': np.float32,
                'Number of trades': np.int32,
                'Taker buy base asset volume': np.float32,
                'Taker buy quote asset volume': np.float32
            }
            
            # Use Modin's parallel read_csv
            df = mpd.read_csv(
                file_path,
                parse_dates=['Open time'],
                dtype=dtypes,
                engine='pyarrow'  # Use PyArrow engine for better performance
            )
            
            df.set_index('Open time', inplace=True)
            self._data_cache[timeframe] = df.sort_index()
            
            logger.info(f"Loaded {timeframe} data: {len(df)} rows")
            
        return self._data_cache[timeframe]
    
    @staticmethod
    @jit(nopython=True, parallel=True)
    def _prepare_sequences_numba(feature_array: np.ndarray, sequence_length: int) -> Tuple[np.ndarray, np.ndarray]:
        """Prepare sequences using Numba acceleration"""
        n_sequences = len(feature_array) - sequence_length
        X = np.empty((n_sequences, sequence_length, feature_array.shape[1]), dtype=np.float32)
        y = np.empty(n_sequences, dtype=np.float32)
        
        for i in range(n_sequences):
            X[i] = feature_array[i:i + sequence_length]
            y[i] = feature_array[i + sequence_length, 0]
            
        return X, y
    
    def _normalize_features_gpu(
        self,
        features: Dict[str, np.ndarray],
        scaler: Optional[RobustScaler] = None,
        fit: bool = True
    ) -> Tuple[Dict[str, np.ndarray], RobustScaler]:
        """Normalize features using GPU acceleration"""
        # Convert features to CuPy array
        feature_array = cp.array(np.column_stack(list(features.values())), dtype=cp.float32)
        
        if scaler is None and fit:
            scaler = RobustScaler()
            normalized = cp.array(scaler.fit_transform(feature_array.get()))
        else:
            normalized = cp.array(scaler.transform(feature_array.get()))
        
        # Convert back to dictionary
        normalized_features = {}
        for i, key in enumerate(features.keys()):
            normalized_features[key] = normalized[:, i].get()
            
        return normalized_features, scaler
    
    def process_timeframe_distributed(
        self,
        timeframe: str,
        save: bool = True
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """Process timeframe data using distributed computing"""
        logger.info(f"Processing {timeframe} timeframe...")
        
        # Load data using Modin
        df = self._load_timeframe_optimized(timeframe)
        
        # Distribute feature calculation using Ray
        num_partitions = psutil.cpu_count()
        df_splits = np.array_split(df, num_partitions)
        
        # Create remote feature calculators
        calculators = [DistributedFeatureCalculator.remote(self.config) for _ in range(num_partitions)]
        
        # Calculate features in parallel
        feature_futures = [calculator.calculate_features.remote(split) for calculator, split in zip(calculators, df_splits)]
        feature_results = ray.get(feature_futures)
        
        # Merge feature results
        features = {}
        for key in feature_results[0].keys():
            features[key] = np.concatenate([result[key] for result in feature_results])
        
        # Normalize features using GPU
        if timeframe not in self._scaler_cache:
            features, scaler = self._normalize_features_gpu(features, fit=True)
            self._scaler_cache[timeframe] = scaler
        else:
            features, _ = self._normalize_features_gpu(features, self._scaler_cache[timeframe], fit=False)
        
        # Prepare sequences using Numba
        X, y = self._prepare_sequences_numba(
            np.column_stack(list(features.values())),
            self.config.sequence_length
        )
        
        # Convert to torch tensors with optimal memory format
        X = torch.from_numpy(X).to(
            device=self.device,
            memory_format=torch.channels_last
        )
        y = torch.from_numpy(y).to(self.device)
        
        # Split into train and validation
        split_idx = int(len(X) * self.config.train_ratio)
        X_train, X_val = X[:split_idx], X[split_idx:]
        y_train, y_val = y[:split_idx], y[split_idx:]
        
        if save:
            self._save_processed_data(timeframe, (X_train, y_train, X_val, y_val))
        
        return X_train, y_train, X_val, y_val
    
    def create_optimized_dataloaders(
        self,
        X_train: torch.Tensor,
        y_train: torch.Tensor,
        X_val: torch.Tensor,
        y_val: torch.Tensor
    ) -> Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader]:
        """Create optimized dataloaders with prefetching and pinned memory"""
        
        class OptimizedDataset(torch.utils.data.Dataset):
            def __init__(self, X, y):
                self.X = X
                self.y = y
                
            def __len__(self):
                return len(self.X)
                
            def __getitem__(self, idx):
                return self.X[idx], self.y[idx]
        
        # Calculate optimal batch size based on available memory
        gpu_mem = torch.cuda.get_device_properties(0).total_memory if torch.cuda.is_available() else 0
        system_mem = psutil.virtual_memory().total
        
        optimal_batch_size = min(
            self.config.batch_size,
            int(min(gpu_mem, system_mem) * 0.1 / (X_train[0].numel() * X_train[0].element_size()))
        )
        
        logger.info(f"Using optimal batch size: {optimal_batch_size}")
        
        train_dataset = OptimizedDataset(X_train, y_train)
        val_dataset = OptimizedDataset(X_val, y_val)
        
        train_loader = torch.utils.data.DataLoader(
            train_dataset,
            batch_size=optimal_batch_size,
            shuffle=True,
            num_workers=psutil.cpu_count() // 2,
            pin_memory=True,
            persistent_workers=True,
            prefetch_factor=2,
            generator=torch.Generator(device=self.device)
        )
        
        val_loader = torch.utils.data.DataLoader(
            val_dataset,
            batch_size=optimal_batch_size * 2,
            shuffle=False,
            num_workers=psutil.cpu_count() // 4,
            pin_memory=True,
            persistent_workers=True,
            prefetch_factor=2
        )
        
        return train_loader, val_loader
    
    def cleanup(self):
        """Clean up resources"""
        # Clear caches
        self._data_cache.clear()
        self._feature_cache.clear()
        self._scaler_cache.clear()
        
        # Clean up GPU memory
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            self.gpu_memory_pool.free_all_blocks()
        
        # Shutdown Ray
        if ray.is_initialized():
            ray.shutdown()
            
        logger.info("Cleaned up all resources")

// File: data\__init__.py
from .processor import DataProcessor
from .features import FeatureCalculator

__all__ = ['DataProcessor', 'FeatureCalculator']

// File: data\raw\btc_15m_data_2018_to_2024-2024-10-10.csv
// Snippet:
          Open time     Open     High      Low    Close     Volume              Close time  Quote asset volume  Number of trades  Taker buy base asset volume  Taker buy quote asset volume  Ignore
2018-01-01 00:00:00 13715.65 13715.65 13400.01 13556.15 123.616013 2018-01-01 00:14:59.999        1.675545e+06              1572                    63.227133                  8.576108e+05       0
2018-01-01 00:15:00 13533.75 13550.87 13402.00 13521.12  98.136430 2018-01-01 00:29:59.999        1.321757e+06              1461                    47.686389                  6.422812e+05       0
2018-01-01 00:30:00 13500.00 13545.37 13450.00 13470.41  79.904037 2018-01-01 00:44:59.999        1.078825e+06              1000                    43.710406                  5.900347e+05       0
2018-01-01 00:45:00 13494.65 13690.87 13450.00 13529.01 141.699719 2018-01-01 00:59:59.999        1.917783e+06              1195                    73.897993                  1.000614e+06       0
2018-01-01 01:00:00 13528.99 13571.74 13402.28 13445.63  72.537533 2018-01-01 01:14:59.999        9.778198e+05               898                    34.257652                  4.618369e+05       0

// File: data\raw\btc_1d_data_2018_to_2024-2024-10-10.csv
// Snippet:
 Open time     Open     High      Low    Close       Volume              Close time  Quote asset volume  Number of trades  Taker buy base asset volume  Taker buy quote asset volume  Ignore
2018-01-01 13715.65 13818.55 12750.00 13380.00  8609.915844 2018-01-01 23:59:59.999        1.147997e+08            105595                  3961.938946                  5.280975e+07       0
2018-01-02 13382.16 15473.49 12890.02 14675.11 20078.092111 2018-01-02 23:59:59.999        2.797171e+08            177728                 11346.326739                  1.580801e+08       0
2018-01-03 14690.00 15307.56 14150.00 14919.51 15905.667639 2018-01-03 23:59:59.999        2.361169e+08            162787                  8994.953566                  1.335873e+08       0
2018-01-04 14919.51 15280.00 13918.04 15059.54 21329.649574 2018-01-04 23:59:59.999        3.127816e+08            170310                 12680.812951                  1.861168e+08       0
2018-01-05 15059.56 17176.24 14600.00 16960.39 23251.491125 2018-01-05 23:59:59.999        3.693220e+08            192969                 13346.622293                  2.118299e+08       0

// File: data\raw\btc_1h_data_2018_to_2024-2024-10-10.csv
// Snippet:
          Open time     Open     High      Low    Close     Volume              Close time  Quote asset volume  Number of trades  Taker buy base asset volume  Taker buy quote asset volume  Ignore
2018-01-01 00:00:00 13715.65 13715.65 13400.01 13529.01 443.356199 2018-01-01 00:59:59.999        5.993910e+06              5228                   228.521921                  3.090541e+06       0
2018-01-01 01:00:00 13528.99 13595.89 13155.38 13203.06 383.697006 2018-01-01 01:59:59.999        5.154522e+06              4534                   180.840403                  2.430449e+06       0
2018-01-01 02:00:00 13203.00 13418.43 13200.00 13330.18 429.064572 2018-01-01 02:59:59.999        5.710192e+06              4887                   192.237935                  2.558505e+06       0
2018-01-01 03:00:00 13330.26 13611.27 13290.00 13410.03 420.087030 2018-01-01 03:59:59.999        5.657448e+06              4789                   137.918407                  1.858041e+06       0
2018-01-01 04:00:00 13434.98 13623.29 13322.15 13601.01 340.807329 2018-01-01 04:59:59.999        4.588047e+06              4563                   172.957635                  2.328058e+06       0

// File: data\raw\btc_4h_data_2018_to_2024-2024-10-10.csv
// Snippet:
          Open time     Open     High      Low    Close      Volume              Close time  Quote asset volume  Number of trades  Taker buy base asset volume  Taker buy quote asset volume  Ignore
2018-01-01 00:00:00 13715.65 13715.65 13155.38 13410.03 1676.204807 2018-01-01 03:59:59.999        2.251607e+07             19438                   739.518666                  9.937537e+06       0
2018-01-01 04:00:00 13434.98 13818.55 13322.15 13570.35 1302.214836 2018-01-01 07:59:59.999        1.770254e+07             18061                   588.516222                  7.998796e+06       0
2018-01-01 08:00:00 13569.98 13735.24 13001.13 13220.56 1319.755931 2018-01-01 11:59:59.999        1.765121e+07             17200                   582.668293                  7.792109e+06       0
2018-01-01 12:00:00 13220.56 13330.00 12750.00 13247.00 1831.933153 2018-01-01 15:59:59.999        2.391259e+07             21357                   830.314747                  1.083304e+07       0
2018-01-01 16:00:00 13247.00 13290.65 12940.00 13240.37 1092.337234 2018-01-01 19:59:59.999        1.434887e+07             13178                   557.453779                  7.324320e+06       0

// File: models\nlnn.py
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.cuda.amp import autocast, GradScaler
from typing import Optional, Tuple, List
from dataclasses import dataclass

@dataclass
class LN2Config:
    """Enhanced configuration for LN² model with hardware optimization settings"""
    feature_dim: int
    hidden_dim: int
    output_dim: int
    num_layers: int
    sequence_length: int
    batch_size: int
    dropout: float = 0.1
    learning_rate: float = 1e-3
    epsilon: float = 1e-8
    eigen_adaptation_rate: float = 0.01
    slerp_rate: float = 0.1
    use_mixed_precision: bool = True
    use_slerp: bool = True

class SphericalLinear(nn.Module):
    """Optimized linear layer with weight normalization on hypersphere"""
    def __init__(self, input_size: int, output_size: int, epsilon: float = 1e-8):
        super().__init__()
        scale = (6.0 / (input_size + output_size)) ** 0.5
        self.weight = nn.Parameter(torch.randn(output_size, input_size) * scale)
        self.scale = nn.Parameter(torch.ones(output_size))
        self.epsilon = epsilon
        self._normalize_weights()

    def _normalize_weights(self):
        with torch.no_grad():
            norm = torch.norm(self.weight, dim=1, keepdim=True)
            self.weight.div_(norm + self.epsilon)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        self._normalize_weights()
        return self.scale.unsqueeze(0) * F.linear(x, self.weight)

class OptimizedSLERP(nn.Module):
    """Enhanced SLERP implementation with numerical stability"""
    def __init__(self, size: int, epsilon: float = 1e-8):
        super().__init__()
        self.alpha = nn.Parameter(torch.zeros(size))
        self.epsilon = epsilon

    def forward(self, h_t: torch.Tensor, h_new: torch.Tensor) -> torch.Tensor:
        alpha = torch.sigmoid(self.alpha)
        h_t = F.normalize(h_t, dim=-1, eps=self.epsilon)
        h_new = F.normalize(h_new, dim=-1, eps=self.epsilon)
        
        dot_product = torch.sum(h_t * h_new, dim=-1, keepdim=True)
        dot_product = torch.clamp(dot_product, -1 + self.epsilon, 1 - self.epsilon)
        
        theta = torch.arccos(dot_product)
        sin_theta = torch.sin(theta) + self.epsilon
        
        h_t_coeff = torch.sin((1 - alpha) * theta) / sin_theta
        h_new_coeff = torch.sin(alpha * theta) / sin_theta
        
        return F.normalize(h_t_coeff * h_t + h_new_coeff * h_new, dim=-1, eps=self.epsilon)

class OptimizedLiquidCell(nn.Module):
    """Hardware-optimized liquid neural network cell"""
    def __init__(self, input_dim: int, hidden_dim: int, config: LN2Config):
        super().__init__()
        self.config = config
        
        self.input_proj = SphericalLinear(input_dim, hidden_dim)
        self.state_proj = SphericalLinear(hidden_dim, hidden_dim)
        self.slerp = OptimizedSLERP(hidden_dim, config.epsilon)
        self.norm = nn.LayerNorm(hidden_dim)
        
    def forward(self, x: torch.Tensor, h: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        batch_size = x.size(0)
        
        if h is None:
            h = F.normalize(
                torch.zeros(batch_size, self.state_proj.weight.size(0), 
                          device=x.device),
                dim=-1,
                eps=self.config.epsilon
            )
        
        input_proj = self.input_proj(x)
        state_proj = self.state_proj(h)
        
        h_new = F.normalize(input_proj + state_proj, dim=-1, eps=self.config.epsilon)
        
        if self.config.use_slerp:
            h_next = self.slerp(h, h_new)
        else:
            h_next = h_new
            
        return self.norm(h_next), h_next

class OptimizedLN2Model(nn.Module):
    """Hardware-optimized Liquid Normalized Neural Network model"""
    def __init__(self, config: LN2Config):
        super().__init__()
        self.config = config
        
        # Hardware optimization
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.scaler = GradScaler(enabled=config.use_mixed_precision)
        
        # Network layers
        self.input_norm = nn.LayerNorm(config.feature_dim)
        self.embedding = SphericalLinear(config.feature_dim, config.hidden_dim)
        
        self.liquid_cells = nn.ModuleList([
            OptimizedLiquidCell(config.hidden_dim, config.hidden_dim, config)
            for _ in range(config.num_layers)
        ])
        
        self.dropout = nn.Dropout(config.dropout)
        self.output_layer = SphericalLinear(config.hidden_dim, config.output_dim)
        
        # Adaptive learning rate
        self.eta = nn.Parameter(torch.ones(1))
        
        self.to(self.device)
        self._optimize_memory_format()

    def _optimize_memory_format(self):
        """Optimize memory layout for better performance"""
        self.to(memory_format=torch.channels_last)
        for param in self.parameters():
            if param.dim() == 4:
                param.data = param.data.to(memory_format=torch.channels_last)

    def init_states(self, batch_size: int) -> List[torch.Tensor]:
        return [F.normalize(
            torch.randn(batch_size, self.config.hidden_dim, device=self.device),
            dim=-1,
            eps=self.config.epsilon
        ) for _ in range(self.config.num_layers)]

    @autocast(device_type='cuda')
    def forward(self, x: torch.Tensor, states: Optional[List[torch.Tensor]] = None) -> Tuple[torch.Tensor, List[torch.Tensor]]:
        batch_size, seq_len = x.shape[:2]
        
        if states is None:
            states = self.init_states(batch_size)
        
        outputs = []
        for t in range(seq_len):
            x_t = self.input_norm(x[:, t])
            h = self.embedding(x_t)
            
            new_states = []
            for i, cell in enumerate(self.liquid_cells):
                h, new_state = cell(h, states[i])
                h = self.dropout(h)
                new_states.append(new_state)
            
            states = new_states
            output = self.output_layer(h)
            outputs.append(output)
        
        outputs = torch.stack(outputs, dim=1)
        return outputs, states

    def configure_optimizers(self) -> torch.optim.Optimizer:
        """Configure optimizer with adaptive learning rate"""
        return torch.optim.AdamW(
            self.parameters(),
            lr=self.config.learning_rate * torch.sigmoid(self.eta),
            eps=self.config.epsilon,
            betas=(0.9, 0.999),
            weight_decay=0.01
        )

class LN2Loss(nn.Module):
    """Loss function for LN² model with regularization"""
    def __init__(self):
        super().__init__()
        self.criterion = nn.MSELoss()
        
    def forward(self, pred: torch.Tensor, target: torch.Tensor, states: List[torch.Tensor]) -> torch.Tensor:
        pred_loss = self.criterion(pred, target)
        
        # State transition regularization
        state_reg = sum(
            torch.norm(s[1:] - s[:-1], dim=-1).mean() 
            for s in states
        ) / len(states)
        
        return pred_loss + 0.1 * state_reg

def optimize_hardware():
    """Optimize hardware settings for maximum performance"""
    if torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        torch.set_float32_matmul_precision('high')
        torch.cuda.empty_cache()
    
    torch.set_num_threads(24)  # Optimize for modern CPUs

def create_dataloaders(config: LN2Config, train_data: torch.Tensor, val_data: torch.Tensor):
    """Create optimized dataloaders for training"""
    train_dataset = torch.utils.data.TensorDataset(
        train_data[:, :-1], train_data[:, 1:]
    )
    val_dataset = torch.utils.data.TensorDataset(
        val_data[:, :-1], val_data[:, 1:]
    )
    
    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        pin_memory=True,
        num_workers=12,
        persistent_workers=True
    )
    
    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=config.batch_size * 2,
        shuffle=False,
        pin_memory=True,
        num_workers=12,
        persistent_workers=True
    )
    
    return train_loader, val_loader

// File: models\__init__.py


// File: utils\activations.py
# File: utils/activations.py

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Callable

class ModifiedReLU(nn.Module):
    """Modified ReLU that maintains normalization properties"""
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return F.relu(x) * (1 - torch.exp(-x))

def get_activation_fn(name: str) -> Callable:
    """Get activation function by name"""
    activations = {
        'tanh': torch.tanh,
        'modified_relu': ModifiedReLU(),
        'none': lambda x: x,
    }

    if name not in activations:
        raise ValueError(f"Unknown activation function: {name}")

    return activations[name]


// File: utils\hardware_config.py
import os
import torch
import torch.backends.cudnn as cudnn
from dataclasses import dataclass, field
from typing import Dict, Optional, Tuple
from pathlib import Path

@dataclass
class HardwareConfig:
    device: torch.device = field(default_factory=lambda: torch.device('cuda' if torch.cuda.is_available() else 'cpu'))
    num_workers: int = 12
    pin_memory: bool = True
    cuda_non_blocking: bool = True
    torch_threads: int = 24
    cudnn_benchmark: bool = True
    amp_dtype: torch.dtype = torch.float16
    memory_format: torch.memory_format = torch.channels_last
    vram_allocation: float = 0.85
    batch_power: int = 9
    gradient_accumulation_steps: int = 2
    prefetch_factor: int = 4
    persistent_workers: bool = True
    
    def __post_init__(self):
        if torch.cuda.is_available():
            torch.cuda.set_device(self.device)
            torch.cuda.empty_cache()
            torch.cuda.memory.set_per_process_memory_fraction(self.vram_allocation)
            
        torch.set_float32_matmul_precision('high')
        torch.set_num_threads(self.torch_threads)
        cudnn.benchmark = self.cudnn_benchmark
        cudnn.deterministic = False
        cudnn.enabled = True
        
        os.environ['OMP_NUM_THREADS'] = str(self.torch_threads)
        os.environ['MKL_NUM_THREADS'] = str(self.torch_threads)
        
        self.batch_size = 2 ** self.batch_power
        
    def optimize_tensor(self, tensor: torch.Tensor) -> torch.Tensor:
        return tensor.to(
            device=self.device,
            dtype=self.amp_dtype if tensor.is_floating_point() else tensor.dtype,
            memory_format=self.memory_format,
            non_blocking=self.cuda_non_blocking
        )
    
    def get_dataloader_args(self) -> Dict:
        return {
            'num_workers': self.num_workers,
            'pin_memory': self.pin_memory,
            'prefetch_factor': self.prefetch_factor,
            'persistent_workers': self.persistent_workers
        }
    
    @property
    def effective_batch_size(self) -> int:
        return self.batch_size * self.gradient_accumulation_steps
    
    def memory_stats(self) -> Dict[str, float]:
        if not torch.cuda.is_available():
            return {}
            
        return {
            'allocated': torch.cuda.memory_allocated() / 1024**3,
            'reserved': torch.cuda.memory_reserved() / 1024**3,
            'max_allocated': torch.cuda.max_memory_allocated() / 1024**3
        }

class OptimizedDataPipeline:
    def __init__(self, config: HardwareConfig):
        self.config = config
        
    def create_dataloaders(
        self,
        dataset_train,
        dataset_val,
        collate_fn=None
    ) -> Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader]:
        dataloader_args = self.config.get_dataloader_args()
        
        train_loader = torch.utils.data.DataLoader(
            dataset_train,
            batch_size=self.config.batch_size,
            shuffle=True,
            collate_fn=collate_fn,
            **dataloader_args
        )
        
        val_loader = torch.utils.data.DataLoader(
            dataset_val,
            batch_size=self.config.batch_size * 2,
            shuffle=False,
            collate_fn=collate_fn,
            **dataloader_args
        )
        
        return train_loader, val_loader

def create_optimized_config() -> HardwareConfig:
    return HardwareConfig(
        num_workers=12,
        torch_threads=24,
        batch_power=9,
        vram_allocation=0.85
    )

// File: utils\logging.py
# File: utils/logging.py

import logging
import sys
from pathlib import Path
from typing import Optional
from datetime import datetime

def get_logger(
    name: str,
    log_dir: Optional[Path] = None,
    level: int = logging.INFO
) -> logging.Logger:
    """
    Create or get a logger with the specified name and configuration

    Args:
        name: Logger name (typically __name__)
        log_dir: Directory to store log files
        level: Logging level

    Returns:
        Configured logger instance
    """
    logger = logging.getLogger(name)

    # Only configure if logger hasn't been configured before
    if not logger.handlers:
        logger.setLevel(level)

        # Create formatters
        console_formatter = logging.Formatter(
            '%(asctime)s | %(levelname)-8s | %(name)s:%(funcName)s:%(lineno)d - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )

        file_formatter = logging.Formatter(
            '%(asctime)s | %(levelname)-8s | %(name)s:%(funcName)s:%(lineno)d - %(message)s'
        )

        # Console handler
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(console_formatter)
        logger.addHandler(console_handler)

        # File handler if log_dir is provided
        if log_dir is not None:
            log_dir = Path(log_dir)
            log_dir.mkdir(parents=True, exist_ok=True)

            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            log_file = log_dir / f'nlnn_{timestamp}.log'

            file_handler = logging.FileHandler(log_file)
            file_handler.setFormatter(file_formatter)
            logger.addHandler(file_handler)

    return logger


// File: utils\monitoring.py
# File: utils/monitoring.py

import psutil
import GPUtil
import time
from datetime import datetime
from pathlib import Path
import json
from typing import Dict, List, Optional
import matplotlib.pyplot as plt
from dataclasses import dataclass
from utils.logging import get_logger

logger = get_logger('Monitoring')

@dataclass
class SystemStats:
    """System resource statistics"""
    timestamp: str
    cpu_percent: float
    memory_percent: float
    memory_used: float
    memory_total: float
    process_cpu_percent: float
    process_memory: float
    gpu_load: Optional[float] = None
    gpu_memory_used: Optional[float] = None
    gpu_memory_total: Optional[float] = None
    gpu_temperature: Optional[float] = None

class ResourceMonitor:
    """Monitor system resources during training"""
    def __init__(self):
        self.process = psutil.Process()
        self.start_time = time.time()
        self.stats_history: List[SystemStats] = []

    def get_gpu_stats(self) -> Dict[str, float]:
        """Get GPU statistics if available"""
        try:
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu = gpus[0]  # Get primary GPU
                return {
                    'gpu_load': gpu.load * 100,
                    'gpu_memory_used': gpu.memoryUsed,
                    'gpu_memory_total': gpu.memoryTotal,
                    'gpu_temperature': gpu.temperature
                }
        except Exception as e:
            logger.warning(f"Could not get GPU stats: {e}")
        return {}

    def get_system_stats(self) -> Dict[str, float]:
        """Get system statistics"""
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        return {
            'cpu_percent': cpu_percent,
            'memory_percent': memory.percent,
            'memory_used': memory.used / (1024 ** 3),  # Convert to GB
            'memory_total': memory.total / (1024 ** 3)  # Convert to GB
        }

    def get_process_stats(self) -> Dict[str, float]:
        """Get statistics for current process"""
        return {
            'process_cpu_percent': self.process.cpu_percent(),
            'process_memory': self.process.memory_info().rss / (1024 ** 3),  # Convert to GB
            'elapsed_time': time.time() - self.start_time
        }

    def log_stats(self, extra_info: Optional[Dict] = None):
        """Log current system statistics"""
        stats = {
            'timestamp': datetime.now().isoformat(),
            **self.get_system_stats(),
            **self.get_gpu_stats(),
            **self.get_process_stats()
        }

        # Create SystemStats object and store
        system_stats = SystemStats(**stats)
        self.stats_history.append(system_stats)

        # Log information
        logger.info("\nResource Usage:")
        logger.info(f"CPU Usage: {stats['cpu_percent']:.1f}%")
        logger.info(f"Memory Usage: {stats['memory_used']:.1f}GB/{stats['memory_total']:.1f}GB "
                    f"({stats['memory_percent']:.1f}%)")

        if 'gpu_load' in stats and stats['gpu_load'] is not None:
            logger.info(f"GPU Usage: {stats['gpu_load']:.1f}%")
            logger.info(f"GPU Memory: {stats['gpu_memory_used']:.1f}MB/{stats['gpu_memory_total']:.1f}MB")
            logger.info(f"GPU Temperature: {stats['gpu_temperature']:.1f}°C")

        logger.info(f"Process Memory: {stats['process_memory']:.2f}GB")
        logger.info(f"Elapsed Time: {stats['elapsed_time']:.1f}s")

        if extra_info:
            logger.info("\nTraining Stats:")
            for key, value in extra_info.items():
                logger.info(f"{key}: {value}")

    def save_stats(self, path: Path):
        """Save resource statistics to file"""
        stats_dict = [vars(stats) for stats in self.stats_history]
        with open(path, 'w') as f:
            json.dump(stats_dict, f, indent=2)

    def plot_stats(self, path: Path):
        """Create visualization of resource usage"""
        if not self.stats_history:
            logger.warning("No stats to plot")
            return

        plt.figure(figsize=(15, 10))

        # CPU and Memory Usage
        plt.subplot(2, 2, 1)
        plt.plot([s.cpu_percent for s in self.stats_history], label='CPU Usage %')
        plt.plot([s.memory_percent for s in self.stats_history], label='Memory Usage %')
        plt.title('CPU and Memory Usage')
        plt.xlabel('Time')
        plt.ylabel('Percentage')
        plt.legend()

        # GPU Usage if available
        if any(s.gpu_load is not None for s in self.stats_history):
            plt.subplot(2, 2, 2)
            plt.plot([s.gpu_load for s in self.stats_history], label='GPU Usage %')
            plt.plot([s.gpu_temperature for s in self.stats_history if s.gpu_temperature is not None], label='GPU Temp °C')
            plt.title('GPU Usage and Temperature')
            plt.xlabel('Time')
            plt.ylabel('Value')
            plt.legend()

        # Process Memory
        plt.subplot(2, 2, 3)
        plt.plot([s.process_memory for s in self.stats_history], label='Process Memory (GB)')
        plt.title('Process Memory Usage')
        plt.xlabel('Time')
        plt.ylabel('GB')
        plt.legend()

        # Elapsed Time
        plt.subplot(2, 2, 4)
        plt.plot([s.elapsed_time for s in self.stats_history], label='Elapsed Time (s)')
        plt.title('Elapsed Time')
        plt.xlabel('Time')
        plt.ylabel('Seconds')
        plt.legend()

        plt.tight_layout()
        plt.savefig(path)
        plt.close()


// File: utils\__init__.py


