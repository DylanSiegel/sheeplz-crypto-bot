// File: config\base_config.yaml
# File: config/base_config.yaml

defaults:
  - _self_
  - exchange: exchange_config
  - features: features_config
  - model: model_config
  - trading: trading_config
  - logging: logging_config
  - ray: ray_config

data:
  start_date: "2023-01-01"
  end_date: "2023-12-31"

agents:
  num_agents: 5

training:
  num_episodes: 1000

evaluation:
  num_episodes: 100

paths:
  model_save_path: "./models/saved/"


// File: config\exchange_config.yaml
# config/exchange_config.yaml
exchange:
  name: mexc
  api_key: ${env:MEXC_API_KEY}
  api_secret: ${env:MEXC_API_SECRET}
  trading_pairs:
    - BTC/USDT
    - ETH/USDT
  timeframe: 1m # Default timeframe
  rate_limit: 1200


// File: config\features_config.yaml
# config/features.yaml

features:
  indicators:
    SMA:
      enabled: true
      timeperiod: 20
    EMA:
      enabled: true
      timeperiod: 20
    RSI:
      enabled: true
      timeperiod: 14
    MACD:
      enabled: true
      fastperiod: 12
      slowperiod: 26
      signalperiod: 9
    ATR:
      enabled: true
      timeperiod: 14
  custom_features:
    pct_change:
      enabled: true
      window: 1
    volatility:
      enabled: true
      window: 20
  feature_selection:
    enabled: true
    method: "SelectFromModel"  # Options: SelectFromModel, RFE, etc.
    threshold: 0.01
    max_features: 10


// File: config\logging_config.yaml


// File: config\model_config.yaml
# config/model_config.yaml

model:
  type: "LSTM"  # Options: LSTM, Transformer, etc.
  input_size: 10  # Number of input features
  hidden_size: 64
  num_layers: 2
  output_size: 3  # Actions: Hold, Buy, Sell
  learning_rate: 0.001
  optimizer: "Adam"  # Options: Adam, SGD, etc.
  loss_function: "MSE"  # Options: MSE, CrossEntropy, etc.
  batch_size: 32
  epochs: 50
  dropout: 0.2
  device: "cuda"  # Options: cuda, cpu


// File: config\trading_config.yaml
# config/trading_config.yaml

trading:
  initial_balance: 10000.0  # Starting balance in USD
  max_trade_size: 0.1  # Maximum 10% of balance per trade
  risk_per_trade: 0.02  # 2% risk per trade
  transaction_fee: 0.001  # 0.1% fee per trade
  slippage: 0.0005  # 0.05% slippage
  leverage: 1  # No leverage by default
  stop_loss: 0.05  # 5% stop loss
  take_profit: 0.10  # 10% take profit
  trade_history_limit: 1000  # Number of past trades to keep


// File: models\trainer.py
# models/trainer.py

import pytorch_lightning as pl
import torch
from torch.utils.data import DataLoader
import pandas as pd
from src.data.data_loader import TradingDataset  # Corrected import
from src.models.lstm_model import TradingModel  # Assuming TradingModel is in lstm_model.py
from src.utils.utils import get_logger           # Corrected import
from src.features.feature_selector import FeatureSelector # Corrected import

logger = get_logger()

logger = get_logger()

class TradingLitModel(pl.LightningModule):
    """
    PyTorch Lightning module for trading.
    """

    def __init__(self, model: TradingModel, learning_rate: float, loss_fn, optimizer_cls):
        super(TradingLitModel, self).__init__()
        self.model = model
        self.learning_rate = learning_rate
        self.loss_fn = loss_fn
        self.optimizer_cls = optimizer_cls

    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_idx):
        X, y = batch
        preds = self.forward(X)
        loss = self.loss_fn(preds, y)
        self.log('train_loss', loss)
        return loss

    def configure_optimizers(self):
        optimizer = self.optimizer_cls(self.model.parameters(), lr=self.learning_rate)
        return optimizer

def train_model(config, train_df: pd.DataFrame, target_df: pd.Series):
    """
    Trains the trading model using PyTorch Lightning.

    Args:
        config (dict): Configuration dictionary.
        train_df (pd.DataFrame): Training feature data.
        target_df (pd.Series): Training target data.
    """
    # Feature Selection
    feature_selector = FeatureSelector(threshold=config['feature_selection']['threshold'],
                                       max_features=config['feature_selection']['max_features'])
    X_selected = feature_selector.fit_transform(train_df, target_df)
    logger.info(f"Selected features: {X_selected.columns.tolist()}")

    # Dataset and DataLoader
    dataset = TradingDataset(X_selected, target_df)
    dataloader = DataLoader(dataset, batch_size=config['model']['batch_size'], shuffle=True)

    # Model
    model = TradingModel(input_size=X_selected.shape[1],
                         hidden_size=config['model']['hidden_size'],
                         output_size=config['model']['output_size'])
    
    # Lightning Module
    lit_model = TradingLitModel(model=model,
                                learning_rate=config['model']['learning_rate'],
                                loss_fn=torch.nn.MSELoss(),
                                optimizer_cls=torch.optim.Adam)

    # Trainer
    trainer = pl.Trainer(max_epochs=config['model']['epochs'],
                         gpus=1 if torch.cuda.is_available() else 0,
                         logger=True)

    # Train
    trainer.fit(lit_model, dataloader)

    # Save the trained model
    torch.save(model.state_dict(), config['model']['model_save_path'] + "trading_model.pth")
    logger.info("Model training completed and saved.")

# Example usage
# train_model(config, train_features, train_targets)


// File: models\__init__.py


// File: scripts\backtest.py
# scripts/backtest.py

import argparse
import pandas as pd
from src.data.data_acquisition import BinanceDataProvider
from src.features.feature_engineer import FeatureEngineer
from src.features.feature_selector import FeatureSelector  # Corrected
from src.trading.trading_executor import TradingExecutor  # Corrected
from src.models.evaluator import Evaluator
from src.rewards.rewards import ProfitReward, SharpeRatioReward
from src.utils.utils import setup_logging, get_logger

def parse_args():
    parser = argparse.ArgumentParser(description="Backtest Trading Strategies")
    parser.add_argument('--symbol', type=str, default='BTC/USDT', help='Trading pair symbol')
    parser.add_argument('--timeframe', type=str, default='1h', help='Candlestick timeframe')
    parser.add_argument('--start_date', type=str, required=True, help='Start date (YYYY-MM-DD)')
    parser.add_argument('--end_date', type=str, required=True, help='End date (YYYY-MM-DD)')
    parser.add_argument('--reward_type', type=str, choices=['profit', 'sharpe'], default='profit', help='Type of reward function')
    return parser.parse_args()

def main():
    # Setup logging
    setup_logging()
    logger = get_logger()

    args = parse_args()

    # Initialize Data Provider
    data_provider = BinanceDataProvider(api_key='YOUR_API_KEY', api_secret='YOUR_API_SECRET')
    logger.info(f"Fetching historical data for {args.symbol} from {args.start_date} to {args.end_date}")
    df = data_provider.get_data(symbol=args.symbol, timeframe=args.timeframe, start_date=args.start_date, end_date=args.end_date)

    # Feature Engineering
    feature_engineer = FeatureEngineer()
    df = feature_engineer.add_technical_indicators(df)
    df = feature_engineer.add_custom_features(df)
    df = df.dropna()

    # Feature Selection
    feature_selector = FeatureSelector(threshold=0.01, max_features=10)
    target = (df['close'].shift(-1) > df['close']).astype(int).fillna(0).astype(int)
    X_selected = feature_selector.fit_transform(df[['SMA_20', 'EMA', 'RSI', 'MACD', 'ATR', 'pct_change', 'volatility']], target)

    # Initialize Reward Function
    if args.reward_type == 'profit':
        reward_function = ProfitReward()
    else:
        reward_function = SharpeRatioReward()

    # Initialize Trading Executor
    trading_executor = TradingExecutor(initial_balance=10000.0, transaction_fee=0.001)
    trade_history = trading_executor.execute_backtest(df, X_selected, target, reward_function)

    # Evaluate Performance
    evaluator = Evaluator(trade_history)
    evaluator.summary()
    evaluator.plot_equity_curve()
    evaluator.plot_drawdown()

if __name__ == "__main__":
    main()


// File: scripts\download_data.py
# scripts/download_data.py

import argparse
import pandas as pd
from src.data.data_acquisition import BinanceDataProvider  
from src.features.feature_engineer import FeatureEngineer  # Corrected import
from src.utils.utils import setup_logging, get_logger  # Corrected import

def parse_args():
    parser = argparse.ArgumentParser(description="Download Historical Data")
    parser.add_argument('--symbol', type=str, default='BTC/USDT', help='Trading pair symbol')
    parser.add_argument('--timeframe', type=str, default='1h', help='Candlestick timeframe')
    parser.add_argument('--start_date', type=str, required=True, help='Start date (YYYY-MM-DD)')
    parser.add_argument('--end_date', type=str, required=True, help='End date (YYYY-MM-DD)')
    parser.add_argument('--output', type=str, default='data/raw/', help='Output directory for raw data')
    return parser.parse_args()

def main():
    # Setup logging
    setup_logging()
    logger = get_logger()

    args = parse_args()

    # Initialize Data Provider
    data_provider = BinanceDataProvider(api_key='YOUR_API_KEY', api_secret='YOUR_API_SECRET')

    # Fetch data
    logger.info(f"Downloading data for {args.symbol} from {args.start_date} to {args.end_date}")
    df = data_provider.get_data(symbol=args.symbol, timeframe=args.timeframe, start_date=args.start_date, end_date=args.end_date)

    # Save raw data
    output_file = f"{args.output}{args.symbol.replace('/', '_')}_{args.timeframe}_{args.start_date}_{args.end_date}.csv"
    df.to_csv(output_file, index=False)
    logger.info(f"Raw data saved to {output_file}")

    # Feature Engineering
    feature_engineer = FeatureEngineer()
    df = feature_engineer.add_technical_indicators(df)
    df = feature_engineer.add_custom_features(df)
    df = df.dropna()

    # Save processed data
    processed_file = f"data/processed/{args.symbol.replace('/', '_')}_{args.timeframe}_{args.start_date}_{args.end_date}_processed.csv"
    df.to_csv(processed_file, index=False)
    logger.info(f"Processed data saved to {processed_file}")

if __name__ == "__main__":
    main()


// File: scripts\live_trading.py
import hydra
from omegaconf import DictConfig
from agents.agent_manager import AgentManager
from environments.crypto_trading_env import CryptoTradingEnv
from src.data.data_acquisition import BinanceDataProvider
from src.feature_engineering import FeatureEngineer
from src.feature_selection import FeatureSelector
from utils.utils import setup_logging, get_logger
import pandas as pd

@hydra.main(config_path="../config", config_name="base_config", version_base=None)
def main(cfg: DictConfig):
    setup_logging(log_level=cfg.logging.log_level, log_file=f"logs/trading_bot_{cfg.base.mode}.log")
    logger = get_logger()
    logger.info(f"Running in {cfg.base.mode} mode")

    # Initialize Data Provider and fetch data
    data_provider = BinanceDataProvider(api_key=cfg.exchange.api_key, api_secret=cfg.exchange.api_secret)
    data = {}
    for symbol in cfg.exchange.trading_pairs:
        df = data_provider.get_data(symbol=symbol, timeframe=cfg.exchange.timeframe, start_date=cfg.data.start_date, end_date=cfg.data.end_date)
        data[symbol] = df

    # Feature Engineering and Selection
    feature_engineer = FeatureEngineer()
    feature_selector = FeatureSelector(threshold=cfg.features.feature_selection.threshold, max_features=cfg.features.feature_selection.max_features)
    
    processed_data = {}
    for symbol, df in data.items():
        df = feature_engineer.process_features(df)
        X = df[cfg.features.feature_columns]
        y = (df['close'].shift(-1) > df['close']).astype(int).fillna(0)
        X_selected = feature_selector.fit_transform(X, y)
        processed_data[symbol] = (X_selected, y)

    # Initialize environments for each timeframe
    environments = {
        timeframe: CryptoTradingEnv(
            data=data[cfg.exchange.trading_pairs[0]],  # Using the first trading pair for simplicity
            processed_features=processed_data[cfg.exchange.trading_pairs[0]][0],
            reward_function=cfg.trading.reward_function,
            **cfg.environment
        ) for timeframe in cfg.trading.timeframes
    }

    # Initialize AgentManager
    agent_manager = AgentManager(agent_configs=cfg.agents, environments=environments)

    if cfg.base.mode == 'train':
        results = agent_manager.train_agents(num_episodes=cfg.training.num_episodes)
        logger.info(f"Training completed. Results: {results}")
        agent_manager.save_agents(cfg.paths.model_save_path)
    elif cfg.base.mode == 'test':
        agent_manager.load_agents(cfg.paths.model_save_path)
        evaluation_results = agent_manager.evaluate_agents(num_episodes=cfg.evaluation.num_episodes)
        logger.info(f"Evaluation completed. Results: {evaluation_results}")

if __name__ == "__main__":
    main()

// File: scripts\preprocess_data.py
# scripts/preprocess_data.py

import argparse
import pandas as pd
from src.features.feature_engineer import FeatureEngineer  # Corrected import
from src.features.feature_selector import FeatureSelector  # Corrected import
from src.utils.utils import setup_logging, get_logger  # Corrected import

def parse_args():
    parser = argparse.ArgumentParser(description="Preprocess Raw Data")
    parser.add_argument('--input', type=str, required=True, help='Input raw data CSV file')
    parser.add_argument('--output', type=str, default='data/processed/', help='Output directory for processed data')
    parser.add_argument('--reward_type', type=str, choices=['profit', 'sharpe'], default='profit', help='Type of reward function for feature selection')
    return parser.parse_args()

def main():
    # Setup logging
    setup_logging()
    logger = get_logger()

    args = parse_args()

    # Load raw data
    logger.info(f"Loading raw data from {args.input}")
    df = pd.read_csv(args.input)
    df['timestamp'] = pd.to_datetime(df['timestamp'])

    # Feature Engineering
    feature_engineer = FeatureEngineer()
    df = feature_engineer.add_technical_indicators(df)
    df = feature_engineer.add_custom_features(df)
    df = df.dropna()
    logger.info("Feature engineering completed.")

    # Feature Selection
    feature_selector = FeatureSelector(threshold=0.01, max_features=10)
    
    # Define target variable based on reward type
    if args.reward_type == 'profit':
        target = (df['close'].shift(-1) > df['close']).astype(int).fillna(0).astype(int)
    else:
        # For Sharpe ratio, define a continuous target based on returns
        target = df['close'].pct_change().fillna(0)

    X = df[['SMA_20', 'EMA', 'RSI', 'MACD', 'ATR', 'pct_change', 'volatility']]
    X_selected = feature_selector.fit_transform(X, target)
    logger.info(f"Selected features: {X_selected.columns.tolist()}")

    # Save processed data
    processed_file = f"{args.output}{args.input.split('/')[-1].replace('.csv', '_processed.csv')}"
    processed_df = X_selected.copy()
    processed_df['target'] = target
    processed_df.to_csv(processed_file, index=False)
    logger.info(f"Processed data saved to {processed_file}")

if __name__ == "__main__":
    main()


// File: scripts\run.py
import hydra
from omegaconf import DictConfig
from src.agents.agent_manager import AgentManager 
from src.environments.crypto_trading_env import CryptoTradingEnv 
from src.data.data_acquisition import BinanceDataProvider
from src.features.feature_engineer import FeatureEngineer
from src.features.feature_selector import FeatureSelector 
from src.utils.utils import setup_logging, get_logger
import pandas as pd

@hydra.main(config_path="../config", config_name="base_config", version_base=None)
def main(cfg: DictConfig):
    setup_logging(log_level=cfg.logging.log_level, log_file=f"logs/trading_bot_{cfg.base.mode}.log")
    logger = get_logger()
    logger.info(f"Running in {cfg.base.mode} mode")

    # Initialize Data Provider and fetch data
    data_provider = BinanceDataProvider(api_key=cfg.exchange.api_key, api_secret=cfg.exchange.api_secret)
    data = {}
    for symbol in cfg.exchange.trading_pairs:
        df = data_provider.get_data(symbol=symbol, timeframe=cfg.exchange.timeframe, start_date=cfg.data.start_date, end_date=cfg.data.end_date)
        data[symbol] = df

    # Feature Engineering and Selection
    feature_engineer = FeatureEngineer()
    feature_selector = FeatureSelector(threshold=cfg.features.feature_selection.threshold, max_features=cfg.features.feature_selection.max_features)
    
    processed_data = {}
    for symbol, df in data.items():
        df = feature_engineer.process_features(df)
        X = df[cfg.features.feature_columns]
        y = (df['close'].shift(-1) > df['close']).astype(int).fillna(0)
        X_selected = feature_selector.fit_transform(X, y)
        processed_data[symbol] = (X_selected, y)

    # Initialize environments for each timeframe
    environments = {
        timeframe: CryptoTradingEnv(
            data=data[cfg.exchange.trading_pairs[0]],  # Using the first trading pair for simplicity
            processed_features=processed_data[cfg.exchange.trading_pairs[0]][0],
            reward_function=cfg.trading.reward_function,
            **cfg.environment
        ) for timeframe in cfg.trading.timeframes
    }

    # Initialize AgentManager
    agent_manager = AgentManager(agent_configs=cfg.agents, environments=environments)

    if cfg.base.mode == 'train':
        results = agent_manager.train_agents(num_episodes=cfg.training.num_episodes)
        logger.info(f"Training completed. Results: {results}")
        agent_manager.save_agents(cfg.paths.model_save_path)
    elif cfg.base.mode == 'test':
        agent_manager.load_agents(cfg.paths.model_save_path)
        evaluation_results = agent_manager.evaluate_agents(num_episodes=cfg.evaluation.num_episodes)
        logger.info(f"Evaluation completed. Results: {evaluation_results}")

if __name__ == "__main__":
    main()

// File: scripts\run_visualization.py


// File: scripts\tune_hyperparameters.py
# scripts/tune_hyperparameters.py

import argparse
import optuna
from src.models.trainer import train_model  # Corrected import
from src.data.data_acquisition import BinanceDataProvider
from src.features.feature_engineer import FeatureEngineer  # Corrected import
from src.features.feature_selector import FeatureSelector
from src.trading.trading_executor import TradingExecutor 
from src.models.evaluator import Evaluator # Corrected import
from src.rewards.rewards import ProfitReward, SharpeRatioReward
from src.utils.utils import setup_logging, get_logger # Corrected import
import pandas as pd
from src.models.lstm_model import TradingModel # Corrected import (Assuming it's in lstm_model.py)
from src.models.trainer import TradingLitModel 
import torch
from torch.utils.data import DataLoader
import pytorch_lightning as pl

def parse_args():
    parser = argparse.ArgumentParser(description="Hyperparameter Tuning with Optuna")
    parser.add_argument('--symbol', type=str, default='BTC/USDT', help='Trading pair symbol')
    parser.add_argument('--timeframe', type=str, default='1h', help='Candlestick timeframe')
    parser.add_argument('--start_date', type=str, required=True, help='Start date (YYYY-MM-DD)')
    parser.add_argument('--end_date', type=str, required=True, help='End date (YYYY-MM-DD)')
    return parser.parse_args()

def objective(trial, cfg):
    # Initialize Data Provider
    data_provider = BinanceDataProvider(api_key=cfg.exchange.api_key, api_secret=cfg.exchange.api_secret)
    df = data_provider.get_data(symbol=cfg.exchange.trading_pairs[0], timeframe=cfg.exchange.timeframe,
                                start_date=cfg.start_date, end_date=cfg.end_date)

    # Feature Engineering
    feature_engineer = FeatureEngineer()
    df = feature_engineer.add_technical_indicators(df)
    df = feature_engineer.add_custom_features(df)
    df = df.dropna()

    # Feature Selection
    feature_selector = FeatureSelector(threshold=0.01, max_features=10)
    target = (df['close'].shift(-1) > df['close']).astype(int).fillna(0).astype(int)
    X = df[['SMA_20', 'EMA', 'RSI', 'MACD', 'ATR', 'pct_change', 'volatility']]
    X_selected = feature_selector.fit_transform(X, target)

    # Define hyperparameters to tune
    hidden_size = trial.suggest_int('hidden_size', 32, 256)
    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)
    num_layers = trial.suggest_int('num_layers', 1, 4)

    # Initialize Model
    model = TradingModel(input_size=X_selected.shape[1],
                         hidden_size=hidden_size,
                         output_size=3)

    # Initialize Lightning Module
    lit_model = TradingLitModel(model=model,
                                learning_rate=learning_rate,
                                loss_fn=torch.nn.MSELoss(),
                                optimizer_cls=torch.optim.Adam)

    # Prepare Dataset and DataLoader
    from models.trainer import TradingDataset
    dataset = TradingDataset(X_selected, target)
    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

    # Initialize Trainer
    trainer = pl.Trainer(max_epochs=10, gpus=1 if torch.cuda.is_available() else 0, logger=False)

    # Train
    trainer.fit(lit_model, dataloader)

    # Evaluate on validation set (split data accordingly)
    # For simplicity, using the same data
    preds = model(torch.tensor(X_selected.values, dtype=torch.float32))
    preds = torch.argmax(preds, dim=1).numpy()
    accuracy = (preds == target.values).mean()

    return accuracy

def main():
    # Setup logging
    setup_logging()
    logger = get_logger()

    args = parse_args()

    # Configuration dictionary (could be loaded from a file or defined here)
    cfg = {
        'exchange': {
            'api_key': 'YOUR_API_KEY',
            'api_secret': 'YOUR_API_SECRET',
            'trading_pairs': [args.symbol],
            'timeframe': args.timeframe
        },
        'start_date': args.start_date,
        'end_date': args.end_date
    }

    study = optuna.create_study(direction='maximize')
    study.optimize(lambda trial: objective(trial, cfg), n_trials=50)

    logger.info("Best trial:")
    trial = study.best_trial

    logger.info(f"  Value: {trial.value}")
    logger.info("  Params: ")
    for key, value in trial.params.items():
        logger.info(f"    {key}: {value}")

    # Save study results
    study.trials_dataframe().to_csv("optuna_study_results.csv")
    logger.info("Study results saved to optuna_study_results.csv")

if __name__ == "__main__":
    main()


// File: scripts\__init__.py
# scripts/__init__.py

# This file can be left empty or include package-level imports if necessary.


// File: src\app.py
# File: app.py

import streamlit as st
from src.visualization.visualization import Visualization
from src.data.data_acquisition import BinanceDataProvider 
from src.features.feature_engineer import FeatureEngineer
from src.models.lstm_model import TradingModel  # Or your preferred model
import torch
import os 
from dotenv import load_dotenv
from omegaconf import OmegaConf # For loading your config

# Load environment variables 
load_dotenv(os.path.join(os.path.dirname(__file__), '../../config/secrets.env'))

# Load config
cfg = OmegaConf.load(os.path.join(os.path.dirname(__file__), '../../config/base_config.yaml')) 

# Instantiate classes
visualization = Visualization()
data_provider = BinanceDataProvider(api_key=cfg.exchange.api_key, api_secret=cfg.exchange.api_secret)
feature_engineer = FeatureEngineer()

# Load your trained model
model = TradingModel(input_size=len(feature_engineer.get_feature_names()), 
                    hidden_size=cfg.model.hidden_size, 
                    num_layers=cfg.model.num_layers, 
                    output_size=3) # Assuming 3 output actions
model.load_state_dict(torch.load(cfg.paths.model_save_path))
model.eval() # Set to evaluation mode

# Fetch Data (Replace with your actual data loading)
df = data_provider.get_data(cfg.exchange.symbol, cfg.exchange.timeframe, 
                                start_date=cfg.data.start_date, end_date=cfg.data.end_date)

# Placeholder for performance and order history data (Replace with actual data)
performance_data = {
    'portfolio_value': [], # Update in your trading logic
    'returns': [] # Update in your trading logic
}
order_history = [] # Update in your trading logic

# Streamlit App
st.title("Crypto Trading Bot Dashboard")

# Price Chart
st.header("Price Chart")
visualization.plot_price_chart(df)

# Performance Metrics
st.header("Performance Metrics")
visualization.plot_performance_metrics(performance_data)

# Order History
st.header("Order History")
visualization.display_order_history(order_history)

# ... (Add other Streamlit components and interactions) ...

if __name__ == '__main__':
    st.write("Starting Streamlit app...")
    # st.experimental_rerun() # Optional: Uncomment for auto-refresh

// File: src\main.py
import hydra
from omegaconf import DictConfig
import torch
import pandas as pd
import numpy as np
from typing import Dict
import time
from src.data.data_acquisition import BinanceDataProvider
from src.features.feature_engineer import FeatureEngineer
from src.models.lstm_model import TradingModel
from src.agents.agent_manager import AgentManager
from src.environments.crypto_trading_env import CryptoTradingEnv
from src.rewards.rewards import get_reward_function
from utils.utils import setup_logging, get_logger
from dotenv import load_dotenv  # For loading environment variables
import os
from src.visualization.visualization import Visualization # Import for visualizations

logger = get_logger(__name__)

# Load environment variables from .env file
load_dotenv(os.path.join(os.path.dirname(__file__), '../../config/secrets.env'))

def preprocess_data(data: pd.DataFrame, feature_engineer: FeatureEngineer) -> pd.DataFrame:
    processed_data = feature_engineer.process_features(data)
    return processed_data.fillna(0)  # Fill NaN values with 0 to avoid issues

def create_environment(data: pd.DataFrame, config: DictConfig) -> CryptoTradingEnv:
    reward_function = get_reward_function(config.trading.reward_function)
    return CryptoTradingEnv(data, initial_balance=config.trading.initial_balance, 
                            transaction_fee=config.trading.transaction_fee, 
                            reward_function=reward_function)

def train_model(model: TradingModel, env: CryptoTradingEnv, config: DictConfig, visualization: Visualization):
    optimizer = torch.optim.Adam(model.parameters(), lr=config.model.learning_rate)
    criterion = torch.nn.MSELoss()
    training_data = [] # Initialize for training visualization

    for episode in range(config.training.num_episodes):
        state = env.reset()
        total_reward = 0
        done = False
        actions_taken = [] # Initialize for tracking actions in the episode

        while not done:
            action = model.get_action(state)
            actions_taken.append(action)
            next_state, reward, done, _ = env.step(action)
            loss = model.update(optimizer, criterion, state, action, reward, next_state, done)
            state = next_state
            total_reward += reward

        # Collect data for training visualization
        episode_data = {
            'episode': episode,
            'loss': loss, 
            'reward': total_reward,
            'actions': actions_taken 
        }
        training_data.append(episode_data)

        if (episode + 1) % 10 == 0:
            logger.info(f"Episode {episode + 1}/{config.training.num_episodes}, Total Reward: {total_reward:.2f}, Loss: {loss:.4f}")
            visualization.plot_training_progress(training_data) # Visualize training progress

def evaluate_model(model: TradingModel, env: CryptoTradingEnv, config: DictConfig) -> float:
    total_reward = 0
    num_episodes = config.evaluation.num_episodes

    for _ in range(num_episodes):
        state = env.reset()
        done = False
        episode_reward = 0

        while not done:
            action = model.get_action(state)
            state, reward, done, _ = env.step(action)
            episode_reward += reward

        total_reward += episode_reward

    avg_reward = total_reward / num_episodes
    logger.info(f"Evaluation - Average Reward: {avg_reward:.2f}")
    return avg_reward

def live_trading(model: TradingModel, data_provider: BinanceDataProvider, feature_engineer: FeatureEngineer, config: DictConfig, visualization: Visualization):
    portfolio_values = []
    returns = []
    order_history = []

    while True:
        # Fetch latest data
        latest_data = data_provider.get_data(config.exchange.symbol, config.exchange.timeframe, 
                                             limit=config.live_trading.lookback_period)
        
        # Preprocess data
        processed_data = preprocess_data(latest_data, feature_engineer)
        
        # Get the latest state
        latest_state = processed_data.iloc[-1].values
        
        # Get model prediction
        action = model.get_action(latest_state)
        
        # Execute trade based on the action
        if action == 1:  # Buy
            order = data_provider.place_order(config.exchange.symbol, 'market', 'buy', config.live_trading.trade_amount)
            logger.info(f"Buy order placed: {order}")
            # ... (Update portfolio_values, returns, order_history)
        elif action == 2:  # Sell
            order = data_provider.place_order(config.exchange.symbol, 'market', 'sell', config.live_trading.trade_amount)
            logger.info(f"Sell order placed: {order}")
            # ... (Update portfolio_values, returns, order_history)
        else:
            logger.info("Hold position")
        
        # Visualizations (Update periodically)
        visualization.plot_price_chart(latest_data)
        performance_data = {
            'portfolio_value': portfolio_values,
            'returns': returns,
            # ...
        }
        visualization.plot_performance_metrics(performance_data)
        visualization.display_order_history(order_history)

        # Wait for next trading interval
        time.sleep(config.live_trading.interval)

@hydra.main(config_path="../../config", config_name="base_config", version_base=None)
def main(cfg: DictConfig):
    setup_logging(cfg.logging)
    visualization = Visualization() # Instantiate for all modes
    
    # Initialize data provider
    data_provider = BinanceDataProvider(api_key=cfg.exchange.api_key, api_secret=cfg.exchange.api_secret)
    
    # Fetch historical data
    historical_data = data_provider.get_data(cfg.exchange.symbol, cfg.exchange.timeframe, 
                                             start_date=cfg.data.start_date, end_date=cfg.data.end_date)
    
    # Initialize feature engineer
    feature_engineer = FeatureEngineer()
    
    # Preprocess data
    processed_data = preprocess_data(historical_data, feature_engineer)
    
    # Create environment
    env = create_environment(processed_data, cfg)
    
    # Initialize model
    model = TradingModel(input_size=len(feature_engineer.get_feature_names()), 
                         hidden_size=cfg.model.hidden_size, 
                         num_layers=cfg.model.num_layers, 
                         output_size=env.action_space.n)
    
    if cfg.mode == "train":
        # Train the model
        train_model(model, env, cfg, visualization)
        
        # Save the trained model
        torch.save(model.state_dict(), cfg.paths.model_save_path)
        logger.info(f"Model saved to {cfg.paths.model_save_path}")
        
        # Evaluate the model
        evaluate_model(model, env, cfg)
    
    elif cfg.mode == "evaluate":
        # Load the trained model
        model.load_state_dict(torch.load(cfg.paths.model_save_path))
        logger.info(f"Model loaded from {cfg.paths.model_save_path}")
        
        # Evaluate the model
        evaluate_model(model, env, cfg)
    
    elif cfg.mode == "live":
        # Load the trained model
        model.load_state_dict(torch.load(cfg.paths.model_save_path))
        logger.info(f"Model loaded from {cfg.paths.model_save_path}")
        
        # Start live trading
        live_trading(model, data_provider, feature_engineer, cfg, visualization)
    
    else:
        logger.error(f"Invalid mode: {cfg.mode}")

if __name__ == "__main__":
    main()

// File: src\__init__.py


// File: src\agents\agent_manager.py
from typing import List, Dict, Any
import ray
from .trading_agent import TradingAgent
from environments.crypto_trading_env import CryptoTradingEnv
from src.models.lstm_model import TradingModel # Corrected import
from src.rewards.rewards import RewardFunction, get_reward_function
from utils.utils import get_logger

logger = get_logger(__name__)

@ray.remote
class RemoteTradingAgent(TradingAgent):
    pass

class AgentManager:
    def __init__(self, agent_configs: List[Dict[str, Any]], environments: Dict[str, CryptoTradingEnv]):
        ray.init(ignore_reinit_error=True)
        self.agents = []
        for config in agent_configs:
            env = environments[config['timeframe']]
            model = TradingModel(
                input_size=env.observation_space.shape[0],
                hidden_size=config['hidden_size'],
                num_layers=config['num_layers'],
                output_size=env.action_space.n
            )
            reward_function = get_reward_function(config['reward_function'])
            agent = RemoteTradingAgent.remote(env, model, reward_function, config['agent_id'])
            self.agents.append(agent)

    def train_agents(self, num_episodes: int):
        results = ray.get([agent.train.remote(num_episodes) for agent in self.agents])
        for result in results:
            logger.info(f"Agent {result['agent_id']} - Average Reward: {result['average_reward']:.2f}")
        return results

    def evaluate_agents(self, num_episodes: int):
        results = ray.get([agent.evaluate.remote(num_episodes) for agent in self.agents])
        for result in results:
            logger.info(f"Agent {result['agent_id']} - Evaluation Average Reward: {result['average_reward']:.2f}")
        return results

    def get_best_agent(self):
        evaluation_results = self.evaluate_agents(num_episodes=100)
        best_agent = max(evaluation_results, key=lambda x: x['average_reward'])
        return best_agent['agent_id']

    def save_agents(self, directory: str):
        ray.get([agent.save.remote(directory) for agent in self.agents])

    def load_agents(self, directory: str):
        ray.get([agent.load.remote(directory) for agent in self.agents])

    def shutdown(self):
        ray.shutdown()

// File: src\agents\base_agent.py


// File: src\agents\dqn_agent.py


// File: src\agents\ppo_agent.py


// File: src\agents\trading_agent.py
import torch
from environments.crypto_trading_env import CryptoTradingEnv
from models.trading_model import TradingModel
from src.rewards.rewards import RewardFunction

class TradingAgent:
    def __init__(self, env: CryptoTradingEnv, model: TradingModel, reward_function: RewardFunction, agent_id: str):
        self.env = env
        self.model = model
        self.reward_function = reward_function
        self.agent_id = agent_id
        self.performance_history = []

    def train(self, num_episodes: int):
        total_reward = 0
        for episode in range(num_episodes):
            state = self.env.reset()
            done = False
            episode_reward = 0

            while not done:
                action = self.model.get_action(state)
                next_state, reward, done, _ = self.env.step(action)
                adjusted_reward = self.reward_function.calculate_reward(state, action, reward, next_state, done)
                self.model.update(state, action, adjusted_reward, next_state, done)
                episode_reward += adjusted_reward
                state = next_state

            self.performance_history.append(episode_reward)
            total_reward += episode_reward

        return total_reward / num_episodes

    def act(self, state):
        return self.model.get_action(state)

    def evaluate(self, num_episodes: int):
        total_reward = 0
        for _ in range(num_episodes):
            state = self.env.reset()
            done = False
            while not done:
                action = self.act(state)
                state, reward, done, _ = self.env.step(action)
                total_reward += reward
        return total_reward / num_episodes

    def save(self, path: str):
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'performance_history': self.performance_history
        }, f"{path}/{self.agent_id}.pth")

    def load(self, path: str):
        checkpoint = torch.load(f"{path}/{self.agent_id}.pth")
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.performance_history = checkpoint['performance_history']

// File: src\agents\__init__.py
from .trading_agent import TradingAgent
from .agent_manager import AgentManager

__all__ = ['TradingAgent', 'AgentManager']

// File: src\data\data_acquisition.py
import ccxt
import pandas as pd
from typing import List
import time
from src.utils.utils import get_logger  # Use absolute import here too

logger = get_logger(__name__)

class BinanceDataProvider:
    def __init__(self, api_key: str, api_secret: str):
        self.exchange = ccxt.binance({
            'apiKey': api_key,
            'secret': api_secret,
            'enableRateLimit': True,
        })
        self.rate_limit = 1200  # Binance rate limit (requests per minute)
        self.last_request_time = 0

    def _rate_limit(self):
        current_time = time.time()
        elapsed = current_time - self.last_request_time
        if elapsed < 60 / self.rate_limit:
            time.sleep((60 / self.rate_limit) - elapsed)
        self.last_request_time = time.time()

    def get_data(self, symbol: str, timeframe: str, start_date: str, end_date: str) -> pd.DataFrame:
        try:
            self._rate_limit()
            data = self.exchange.fetch_ohlcv(symbol, timeframe, self.exchange.parse8601(start_date), limit=1000)
            df = pd.DataFrame(data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
            df.set_index('timestamp', inplace=True)
            return df
        except Exception as e:
            logger.error(f"Error fetching data: {str(e)}")
            return pd.DataFrame()

    def get_multiple_symbols(self, symbols: List[str], timeframe: str, start_date: str, end_date: str) -> dict:
        data = {}
        for symbol in symbols:
            data[symbol] = self.get_data(symbol, timeframe, start_date, end_date)
        return data

    def get_account_balance(self) -> dict:
        try:
            self._rate_limit()
            balance = self.exchange.fetch_balance()
            return {asset: balance[asset] for asset in balance if balance[asset]['total'] > 0}
        except Exception as e:
            logger.error(f"Error fetching account balance: {str(e)}")
            return {}

    def place_order(self, symbol: str, order_type: str, side: str, amount: float, price: float = None) -> dict:
        try:
            self._rate_limit()
            order = self.exchange.create_order(symbol, order_type, side, amount, price)
            return order
        except Exception as e:
            logger.error(f"Error placing order: {str(e)}")
            return {}

// File: src\data\data_loader.py
# src/data/data_loader.py

import torch
from torch.utils.data import Dataset, DataLoader
import pandas as pd

class TradingDataset(Dataset):
    """
    PyTorch Dataset for trading data.
    """

    def __init__(self, features: pd.DataFrame, targets: pd.Series):
        self.X = torch.tensor(features.values, dtype=torch.float32)
        self.y = torch.tensor(targets.values, dtype=torch.float32)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# Example usage (you can remove this if you don't need it in this file)
# dataset = TradingDataset(X_selected, target)
# dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

// File: src\data\data_provider.py
# File: src/data/data_provider.py

from abc import ABC, abstractmethod
import ccxt
import pandas as pd

class DataProvider(ABC):
    """Abstract base class for data providers."""

    @abstractmethod
    def get_data(self, symbol: str, timeframe: str, start_date: str, end_date: str) -> pd.DataFrame:
        """
        Retrieves historical data for a given symbol and timeframe.
        """
        pass

class BinanceDataProvider(DataProvider):
    """Data provider for Binance exchange."""

    def __init__(self, api_key: str, api_secret: str):
        self.exchange = ccxt.binance({
            'apiKey': api_key,
            'secret': api_secret,
        })

    def get_data(self, symbol: str, timeframe: str, start_date: str, end_date: str) -> pd.DataFrame:
        # Fetch data from Binance API
        data = self.exchange.fetch_ohlcv(symbol, timeframe, since=start_date, limit=1000)  # Adjust limit as needed

        # Convert to pandas DataFrame
        df = pd.DataFrame(data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')

        return df

# Example usage
# data_provider = BinanceDataProvider(api_key="YOUR_API_KEY", api_secret="YOUR_API_SECRET")
# df = data_provider.get_data(symbol="BTC/USDT", timeframe="1h", start_date="2023-01-01", end_date="2023-01-31")

// File: src\data\feature_store.py
# File: src/data/feature_store.py

import pandas as pd
import os
from typing import Optional

class FeatureStore:
    """
    Manages the storage and retrieval of features.
    """

    def __init__(self, feature_save_path: str):
        """
        Initializes the FeatureStore.

        Args:
            feature_save_path (str): Path to save the features.
        """
        self.feature_save_path = feature_save_path
        os.makedirs(self.feature_save_path, exist_ok=True)

    def save_features(self, df: pd.DataFrame, filename: str):
        """
        Saves the feature DataFrame to a CSV file.

        Args:
            df (pd.DataFrame): Feature DataFrame.
            filename (str): Name of the file to save.
        """
        filepath = os.path.join(self.feature_save_path, filename)
        df.to_csv(filepath, index=False)

    def load_features(self, filename: str) -> pd.DataFrame:
        """
        Loads features from a CSV file.

        Args:
            filename (str): Name of the file to load.

        Returns:
            pd.DataFrame: Loaded feature DataFrame.
        """
        filepath = os.path.join(self.feature_save_path, filename)
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"Feature file not found: {filepath}")
        return pd.read_csv(filepath)

    def list_features(self) -> list:
        """
        Lists all saved feature files.

        Returns:
            list: List of feature filenames.
        """
        return os.listdir(self.feature_save_path)

// File: src\data\__init__.py
# File: src/data/__init__.py

from .data_provider import DataProvider, BinanceDataProvider
from .feature_store import FeatureStore

__all__ = ['DataProvider', 'BinanceDataProvider', 'FeatureStore']

// File: src\environments\crypto_trading_env.py
import gym
from gym import spaces
import numpy as np
import pandas as pd
from typing import Dict, Tuple
from utils.utils import get_logger

logger = get_logger()


class CryptoTradingEnv(gym.Env):
    """
    A cryptocurrency trading environment for OpenAI gym
    """
    metadata = {'render.modes': ['human']}

    def __init__(self, df: pd.DataFrame, initial_balance=10000, transaction_fee_percent: float = 0.001):
        super(CryptoTradingEnv, self).__init__()
        self.df = df
        self.initial_balance = initial_balance
        self.transaction_fee_percent = transaction_fee_percent
        self.current_step = 0
        self.balance = initial_balance
        self.crypto_held = 0
        
        # Define action and observation space
        self.action_space = spaces.Discrete(3)  # Buy, Sell, Hold
        
        # Observation space: Normalized prices + current balance + crypto held
        self.observation_space = spaces.Box(
            low=0, high=1, shape=(len(self.df.columns) + 2,), dtype=np.float32
        )

    def reset(self) -> np.array:
        self.balance = self.initial_balance
        self.crypto_held = 0
        self.current_step = 0
        return self._next_observation()

    def step(self, action: int) -> Tuple[np.array, float, bool, Dict]:
        self.current_step += 1
        current_price = self._get_current_price()
        
        if action == 0:  # Buy
            max_buy = self.balance / current_price
            buy_amount = max_buy * 0.1  # Buy 10% of max possible
            impacted_price = self._apply_market_impact(action, buy_amount)
            cost = buy_amount * impacted_price * (1 + self.transaction_fee_percent)
            self.balance -= cost
            self.crypto_held += buy_amount
        elif action == 1:  # Sell
            sell_amount = self.crypto_held * 0.1  # Sell 10% of holdings
            impacted_price = self._apply_market_impact(action, sell_amount)
            revenue = sell_amount * impacted_price * (1 - self.transaction_fee_percent)
            self.balance += revenue
            self.crypto_held -= sell_amount
        
        # Hold case: do nothing
        
        done = self.current_step >= len(self.df) - 1
        obs = self._next_observation()
        reward = self._calculate_reward(current_price)
        info = self._get_info()
        
        return obs, reward, done, info

    def _next_observation(self) -> np.array:
        frame = np.array(self.df.iloc[self.current_step])
        normalized_frame = frame / frame.max()  # Simple normalization
        obs = np.append(normalized_frame, [self.balance / self.initial_balance, self.crypto_held])
        return obs

    def _get_current_price(self) -> float:
        return self.df.iloc[self.current_step]['close']

    def _calculate_reward(self, current_price: float) -> float:
        portfolio_value = self.balance + self.crypto_held * current_price
        return (portfolio_value / self.initial_balance) - 1

    def _get_info(self) -> Dict:
        return {
            'current_step': self.current_step,
            'balance': self.balance,
            'crypto_held': self.crypto_held,
            'portfolio_value': self.balance + self.crypto_held * self._get_current_price()
        }

    def _apply_market_impact(self, action: int, amount: float) -> float:
        current_price = self._get_current_price()
        impact = 0.1 * (amount / self.df['volume'].iloc[self.current_step])
        if action == 0:  # Buy
            return current_price * (1 + impact)
        elif action == 1:  # Sell
            return current_price * (1 - impact)
        return current_price

    def render(self, mode='human'):
        # Implement visualization logic here if needed
        pass

// File: src\environments\__init__.py


// File: src\features\feature_engineer.py
from typing import List
import pandas as pd
import numpy as np
from ta.trend import SMAIndicator, EMAIndicator, MACD, IchimokuIndicator
from ta.momentum import RSIIndicator, StochasticOscillator
from ta.volatility import BollingerBands, AverageTrueRange
from ta.volume import OnBalanceVolumeIndicator, AccumulationDistributionIndicator
from typing import List

class FeatureEngineer:
    def __init__(self):
        self.feature_names = []

    def add_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        # Trend indicators
        df['SMA_20'] = SMAIndicator(close=df['close'], window=20).sma_indicator()
        df['EMA_20'] = EMAIndicator(close=df['close'], window=20).ema_indicator()
        macd = MACD(close=df['close'])
        df['MACD'] = macd.macd()
        df['MACD_Signal'] = macd.macd_signal()
        ichimoku = IchimokuIndicator(high=df['high'], low=df['low'])
        df['Ichimoku_a'] = ichimoku.ichimoku_a()
        df['Ichimoku_b'] = ichimoku.ichimoku_b()

        # Momentum indicators
        df['RSI'] = RSIIndicator(close=df['close']).rsi()
        stoch = StochasticOscillator(high=df['high'], low=df['low'], close=df['close'])
        df['Stoch_k'] = stoch.stoch()
        df['Stoch_d'] = stoch.stoch_signal()

        # Volatility indicators
        bb = BollingerBands(close=df['close'])
        df['BB_high'] = bb.bollinger_hband()
        df['BB_low'] = bb.bollinger_lband()
        df['ATR'] = AverageTrueRange(high=df['high'], low=df['low'], close=df['close']).average_true_range()

        # Volume indicators
        df['OBV'] = OnBalanceVolumeIndicator(close=df['close'], volume=df['volume']).on_balance_volume()
        df['ADI'] = AccumulationDistributionIndicator(high=df['high'], low=df['low'], close=df['close'], volume=df['volume']).acc_dist_index()

        self.feature_names.extend([col for col in df.columns if col not in ['open', 'high', 'low', 'close', 'volume']])
        return df

    def add_custom_features(self, df: pd.DataFrame) -> pd.DataFrame:
        df['log_return'] = np.log(df['close'] / df['close'].shift(1))
        df['volatility'] = df['log_return'].rolling(window=20).std() * np.sqrt(252)
        df['z_score'] = (df['close'] - df['close'].rolling(window=20).mean()) / df['close'].rolling(window=20).std()
        df['momentum'] = df['close'] - df['close'].shift(10)
        df['price_volume'] = df['close'] * df['volume']

        self.feature_names.extend(['log_return', 'volatility', 'z_score', 'momentum', 'price_volume'])
        return df

    def create_lagged_features(self, df: pd.DataFrame, lags: List[int] = [1, 5, 10]) -> pd.DataFrame:
        for feature in self.feature_names:
            for lag in lags:
                df[f'{feature}_lag_{lag}'] = df[feature].shift(lag)
                self.feature_names.append(f'{feature}_lag_{lag}')
        return df

    def process_features(self, df: pd.DataFrame) -> pd.DataFrame:
        df = self.add_technical_indicators(df)
        df = self.add_custom_features(df)
        df = self.create_lagged_features(df)
        return df.dropna()

    def get_feature_names(self) -> List[str]:
        return self.feature_names

// File: src\features\feature_selector.py
# File: src/features/feature_selection.py

from sklearn.feature_selection import SelectFromModel, RFE
from sklearn.linear_model import Lasso
from sklearn.ensemble import RandomForestClassifier
from typing import Optional
import pandas as pd

class FeatureSelector:
    def __init__(self, method: str = "SelectFromModel", threshold: Optional[float] = None, max_features: Optional[int] = None):
        self.method = method
        self.threshold = threshold
        self.max_features = max_features
        self.selector = None

    def fit_transform(self, X: pd.DataFrame, y: pd.Series) -> pd.DataFrame:
        if self.method == "SelectFromModel":
            estimator = Lasso(alpha=0.01)
            self.selector = SelectFromModel(estimator, threshold=self.threshold)
        elif self.method == "RFE":
            estimator = RandomForestClassifier(n_estimators=100)
            self.selector = RFE(estimator, n_features_to_select=self.max_features)
        else:
            raise ValueError(f"Unsupported feature selection method: {self.method}")

        self.selector.fit(X, y)
        selected_features = X.columns[self.selector.get_support()]
        return X[selected_features]

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        if not self.selector:
            raise RuntimeError("FeatureSelector not fitted yet.")
        selected_features = X.columns[self.selector.get_support()]
        return X[selected_features]

    def get_selected_features(self) -> list:
        if not self.selector:
            raise RuntimeError("FeatureSelector not fitted yet.")
        return list(self.selector.get_support(indices=True))

// File: src\features\__init__.py
from .feature_engineer import FeatureEngineer
from .feature_selector import FeatureSelector

__all__ = ['FeatureEngineer', 'FeatureSelector']



// File: src\models\base_model.py
# File: models/model.py

import torch
import torch.nn as nn
import torch.nn.functional as F

class TradingModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):
        super(TradingModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, output_size)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        # Assuming x shape: (batch, seq_length, input_size)
        lstm_out, _ = self.lstm(x)
        # Taking the output of the last time step
        last_output = lstm_out[:, -1, :]
        out = self.dropout(last_output)
        out = self.fc(out)
        return out

    def get_action(self, state):
        with torch.no_grad():
            q_values = self(torch.FloatTensor(state).unsqueeze(0))
            return q_values.argmax().item()

    def update(self, state, action, reward, next_state, done):
        # Implement your update logic here (e.g., Q-learning updates)
        pass


// File: src\models\evaluator.py
# models/evaluator.py

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from typing import List
from utils.utils import get_logger

logger = get_logger()

class Evaluator:
    """
    Evaluates trading strategies based on various performance metrics.
    """

    def __init__(self, trade_history: pd.DataFrame):
        """
        Initializes the evaluator with trade history data.

        Args:
            trade_history (pd.DataFrame): DataFrame containing trade details.
        """
        self.trade_history = trade_history

    def calculate_sharpe_ratio(self, risk_free_rate: float = 0.0) -> float:
        """
        Calculates the Sharpe Ratio of the trading strategy.

        Args:
            risk_free_rate (float): The risk-free rate. Defaults to 0.0.

        Returns:
            float: Sharpe Ratio.
        """
        returns = self.trade_history['returns']
        excess_returns = returns - risk_free_rate
        sharpe_ratio = excess_returns.mean() / excess_returns.std()
        return sharpe_ratio

    def calculate_max_drawdown(self) -> float:
        """
        Calculates the Maximum Drawdown of the trading strategy.

        Returns:
            float: Maximum Drawdown.
        """
        cumulative_returns = (1 + self.trade_history['returns']).cumprod()
        peak = cumulative_returns.expanding(min_periods=1).max()
        drawdown = (cumulative_returns - peak) / peak
        max_drawdown = drawdown.min()
        return max_drawdown

    def calculate_total_return(self) -> float:
        """
        Calculates the Total Return of the trading strategy.

        Returns:
            float: Total Return.
        """
        total_return = (self.trade_history['portfolio_value'].iloc[-1] / self.trade_history['portfolio_value'].iloc[0]) - 1
        return total_return

    def plot_equity_curve(self):
        """
        Plots the equity curve of the trading strategy.
        """
        cumulative_returns = (1 + self.trade_history['returns']).cumprod()
        plt.figure(figsize=(12, 6))
        plt.plot(cumulative_returns, label='Equity Curve')
        plt.xlabel('Trade Number')
        plt.ylabel('Cumulative Returns')
        plt.title('Equity Curve')
        plt.legend()
        plt.grid(True)
        plt.show()

    def plot_drawdown(self):
        """
        Plots the drawdown over time.
        """
        cumulative_returns = (1 + self.trade_history['returns']).cumprod()
        peak = cumulative_returns.expanding(min_periods=1).max()
        drawdown = (cumulative_returns - peak) / peak
        plt.figure(figsize=(12, 6))
        plt.plot(drawdown, label='Drawdown', color='red')
        plt.xlabel('Trade Number')
        plt.ylabel('Drawdown')
        plt.title('Drawdown Over Time')
        plt.legend()
        plt.grid(True)
        plt.show()

    def summary(self):
        """
        Prints a summary of key performance metrics.
        """
        sharpe = self.calculate_sharpe_ratio()
        max_dd = self.calculate_max_drawdown()
        total_ret = self.calculate_total_return()

        print(f"Sharpe Ratio: {sharpe:.2f}")
        print(f"Maximum Drawdown: {max_dd:.2%}")
        print(f"Total Return: {total_ret:.2%}")

# Example usage
# evaluator = Evaluator(trade_history=trade_history_df)
# evaluator.summary()
# evaluator.plot_equity_curve()
# evaluator.plot_drawdown()


// File: src\models\lstm_model.py
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class AttentionLayer(nn.Module):
    def __init__(self, hidden_size):
        super(AttentionLayer, self).__init__()
        self.attention = nn.Linear(hidden_size * 2, 1)

    def forward(self, hidden_states):
        attention_weights = F.softmax(self.attention(hidden_states), dim=1)
        context_vector = torch.sum(attention_weights * hidden_states, dim=1)
        return context_vector, attention_weights

class TradingModel(nn.Module):
    def __init__(self, input_size: int, hidden_size: int, num_layers: int, output_size: int, dropout: float = 0.2):
        super(TradingModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
        self.attention = AttentionLayer(hidden_size)
        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)
        self.fc2 = nn.Linear(hidden_size // 2, output_size)
        self.dropout = nn.Dropout(dropout)
        self.batch_norm = nn.BatchNorm1d(hidden_size)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        lstm_out, _ = self.lstm(x)
        context_vector, attention_weights = self.attention(lstm_out)
        out = self.batch_norm(context_vector)
        out = F.relu(self.fc1(out))
        out = self.dropout(out)
        out = self.fc2(out)
        return out, attention_weights

    def get_action(self, state: np.ndarray) -> int:
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            q_values, _ = self(state_tensor)
            return q_values.argmax().item()

    def update(self, optimizer: torch.optim.Optimizer, criterion: nn.Module, 
               state: np.ndarray, action: int, reward: float, next_state: np.ndarray, done: bool):
        optimizer.zero_grad()
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)
        
        current_q_values, _ = self(state_tensor)
        next_q_values, _ = self(next_state_tensor)
        
        current_q_value = current_q_values[0][action]
        next_q_value = next_q_values.max()
        
        expected_q_value = reward + (0.99 * next_q_value * (1 - int(done)))
        
        loss = criterion(current_q_value, expected_q_value.detach())
        loss.backward()
        optimizer.step()
        
        return loss.item()

// File: src\models\timesnet_model.py
import torch
import torch.nn as nn
import torch.nn.functional as F

class TimesBlock(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(TimesBlock, self).__init__()
        self.conv1 = nn.Conv1d(input_size, hidden_size, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)
        self.ln = nn.LayerNorm(hidden_size)

    def forward(self, x):
        residual = x
        x = self.conv1(x.transpose(1, 2)).transpose(1, 2)
        x = F.relu(x)
        x = self.conv2(x.transpose(1, 2)).transpose(1, 2)
        x = self.ln(x + residual)
        return x

class TimesNet(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(TimesNet, self).__init__()
        self.blocks = nn.ModuleList([TimesBlock(input_size if i == 0 else hidden_size, hidden_size) for i in range(num_layers)])
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        for block in self.blocks:
            x = block(x)
        x = x.mean(dim=1)  # Global average pooling
        return self.fc(x)

class TimesNetTradingModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(TimesNetTradingModel, self).__init__()
        self.timesnet = TimesNet(input_size, hidden_size, num_layers, output_size)

    def forward(self, x):
        return self.timesnet(x)

    def get_action(self, state):
        with torch.no_grad():
            q_values = self(torch.FloatTensor(state).unsqueeze(0))
            return q_values.argmax().item()

    def update(self, state, action, reward, next_state, done):
        # Implement your update logic here
        pass

// File: src\models\trainer.py
# models/trainer.py

import pytorch_lightning as pl
import torch
from torch.utils.data import DataLoader
import pandas as pd
from src.data.data_loader import TradingDataset
from src.models.base_model import TradingModel
from src.utils.utils import get_logger
from src.features.feature_selector import FeatureSelector

logger = get_logger()

logger = get_logger()

class TradingLitModel(pl.LightningModule):
    """
    PyTorch Lightning module for trading.
    """

    def __init__(self, model: TradingModel, learning_rate: float, loss_fn, optimizer_cls):
        super(TradingLitModel, self).__init__()
        self.model = model
        self.learning_rate = learning_rate
        self.loss_fn = loss_fn
        self.optimizer_cls = optimizer_cls

    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_idx):
        X, y = batch
        preds = self.forward(X)
        loss = self.loss_fn(preds, y)
        self.log('train_loss', loss)
        return loss

    def configure_optimizers(self):
        optimizer = self.optimizer_cls(self.model.parameters(), lr=self.learning_rate)
        return optimizer

def train_model(config, train_df: pd.DataFrame, target_df: pd.Series):
    """
    Trains the trading model using PyTorch Lightning.

    Args:
        config (dict): Configuration dictionary.
        train_df (pd.DataFrame): Training feature data.
        target_df (pd.Series): Training target data.
    """
    # Feature Selection
    feature_selector = FeatureSelector(threshold=config['feature_selection']['threshold'],
                                       max_features=config['feature_selection']['max_features'])
    X_selected = feature_selector.fit_transform(train_df, target_df)
    logger.info(f"Selected features: {X_selected.columns.tolist()}")

    # Dataset and DataLoader
    dataset = TradingDataset(X_selected, target_df)
    dataloader = DataLoader(dataset, batch_size=config['model']['batch_size'], shuffle=True)

    # Model
    model = TradingModel(input_size=X_selected.shape[1],
                         hidden_size=config['model']['hidden_size'],
                         output_size=config['model']['output_size'])
    
    # Lightning Module
    lit_model = TradingLitModel(model=model,
                                learning_rate=config['model']['learning_rate'],
                                loss_fn=torch.nn.MSELoss(),
                                optimizer_cls=torch.optim.Adam)

    # Trainer
    trainer = pl.Trainer(max_epochs=config['model']['epochs'],
                         gpus=1 if torch.cuda.is_available() else 0,
                         logger=True)

    # Train
    trainer.fit(lit_model, dataloader)

    # Save the trained model
    torch.save(model.state_dict(), config['model']['model_save_path'] + "trading_model.pth")
    logger.info("Model training completed and saved.")

# Example usage
# train_model(config, train_features, train_targets)


// File: src\models\transformer_model.py


// File: src\models\__init__.py
from .base_model import BaseModel
from .lstm_model import LSTMModel
from .transformer_model import TransformerModel
from .timesnet_model import TimesNetModel

__all__ = ['BaseModel', 'LSTMModel', 'TransformerModel', 'TimesNetModel']

// File: src\rewards\base_reward.py


// File: src\rewards\profit_reward.py


// File: src\rewards\rewards.py
import numpy as np
from typing import List

class RewardFunction:
    def calculate_reward(self, action: int, current_price: float, next_price: float, portfolio_value: float) -> float:
        pass

class ProfitReward(RewardFunction):
    def calculate_reward(self, action: int, current_price: float, next_price: float, portfolio_value: float) -> float:
        if action == 1:  # Buy
            return (next_price - current_price) / current_price
        elif action == 2:  # Sell
            return (current_price - next_price) / current_price
        else:  # Hold
            return 0

class SharpeRatioReward(RewardFunction):
    def __init__(self, risk_free_rate: float = 0.0, window_size: int = 20):
        self.risk_free_rate = risk_free_rate
        self.window_size = window_size
        self.returns = []

    def calculate_reward(self, action: int, current_price: float, next_price: float, portfolio_value: float) -> float:
        return_ = (next_price - current_price) / current_price
        self.returns.append(return_)

        if len(self.returns) < self.window_size:
            return 0

        returns_array = np.array(self.returns[-self.window_size:])
        excess_returns = returns_array - self.risk_free_rate
        sharpe_ratio = np.sqrt(252) * np.mean(excess_returns) / np.std(excess_returns)

        return sharpe_ratio

class CombinedReward(RewardFunction):
    def __init__(self, profit_weight: float = 0.5, sharpe_weight: float = 0.5):
        self.profit_reward = ProfitReward()
        self.sharpe_reward = SharpeRatioReward()
        self.profit_weight = profit_weight
        self.sharpe_weight = sharpe_weight

    def calculate_reward(self, action: int, current_price: float, next_price: float, portfolio_value: float) -> float:
        profit = self.profit_reward.calculate_reward(action, current_price, next_price, portfolio_value)
        sharpe = self.sharpe_reward.calculate_reward(action, current_price, next_price, portfolio_value)
        return self.profit_weight * profit + self.sharpe_weight * sharpe

// File: src\rewards\reward_functions.py
# File: rewards/reward_functions.py

from abc import ABC, abstractmethod
import numpy as np

class RewardFunction(ABC):
    @abstractmethod
    def __call__(self, state: np.ndarray, action: float, reward: float, next_state: np.ndarray, done: bool) -> float:
        pass

class ProfitReward(RewardFunction):
    def __call__(self, state: np.ndarray, action: float, reward: float, next_state: np.ndarray, done: bool) -> float:
        return reward

class SharpeRatioReward(RewardFunction):
    def __init__(self, window_size: int = 20, risk_free_rate: float = 0.0):
        self.window_size = window_size
        self.risk_free_rate = risk_free_rate
        self.returns = []

    def __call__(self, state: np.ndarray, action: float, reward: float, next_state: np.ndarray, done: bool) -> float:
        self.returns.append(reward)
        if len(self.returns) < self.window_size:
            return 0.0
        
        returns_array = np.array(self.returns[-self.window_size:])
        excess_returns = returns_array - self.risk_free_rate
        sharpe_ratio = np.sqrt(252) * np.mean(excess_returns) / np.std(excess_returns)
        return sharpe_ratio



// File: src\rewards\sharpe_ratio_reward.py


// File: src\rewards\__init__.py
from .base_reward import BaseReward
from .profit_reward import ProfitReward
from .sharpe_ratio_reward import SharpeRatioReward

__all__ = ['BaseReward', 'ProfitReward', 'SharpeRatioReward']

// File: src\trading\trading_executor.py
# src/trading/trading_executor.py

import ccxt
from src.utils.utils import get_logger

logger = get_logger(__name__)

class TradingExecutor:
    def __init__(self, api_key: str, api_secret: str, exchange_name: str = 'binance'):
        self.exchange = getattr(ccxt, exchange_name)({
            'apiKey': api_key,
            'secret': api_secret,
            'enableRateLimit': True,
        })

    def execute_trade(self, symbol: str, order_type: str, side: str, amount: float, price: float = None):
        """
        Executes a trade on the exchange.

        Args:
            symbol (str): The trading pair (e.g., 'BTC/USDT').
            order_type (str): 'market' or 'limit'.
            side (str): 'buy' or 'sell'.
            amount (float): The amount of cryptocurrency to buy or sell.
            price (float, optional): The limit price for limit orders. Defaults to None.
        """
        try:
            if order_type == 'market':
                order = self.exchange.create_market_order(symbol, side, amount)
            elif order_type == 'limit':
                order = self.exchange.create_limit_order(symbol, side, amount, price)
            else:
                raise ValueError("Invalid order type. Must be 'market' or 'limit'.")
            
            logger.info(f"Trade executed: {order}")
            return order
        except Exception as e:
            logger.error(f"Error executing trade: {e}")
            return None

    # Add more methods for order management, risk management, etc. 

// File: src\trading\__init__.py
# src/trading/__init__.py

# This file can be left empty. 

// File: src\utils\logging_config.py
# File: utils/logging_config.py

import logging
import os
from typing import Dict

def setup_logging(config: Dict):
    log_level = getattr(logging, config.level.upper())
    log_format = config.format
    log_file = config.file

    logging.basicConfig(level=log_level, format=log_format)

    if log_file:
        file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(logging.Formatter(log_format))
        logging.getLogger().addHandler(file_handler)

def get_logger(name: str):
    return logging.getLogger(name)

// File: src\utils\utils.py
# File: src/utils.py

import logging
import os
import sys

def setup_logging(log_level: str = "INFO", log_file: Optional[str] = None):
    """
    Sets up logging configuration.

    Args:
        log_level (str): Logging level.
        log_file (Optional[str]): File to log messages. If None, logs to stdout.
    """
    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    handlers = [logging.StreamHandler(sys.stdout)]
    if log_file:
        os.makedirs(os.path.dirname(log_file), exist_ok=True)
        handlers.append(logging.FileHandler(log_file))

    logging.basicConfig(
        level=getattr(logging, log_level.upper(), logging.INFO),
        format=log_format,
        handlers=handlers
    )

def get_logger(name: Optional[str] = None) -> logging.Logger:
    """
    Returns a logger instance.

    Args:
        name (Optional[str]): Name of the logger. If None, returns the root logger.

    Returns:
        logging.Logger: Logger instance.
    """
    return logging.getLogger(name)


// File: src\utils\__init__.py


// File: src\visualization\visualization.py
# File: src/visualization/visualization.py

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from typing import List, Dict
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import streamlit as st
from src.utils.utils import get_logger

logger = get_logger(__name__)

class Visualization:
    """
    Provides various visualization methods for trading data, 
    performance, and training progress.
    """

    def __init__(self):
        pass

    def plot_price_chart(self, df: pd.DataFrame, title: str = "Price Chart"):
        """
        Creates an interactive candlestick chart with optional technical indicators.

        Args:
            df (pd.DataFrame): DataFrame containing price data ('open', 'high', 'low', 'close', 'volume').
            title (str, optional): Title of the chart. Defaults to "Price Chart".
        """
        fig = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.1, 
                           subplot_titles=(title, "Volume"))

        # Candlestick chart
        fig.add_trace(go.Candlestick(x=df.index,
                        open=df['open'],
                        high=df['high'],
                        low=df['low'],
                        close=df['close'],
                        name="Price"), row=1, col=1)

        # Add indicators (example: SMA and RSI)
        if 'SMA_20' in df.columns:
            fig.add_trace(go.Scatter(x=df.index, y=df['SMA_20'], mode='lines', name='SMA 20'), row=1, col=1)
        if 'RSI' in df.columns:
            fig.add_trace(go.Scatter(x=df.index, y=df['RSI'], mode='lines', name='RSI'), row=2, col=1)

        # Volume bar chart
        fig.add_trace(go.Bar(x=df.index, y=df['volume'], name='Volume'), row=2, col=1)

        fig.update_layout(xaxis_rangeslider_visible=False, height=800)
        st.plotly_chart(fig)

    def plot_performance_metrics(self, performance_data: Dict):
        """
        Visualizes key performance metrics such as equity curve, Sharpe ratio, 
        and maximum drawdown.

        Args:
            performance_data (Dict): Dictionary containing performance data.
                Example: {'portfolio_value': [], 'returns': [], ...}
        """
        df = pd.DataFrame(performance_data)

        # Equity Curve
        fig_equity = go.Figure()
        fig_equity.add_trace(go.Scatter(x=df.index, y=df['portfolio_value'], mode='lines', name='Equity Curve'))
        fig_equity.update_layout(title="Equity Curve", xaxis_title="Time", yaxis_title="Portfolio Value")
        st.plotly_chart(fig_equity)

        # Sharpe Ratio (Display as a single value)
        sharpe_ratio = self.calculate_sharpe_ratio(df['returns'])
        st.write(f"**Sharpe Ratio:** {sharpe_ratio:.2f}")

        # Maximum Drawdown (Display as a single value)
        max_drawdown = self.calculate_max_drawdown(df['portfolio_value'])
        st.write(f"**Maximum Drawdown:** {max_drawdown:.2%}")

    def display_order_history(self, order_history: List[Dict]):
        """
        Displays the order history in a tabular format.

        Args:
            order_history (List[Dict]): List of order dictionaries.
        """
        df = pd.DataFrame(order_history)
        st.table(df)

    def plot_training_progress(self, training_data: List[Dict]):
        """
        Visualizes the training progress, including reward/loss curves, 
        actions over time, etc.

        Args:
            training_data (List[Dict]): List of dictionaries containing training data 
                for each episode.
        """
        df = pd.DataFrame(training_data)

        # Reward/Loss Curve
        fig_reward = go.Figure()
        fig_reward.add_trace(go.Scatter(x=df['episode'], y=df['reward'], mode='lines', name='Reward'))
        if 'loss' in df.columns:
            fig_reward.add_trace(go.Scatter(x=df['episode'], y=df['loss'], mode='lines', name='Loss'))
        fig_reward.update_layout(title="Reward/Loss Curve", xaxis_title="Episode", yaxis_title="Reward/Loss")
        st.plotly_chart(fig_reward)

        # Actions Over Time (Example with bar chart)
        if 'actions' in df.columns:
            action_counts = df['actions'].explode().value_counts()
            fig_actions = go.Figure(data=[go.Bar(x=action_counts.index, y=action_counts.values)])
            fig_actions.update_layout(title="Action Distribution", xaxis_title="Action", yaxis_title="Count")
            st.plotly_chart(fig_actions)

    def calculate_sharpe_ratio(self, returns: pd.Series, risk_free_rate: float = 0.0) -> float:
        """
        Calculates the Sharpe Ratio.
        """
        excess_returns = returns - risk_free_rate
        return np.sqrt(252) * excess_returns.mean() / excess_returns.std()

    def calculate_max_drawdown(self, portfolio_value: pd.Series) -> float:
        """
        Calculates the Maximum Drawdown.
        """
        peak = portfolio_value.expanding(min_periods=1).max()
        drawdown = (portfolio_value - peak) / peak
        return drawdown.min()

// File: src\visualization\__init__.py


// File: tests\test_data_acquisition.py
import unittest
import pandas as pd
from src.data.data_acquisition import BinanceDataProvider 

class TestBinanceDataProvider(unittest.TestCase):

    def setUp(self):
        """Set up for test methods."""
        self.api_key = 'YOUR_API_KEY'  
        self.api_secret = 'YOUR_API_SECRET' 
        self.data_provider = BinanceDataProvider(self.api_key, self.api_secret)

    def test_get_data(self):
        """Test fetching data from Binance."""
        symbol = 'BTC/USDT'
        timeframe = '1h'
        start_date = '2024-01-01'
        end_date = '2024-01-10'  # Fetch a small amount of data for testing

        df = self.data_provider.get_data(symbol, timeframe, start_date, end_date)

        # Assertions to validate the data
        self.assertIsInstance(df, pd.DataFrame)
        self.assertFalse(df.empty)
        self.assertGreater(len(df), 0)
        self.assertIn('timestamp', df.columns)
        self.assertIn('open', df.columns)
        self.assertIn('high', df.columns)
        self.assertIn('low', df.columns)
        self.assertIn('close', df.columns)
        self.assertIn('volume', df.columns)

    # Add more test methods for other functionalities
    # in BinanceDataProvider (e.g., get_account_balance, place_order).

if __name__ == '__main__':
    unittest.main()

