**File Tree (Relevant Files Only)**
  .
    - lnn_module.py
    - preprocess.py
    - test.py
  data\raw
    - btc_15m_data_2018_to_2024-2024-10-10.csv
    - btc_1d_data_2018_to_2024-2024-10-10.csv
    - btc_1h_data_2018_to_2024-2024-10-10.csv
    - btc_4h_data_2018_to_2024-2024-10-10.csv
// File: lnn_module.py
# File: lnn_module.py

import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
import joblib
import os
from datetime import datetime
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
import multiprocessing
from multiprocessing import Pool, cpu_count  # Correctly import Pool and cpu_count
from torch.cuda.amp import GradScaler, autocast

# Define file paths for all timeframes
PROCESSED_DATA_DIR = r"data\final"
PROCESSED_DATA_PATHS = {
    "15m": os.path.join(PROCESSED_DATA_DIR, "processed_data_15m.csv.gz"),
    "1h": os.path.join(PROCESSED_DATA_DIR, "processed_data_1h.csv.gz"),
    "4h": os.path.join(PROCESSED_DATA_DIR, "processed_data_4h.csv.gz"),
    "1d": os.path.join(PROCESSED_DATA_DIR, "processed_data_1d.csv.gz")
}

# Parameters
TARGET_COLUMNS = {
    '15m': ['target_15m', 'target_1h', 'target_4h', 'target_1d'],
    '1h': ['target_1h', 'target_4h', 'target_1d'],
    '4h': ['target_4h', 'target_1d'],
    '1d': ['target_1d']
}
FEATURE_COLUMNS = {
    '15m': [
        'open', 'high', 'low', 'close', 'volume',
        'return_15m', 'return_1h', 'return_4h', 'return_1d',
        'ema_14', 'sma_14', 'rsi_14', 'stoch_k', 'stoch_d',
        'bb_mavg', 'bb_hband', 'bb_lband', 'bb_pband', 'bb_wband',
        'kc_hband', 'kc_lband', 'kc_mband', 'kc_pband', 'kc_wband',
        'atr_14', 'obv', 'macd', 'macd_signal', 'macd_diff',
        'adx', 'adx_pos', 'adx_neg', 'ulcer_index',
        'adi', 'cmf', 'eom', 'vpt'
    ],
    '1h': [
        'open', 'high', 'low', 'close', 'volume',
        'return_15m', 'return_1h', 'return_4h', 'return_1d',
        'ema_14', 'sma_14', 'rsi_14', 'stoch_k', 'stoch_d',
        'bb_mavg', 'bb_hband', 'bb_lband', 'bb_pband', 'bb_wband',
        'kc_hband', 'kc_lband', 'kc_mband', 'kc_pband', 'kc_wband',
        'atr_14', 'obv', 'macd', 'macd_signal', 'macd_diff',
        'adx', 'adx_pos', 'adx_neg', 'ulcer_index',
        'adi', 'cmf', 'eom', 'vpt'
    ],
    '4h': [
        'open', 'high', 'low', 'close', 'volume',
        'return_15m', 'return_1h', 'return_4h', 'return_1d',
        'ema_14', 'sma_14', 'rsi_14', 'stoch_k', 'stoch_d',
        'bb_mavg', 'bb_hband', 'bb_lband', 'bb_pband', 'bb_wband',
        'kc_hband', 'kc_lband', 'kc_mband', 'kc_pband', 'kc_wband',
        'atr_14', 'obv', 'macd', 'macd_signal', 'macd_diff',
        'adx', 'adx_pos', 'adx_neg', 'ulcer_index',
        'adi', 'cmf', 'eom', 'vpt'
    ],
    '1d': [
        'open', 'high', 'low', 'close', 'volume',
        'return_15m', 'return_1h', 'return_4h', 'return_1d',
        'ema_14', 'sma_14', 'rsi_14', 'stoch_k', 'stoch_d',
        'bb_mavg', 'bb_hband', 'bb_lband', 'bb_pband', 'bb_wband',
        'kc_hband', 'kc_lband', 'kc_mband', 'kc_pband', 'kc_wband',
        'atr_14', 'obv', 'macd', 'macd_signal', 'macd_diff',
        'adx', 'adx_pos', 'adx_neg', 'ulcer_index',
        'adi', 'cmf', 'eom', 'vpt'
    ]
}
SEQ_LENGTH = 10
BATCH_SIZE = 256  # Adjust based on GPU memory
NUM_WORKERS = cpu_count()  # Define NUM_WORKERS using cpu_count()
SCALER_DIR = 'scalers'
MODEL_DIR = 'models'
NUM_EPOCHS = 50
LEARNING_RATE = 0.0001

# Ensure directories exist
os.makedirs(SCALER_DIR, exist_ok=True)
os.makedirs(MODEL_DIR, exist_ok=True)

# Enable CUDA benchmark for optimized performance on fixed input sizes
torch.backends.cudnn.benchmark = True

# Check device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def load_and_preprocess(processed_path, feature_cols, target_cols, scaler_path):
    """
    Loads and preprocesses the data.

    Parameters:
    - processed_path: Path to the processed CSV file.
    - feature_cols: List of feature column names.
    - target_cols: List of target column names.
    - scaler_path: Path to save/load the scaler.

    Returns:
    - df_scaled: DataFrame with scaled features and targets.
    """
    # Load processed data
    df_processed = pd.read_csv(
        processed_path,
        parse_dates=['open time'],
        dtype={col: np.float32 for col in feature_cols},
        compression='gzip'
    )

    # Handle missing values using forward and backward fill
    df_processed.ffill(inplace=True)
    df_processed.bfill(inplace=True)

    # Feature scaling
    scaler = StandardScaler()
    scaled_features = scaler.fit_transform(df_processed[feature_cols].astype(np.float32))
    df_scaled = pd.DataFrame(scaled_features, columns=feature_cols, index=df_processed.index)
    df_scaled['open time'] = df_processed['open time']
    for target in target_cols:
        df_scaled[target] = df_processed[target].astype(np.float32)

    # Save the scaler
    joblib.dump(scaler, scaler_path)

    return df_scaled

def create_sequences(data, seq_length, feature_cols, target_cols):
    """
    Creates input sequences and corresponding targets.

    Parameters:
    - data: DataFrame containing features and targets.
    - seq_length: Number of time steps in each input sequence.
    - feature_cols: List of feature column names.
    - target_cols: List of target column names.

    Returns:
    - sequences: Numpy array of shape (num_samples, seq_length, num_features)
    - targets: Dictionary of Numpy arrays for each target
    """
    sequences = []
    targets = {target: [] for target in target_cols}

    for i in range(len(data) - seq_length):
        seq = data[feature_cols].iloc[i:i+seq_length].values
        sequences.append(seq)
        for target in target_cols:
            targets[target].append(data[target].iloc[i+seq_length])

    sequences = np.array(sequences, dtype=np.float32)
    for target in target_cols:
        targets[target] = np.array(targets[target], dtype=np.float32)

    return sequences, targets

class FinancialDataset(Dataset):
    """
    Custom Dataset for financial data sequences.
    """
    def __init__(self, sequences, targets):
        self.sequences = torch.tensor(sequences, dtype=torch.float32)
        self.targets = {key: torch.tensor(val, dtype=torch.float32).unsqueeze(1) for key, val in targets.items()}

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        sequence = self.sequences[idx]
        target = {key: self.targets[key][idx] for key in self.targets}
        return sequence, target

class LNNModel(nn.Module):
    """
    Optimized Liquid Neural Network Model using LSTM layers with dropout and batch normalization.
    """
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.3):
        super(LNNModel, self).__init__()
        self.lstm = nn.LSTM(
            input_size,
            hidden_size,
            num_layers,
            batch_first=True,
            dropout=dropout,
            bidirectional=True
        )
        self.batch_norm = nn.BatchNorm1d(hidden_size * 2)
        self.fc = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, output_size)
        )

    def forward(self, x):
        # Forward propagate LSTM
        out, _ = self.lstm(x)

        # Apply batch normalization on the last time step
        out = self.batch_norm(out[:, -1, :])

        # Decode the hidden state of the last time step
        out = self.fc(out)
        return out

def train_model(timeframe):
    """
    Trains the LNN model for a specific timeframe.

    Parameters:
    - timeframe: Timeframe identifier (e.g., '15m', '1h').

    Returns:
    - None
    """
    processed_path = PROCESSED_DATA_PATHS[timeframe]
    feature_cols = FEATURE_COLUMNS[timeframe]
    target_cols = TARGET_COLUMNS[timeframe]
    scaler_path = os.path.join(SCALER_DIR, f"scaler_{timeframe}.save")
    model_save_path = os.path.join(MODEL_DIR, f"lnn_model_{timeframe}.pth")

    # Load and preprocess data
    df_scaled = load_and_preprocess(processed_path, feature_cols, target_cols, scaler_path)

    # Create sequences and targets
    sequences, targets = create_sequences(df_scaled, SEQ_LENGTH, feature_cols, target_cols)

    # For simplicity, let's train to predict the first target in target_cols
    primary_target = target_cols[0]
    dataset = FinancialDataset(sequences, {primary_target: targets[primary_target]})
    data_loader = DataLoader(
        dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=NUM_WORKERS,
        pin_memory=True
    )

    # Initialize model, loss function, and optimizer
    input_size = len(feature_cols)
    hidden_size = 256
    num_layers = 3
    output_size = 1

    model = LNNModel(input_size, hidden_size, num_layers, output_size).to(device)
    criterion = nn.MSELoss()
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)

    # Initialize GradScaler for mixed precision training
    scaler = GradScaler()

    # Learning rate scheduler
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)

    # Training loop
    for epoch in range(NUM_EPOCHS):
        model.train()
        epoch_loss = 0
        progress = tqdm(
            data_loader,
            desc=f"{timeframe} Epoch {epoch+1}/{NUM_EPOCHS}",
            leave=False,
            ncols=100
        )

        for sequences_batch, targets_batch in progress:
            sequences_batch = sequences_batch.to(device, non_blocking=True)
            targets_batch = targets_batch.to(device, non_blocking=True)

            optimizer.zero_grad()

            # Mixed precision forward pass
            with autocast():
                outputs = model(sequences_batch)
                loss = criterion(outputs, targets_batch)

            # Scales loss and backpropagates
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            epoch_loss += loss.item()
            progress.set_postfix(loss=loss.item())

        avg_loss = epoch_loss / len(data_loader)
        print(f"{timeframe} Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {avg_loss:.6f}")

        # Step the scheduler
        scheduler.step(avg_loss)

    # Save the trained model
    torch.save(model.state_dict(), model_save_path)
    print(f"{timeframe} Model training complete and saved to {model_save_path}")

def main():
    """
    Main function to train models for all timeframes in parallel.
    """
    timeframes = list(PROCESSED_DATA_PATHS.keys())
    with Pool(processes=cpu_count()) as pool:
        pool.map(train_model, timeframes)

if __name__ == "__main__":
    main()


// File: preprocess.py
import pandas as pd
import numpy as np
import os
from ta import trend, momentum, volatility, volume
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import torch
import torch.utils.data
from numba import jit, cuda
import cupy as cp
from functools import partial
from pathlib import Path
import psutil
import warnings
from tqdm.auto import tqdm
import gc

class GPUAcceleratedPreprocessor:
    def __init__(self):
        self.num_cpu_cores = psutil.cpu_count(logical=True)
        self.batch_size = self._calculate_optimal_batch_size()
        self.raw_data_paths = {
            "15m": Path("data/raw/btc_15m_data_2018_to_2024-2024-10-10.csv"),
            "1h": Path("data/raw/btc_1h_data_2018_to_2024-2024-10-10.csv"),
            "4h": Path("data/raw/btc_4h_data_2018_to_2024-2024-10-10.csv"),
            "1d": Path("data/raw/btc_1d_data_2018_to_2024-2024-10-10.csv")
        }
        self.processed_data_dir = Path("data/final")
        self.processed_data_dir.mkdir(parents=True, exist_ok=True)

    def _calculate_optimal_batch_size(self):
        available_ram = psutil.virtual_memory().available
        return min(int(available_ram * 0.2 / (1024 * 1024)), 1000000)

    @staticmethod
    @jit(nopython=True)
    def _calculate_returns(prices, period):
        returns = np.zeros_like(prices)
        returns[period:] = (prices[period:] - prices[:-period]) / prices[:-period]
        returns[:period] = returns[period]
        return returns

    @staticmethod
    @jit(nopython=True)
    def _calculate_ma(data, window):
        result = np.zeros_like(data)
        for i in range(len(data)):
            if i < window:
                result[i] = np.mean(data[:i+1])
            else:
                result[i] = np.mean(data[i-window+1:i+1])
        return result

    @staticmethod
    @jit(nopython=True)
    def _calculate_vwap(high, low, close, volume):
        typical_price = (high + low + close) / 3
        cumulative_tp_vol = np.cumsum(typical_price * volume)
        cumulative_vol = np.cumsum(volume)
        return np.where(cumulative_vol > 0, cumulative_tp_vol / cumulative_vol, typical_price)

    @staticmethod
    @jit(nopython=True)
    def _calculate_price_impact(volume, price_change):
        return np.where(volume > 0, price_change / volume, 0)

    @staticmethod
    @jit(nopython=True)
    def _calculate_orderflow_imbalance(buy_volume, sell_volume, window=14):
        total_volume = buy_volume + sell_volume
        imbalance = np.zeros_like(buy_volume)
        
        for i in range(len(buy_volume)):
            start_idx = max(0, i - window + 1)
            buy_sum = np.sum(buy_volume[start_idx:i+1])
            total_sum = np.sum(total_volume[start_idx:i+1])
            imbalance[i] = buy_sum / total_sum if total_sum > 0 else 0.5
            
        return imbalance

    def _gpu_preprocess_chunk(self, df_chunk, device):
        df_chunk.columns = df_chunk.columns.str.lower()
        df_chunk.sort_values('open time', inplace=True)
        df_chunk.reset_index(drop=True, inplace=True)
        
        price_columns = ['open', 'high', 'low', 'close']
        periods = {'15m': 1, '1h': 4, '4h': 16, '1d': 96}
        
        for col in price_columns:
            prices = df_chunk[col].values
            for period_name, period in periods.items():
                df_chunk[f'return_{col}_{period_name}'] = self._calculate_returns(prices, period)

        df_chunk['vwap'] = self._calculate_vwap(
            df_chunk['high'].values,
            df_chunk['low'].values,
            df_chunk['close'].values,
            df_chunk['volume'].values
        )
        
        df_chunk['taker_buy_ratio'] = (
            df_chunk['taker buy base asset volume'] / df_chunk['volume']
        ).fillna(0.5)
        
        df_chunk['quote_volume_ratio'] = (
            df_chunk['quote asset volume'] / df_chunk['volume']
        ).fillna(1.0)
        
        df_chunk['trades_per_volume'] = (
            df_chunk['number of trades'] / df_chunk['volume']
        ).ffill().bfill()
        
        df_chunk['avg_trade_size'] = (
            df_chunk['volume'] / df_chunk['number of trades']
        ).ffill().bfill()
        
        price_changes = df_chunk['close'] - df_chunk['open']
        df_chunk['price_impact'] = self._calculate_price_impact(
            df_chunk['volume'].values,
            price_changes.values
        )
        
        df_chunk['order_flow_imbalance'] = self._calculate_orderflow_imbalance(
            df_chunk['taker buy base asset volume'].values,
            df_chunk['volume'].values - df_chunk['taker buy base asset volume'].values
        )
        
        for window in [14, 20]:
            close_prices = df_chunk['close'].values
            df_chunk[f'ma_{window}'] = self._calculate_ma(close_prices, window)
        
        indicators = {
            'rsi': momentum.RSIIndicator(close=df_chunk['close'], window=14),
            'stoch': momentum.StochasticOscillator(
                high=df_chunk['high'],
                low=df_chunk['low'],
                close=df_chunk['close'],
                window=14,
                smooth_window=3
            ),
            'bb': volatility.BollingerBands(
                close=df_chunk['close'],
                window=20,
                window_dev=2
            ),
            'atr': volatility.AverageTrueRange(
                high=df_chunk['high'],
                low=df_chunk['low'],
                close=df_chunk['close'],
                window=14
            ),
            'adi': volume.AccDistIndexIndicator(
                high=df_chunk['high'],
                low=df_chunk['low'],
                close=df_chunk['close'],
                volume=df_chunk['volume']
            )
        }
        
        df_chunk['rsi'] = indicators['rsi'].rsi()
        df_chunk['stoch_k'] = indicators['stoch'].stoch()
        df_chunk['stoch_d'] = indicators['stoch'].stoch_signal()
        df_chunk['bb_upper'] = indicators['bb'].bollinger_hband()
        df_chunk['bb_lower'] = indicators['bb'].bollinger_lband()
        df_chunk['bb_middle'] = indicators['bb'].bollinger_mavg()
        df_chunk['atr'] = indicators['atr'].average_true_range()
        df_chunk['adi'] = indicators['adi'].acc_dist_index()
        
        volatility_windows = [14, 30, 60]
        returns = df_chunk['close'].pct_change()
        for window in volatility_windows:
            df_chunk[f'volatility_{window}'] = returns.rolling(
                window=window, 
                min_periods=1
            ).std().bfill()
        
        df_chunk['high_low_range'] = (df_chunk['high'] - df_chunk['low']) / df_chunk['low']
        df_chunk['body_range'] = abs(df_chunk['close'] - df_chunk['open']) / df_chunk['open']
        df_chunk['upper_wick'] = (df_chunk['high'] - df_chunk[['open', 'close']].max(axis=1)) / df_chunk['close']
        df_chunk['lower_wick'] = (df_chunk[['open', 'close']].min(axis=1) - df_chunk['low']) / df_chunk['close']
        
        time_features = pd.to_datetime(df_chunk['open time'])
        df_chunk['hour'] = time_features.dt.hour
        df_chunk['day_of_week'] = time_features.dt.dayofweek
        df_chunk['day_of_month'] = time_features.dt.day
        df_chunk['week_of_year'] = time_features.dt.isocalendar().week
        df_chunk['month'] = time_features.dt.month
        
        return df_chunk.ffill().bfill()

    def preprocess_chunk(self, df_chunk, timeframe):
        try:
            if torch.cuda.is_available():
                device = torch.device('cuda')
                return self._gpu_preprocess_chunk(df_chunk, device)
            else:
                return self._gpu_preprocess_chunk(df_chunk, torch.device('cpu'))
        except Exception as e:
            print(f"Error processing chunk in {timeframe}: {str(e)}")
            raise

    def process_file(self, timeframe):
        try:
            raw_path = self.raw_data_paths[timeframe]
            processed_path = self.processed_data_dir / f"processed_data_{timeframe}.csv.gz"
            
            chunks = []
            reader = pd.read_csv(raw_path, 
                               chunksize=self.batch_size, 
                               parse_dates=['Open time', 'Close time'])
            
            with ThreadPoolExecutor(max_workers=self.num_cpu_cores) as executor:
                futures = []
                for chunk in reader:
                    future = executor.submit(self.preprocess_chunk, chunk, timeframe)
                    futures.append(future)
                
                chunks = []
                for future in tqdm(futures, desc=f"Processing {timeframe}"):
                    try:
                        result = future.result()
                        chunks.append(result)
                    except Exception as e:
                        print(f"Error processing chunk in {timeframe}: {str(e)}")
                        continue
            
            if not chunks:
                raise ValueError(f"No chunks were successfully processed for {timeframe}")
            
            df_processed = pd.concat(chunks, ignore_index=True)
            self._add_target_variables(df_processed, timeframe)
            
            df_processed.to_csv(processed_path, index=False, compression='gzip')
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
        except Exception as e:
            print(f"Error processing file {timeframe}: {str(e)}")
            raise

    def _add_target_variables(self, df, timeframe):
        shifts = {
            '15m': {'15m': 1, '1h': 4, '4h': 16, '1d': 96},
            '1h': {'1h': 1, '4h': 4, '1d': 16},
            '4h': {'4h': 1, '1d': 4},
            '1d': {'1d': 1}
        }
        
        for target, shift in shifts[timeframe].items():
            df[f'target_{target}'] = df[f'return_close_{target}'].shift(-shift)
        
        df.dropna(subset=[f'target_{t}' for t in shifts[timeframe].keys()], 
                 inplace=True)
        df.reset_index(drop=True, inplace=True)

    def process_all_files(self):
        with ProcessPoolExecutor(max_workers=min(len(self.raw_data_paths), 
                                               self.num_cpu_cores)) as executor:
            list(executor.map(self.process_file, self.raw_data_paths.keys()))

if __name__ == "__main__":
    preprocessor = GPUAcceleratedPreprocessor()
    preprocessor.process_all_files()

// File: test.py
# File: test.py

import unittest
import pandas as pd
import numpy as np
import torch
from torch.utils.data import DataLoader
from sklearn.preprocessing import StandardScaler
from unittest.mock import patch, MagicMock
import joblib
import os
import shutil
import multiprocessing  # Import multiprocessing
from multiprocessing import Pool, cpu_count  # Import Pool and cpu_count

# Import functions and classes from lnn_module.py
from lnn_module import (
    load_and_preprocess,
    FEATURE_COLUMNS,
    TARGET_COLUMNS,
    create_sequences,
    FinancialDataset,
    LNNModel,
    SEQ_LENGTH,
    BATCH_SIZE,
    SCALER_DIR,
    MODEL_DIR
)

# Define constants for testing
TEST_PROCESSED_DATA_DIR = 'test_data_final'
TEST_RAW_DATA_DIR = 'test_data_raw'
TEST_SCALER_DIR = 'test_scalers'
TEST_MODEL_DIR = 'test_models'

# Define NUM_WORKERS
NUM_WORKERS = cpu_count()

class TestLNNProgram(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        """
        Set up synthetic datasets for testing purposes.
        This method runs once before all tests.
        """
        os.makedirs(TEST_PROCESSED_DATA_DIR, exist_ok=True)
        os.makedirs(TEST_RAW_DATA_DIR, exist_ok=True)
        os.makedirs(TEST_SCALER_DIR, exist_ok=True)
        os.makedirs(TEST_MODEL_DIR, exist_ok=True)

        # Create synthetic raw data for each timeframe
        cls.synthetic_raw_data = {}
        timeframes = ['15m', '1h', '4h', '1d']
        for timeframe in timeframes:
            raw_filename = f"btc_{timeframe}_data_2018_to_2024-2024-10-10.csv"
            raw_path = os.path.join(TEST_RAW_DATA_DIR, raw_filename)
            freq = '15T' if timeframe == '15m' else 'H' if timeframe == '1h' else '4H' if timeframe == '4h' else 'D'
            data = {
                'Open time': pd.date_range(start='2022-01-01', periods=1000, freq=freq),
                'Open': np.random.rand(1000).astype(np.float32) * 10000,
                'High': np.random.rand(1000).astype(np.float32) * 10000,
                'Low': np.random.rand(1000).astype(np.float32) * 10000,
                'Close': np.random.rand(1000).astype(np.float32) * 10000,
                'Volume': np.random.rand(1000).astype(np.float32) * 100,
                'Close time': pd.date_range(start='2022-01-01', periods=1000, freq=freq),
                'Quote asset volume': np.random.rand(1000).astype(np.float32) * 1e6,
                'Number of trades': np.random.randint(100, 1000, size=1000),
                'Taker buy base asset volume': np.random.rand(1000).astype(np.float32) * 100,
                'Taker buy quote asset volume': np.random.rand(1000).astype(np.float32) * 1e5,
                'Ignore': [0]*1000
            }
            df_raw = pd.DataFrame(data)
            df_raw.to_csv(raw_path, index=False)
            cls.synthetic_raw_data[timeframe] = raw_path

        # Preprocess synthetic raw data
        from preprocess import preprocess_file  # Import the preprocess_file function

        for timeframe, raw_path in cls.synthetic_raw_data.items():
            processed_filename = f"processed_data_{timeframe}.csv.gz"
            processed_path = os.path.join(TEST_PROCESSED_DATA_DIR, processed_filename)
            preprocess_file(timeframe, raw_path, processed_path)

    @classmethod
    def tearDownClass(cls):
        """
        Clean up the synthetic datasets after all tests have run.
        """
        shutil.rmtree(TEST_PROCESSED_DATA_DIR, ignore_errors=True)
        shutil.rmtree(TEST_RAW_DATA_DIR, ignore_errors=True)
        shutil.rmtree(TEST_SCALER_DIR, ignore_errors=True)
        shutil.rmtree(TEST_MODEL_DIR, ignore_errors=True)

    def test_load_and_preprocess(self):
        """
        Test the load_and_preprocess function to ensure it correctly loads data,
        handles missing values, and scales features for all timeframes.
        """
        timeframes = ['15m', '1h', '4h', '1d']
        for timeframe in timeframes:
            with self.subTest(timeframe=timeframe):
                processed_path = os.path.join(TEST_PROCESSED_DATA_DIR, f"processed_data_{timeframe}.csv.gz")
                feature_cols = FEATURE_COLUMNS[timeframe]
                target_cols = TARGET_COLUMNS[timeframe]
                scaler_path = os.path.join(TEST_SCALER_DIR, f"scaler_{timeframe}.save")

                df_scaled = load_and_preprocess(processed_path, feature_cols, target_cols, scaler_path)

                # Check for missing values
                self.assertFalse(df_scaled.isnull().values.any(), f"Missing values in {timeframe} data.")

                # Check if scaler was saved
                self.assertTrue(os.path.exists(scaler_path), f"Scaler not saved for {timeframe} data.")

                # Load scaler and verify scaling
                scaler = joblib.load(scaler_path)
                original_features = pd.read_csv(processed_path, compression='gzip')[feature_cols].astype(np.float32)
                scaled_features = scaler.transform(original_features)
                np.testing.assert_array_almost_equal(
                    df_scaled[feature_cols].values,
                    scaled_features,
                    decimal=5,
                    err_msg=f"Feature scaling incorrect for {timeframe} data."
                )

    def test_create_sequences(self):
        """
        Test the create_sequences function to ensure it correctly creates input sequences and targets for all timeframes.
        """
        timeframes = ['15m', '1h', '4h', '1d']
        for timeframe in timeframes:
            with self.subTest(timeframe=timeframe):
                processed_path = os.path.join(TEST_PROCESSED_DATA_DIR, f"processed_data_{timeframe}.csv.gz")
                feature_cols = FEATURE_COLUMNS[timeframe]
                target_cols = TARGET_COLUMNS[timeframe]
                scaler_path = os.path.join(TEST_SCALER_DIR, f"scaler_{timeframe}.save")

                df_scaled = load_and_preprocess(processed_path, feature_cols, target_cols, scaler_path)
                sequences, targets = create_sequences(df_scaled, SEQ_LENGTH, feature_cols, target_cols)

                # Calculate expected number of sequences
                expected_num_sequences = len(df_scaled) - SEQ_LENGTH
                actual_num_sequences = sequences.shape[0]

                self.assertEqual(
                    actual_num_sequences,
                    expected_num_sequences,
                    f"Sequences count mismatch for {timeframe} data. Expected {expected_num_sequences}, got {actual_num_sequences}."
                )

                # Check shapes
                self.assertEqual(
                    sequences.shape[1],
                    SEQ_LENGTH,
                    f"Sequence length incorrect for {timeframe} data."
                )
                self.assertEqual(
                    sequences.shape[2],
                    len(feature_cols),
                    f"Number of features incorrect for {timeframe} data."
                )

                for target in target_cols:
                    self.assertEqual(
                        len(targets[target]),
                        expected_num_sequences,
                        f"Target length mismatch for {timeframe} data."
                    )

    def test_financial_dataset(self):
        """
        Test the FinancialDataset class to ensure it correctly retrieves sequences and targets for all timeframes.
        """
        timeframes = ['15m', '1h', '4h', '1d']
        for timeframe in timeframes:
            with self.subTest(timeframe=timeframe):
                processed_path = os.path.join(TEST_PROCESSED_DATA_DIR, f"processed_data_{timeframe}.csv.gz")
                feature_cols = FEATURE_COLUMNS[timeframe]
                target_cols = TARGET_COLUMNS[timeframe]
                scaler_path = os.path.join(TEST_SCALER_DIR, f"scaler_{timeframe}.save")

                df_scaled = load_and_preprocess(processed_path, feature_cols, target_cols, scaler_path)
                sequences, targets = create_sequences(df_scaled, SEQ_LENGTH, feature_cols, target_cols)
                dataset = FinancialDataset(sequences, {target_cols[0]: targets[target_cols[0]]})  # Testing first target

                # Test dataset length
                self.assertEqual(
                    len(dataset),
                    len(sequences),
                    f"Dataset length mismatch for {timeframe} data."
                )

                # Test getting an item
                sample_seq, sample_target = dataset[0]
                np.testing.assert_array_almost_equal(
                    sample_seq.numpy(),
                    sequences[0],
                    decimal=5,
                    err_msg=f"Sequence data incorrect for {timeframe} data."
                )
                self.assertAlmostEqual(
                    sample_target.item(),
                    targets[target_cols[0]][0],
                    places=6,
                    msg=f"Target value incorrect for {timeframe} data."
                )

    def test_lnn_model_forward(self):
        """
        Test the LNNModel's forward pass to ensure it produces outputs of the correct shape for all timeframes.
        """
        timeframes = ['15m', '1h', '4h', '1d']
        for timeframe in timeframes:
            with self.subTest(timeframe=timeframe):
                input_size = len(FEATURE_COLUMNS[timeframe])
                hidden_size = 128
                num_layers = 2
                output_size = 1

                model = LNNModel(input_size, hidden_size, num_layers, output_size)
                model.eval()  # Set model to evaluation mode

                # Create a dummy input tensor with batch_size=2
                dummy_input = torch.randn(2, SEQ_LENGTH, input_size)
                with torch.no_grad():
                    output = model(dummy_input)

                # Check output shape
                self.assertEqual(
                    output.shape,
                    (2, output_size),
                    f"Model output shape incorrect for {timeframe} data."
                )

    def test_full_pipeline(self):
        """
        Test the full data loading, preprocessing, dataset creation, and model initialization pipeline for all timeframes.
        This ensures that all components work together without errors.
        """
        timeframes = ['15m', '1h', '4h', '1d']
        for timeframe in timeframes:
            with self.subTest(timeframe=timeframe):
                try:
                    processed_path = os.path.join(TEST_PROCESSED_DATA_DIR, f"processed_data_{timeframe}.csv.gz")
                    feature_cols = FEATURE_COLUMNS[timeframe]
                    target_cols = TARGET_COLUMNS[timeframe]
                    scaler_path = os.path.join(TEST_SCALER_DIR, f"scaler_{timeframe}.save")
                    model_save_path = os.path.join(TEST_MODEL_DIR, f"lnn_model_{timeframe}.pth")

                    # Load and preprocess
                    df_scaled = load_and_preprocess(processed_path, feature_cols, target_cols, scaler_path)

                    # Create sequences and targets
                    sequences, targets = create_sequences(df_scaled, SEQ_LENGTH, feature_cols, target_cols)

                    # Create Dataset and DataLoader
                    dataset = FinancialDataset(sequences, {target_cols[0]: targets[target_cols[0]]})  # Using first target
                    data_loader = DataLoader(
                        dataset,
                        batch_size=BATCH_SIZE,
                        shuffle=True,
                        num_workers=NUM_WORKERS,
                        pin_memory=True
                    )

                    # Initialize model
                    input_size = len(feature_cols)
                    hidden_size = 128
                    num_layers = 2
                    output_size = 1

                    model = LNNModel(input_size, hidden_size, num_layers, output_size)
                    criterion = torch.nn.MSELoss()
                    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

                    # Perform a single training step
                    model.train()
                    for sequences_batch, targets_batch in data_loader:
                        # Forward pass
                        outputs = model(sequences_batch)
                        loss = criterion(outputs, targets_batch)

                        # Backward pass and optimization
                        optimizer.zero_grad()
                        loss.backward()
                        optimizer.step()
                        break  # Only one batch for testing

                    # Check if loss is a finite number
                    self.assertTrue(np.isfinite(loss.item()), f"Loss is not finite for {timeframe} data.")

                except Exception as e:
                    self.fail(f"Full pipeline test failed for {timeframe} with exception: {e}")

    @patch('joblib.dump')
    def test_scaler_save(self, mock_joblib_dump):
        """
        Test that the scaler is saved correctly during preprocessing for all timeframes.
        """
        timeframes = ['15m', '1h', '4h', '1d']
        for timeframe in timeframes:
            with self.subTest(timeframe=timeframe):
                processed_path = os.path.join(TEST_PROCESSED_DATA_DIR, f"processed_data_{timeframe}.csv.gz")
                feature_cols = FEATURE_COLUMNS[timeframe]
                target_cols = TARGET_COLUMNS[timeframe]
                scaler_path = os.path.join(TEST_SCALER_DIR, f"scaler_{timeframe}.save")

                load_and_preprocess(processed_path, feature_cols, target_cols, scaler_path)

                # Check that joblib.dump was called to save the scaler
                mock_joblib_dump.assert_called_with(
                    unittest.mock.ANY,  # The scaler object
                    scaler_path
                )

    def test_model_save(self):
        """
        Test that the model can be saved without errors for all timeframes.
        """
        timeframes = ['15m', '1h', '4h', '1d']
        for timeframe in timeframes:
            with self.subTest(timeframe=timeframe):
                model = LNNModel(
                    input_size=len(FEATURE_COLUMNS[timeframe]),
                    hidden_size=128,
                    num_layers=2,
                    output_size=1
                )
                model_save_path = os.path.join(TEST_MODEL_DIR, f"lnn_model_{timeframe}.pth")
                try:
                    torch.save(model.state_dict(), model_save_path)
                    self.assertTrue(os.path.exists(model_save_path), f"Model not saved correctly for {timeframe} data.")
                except Exception as e:
                    self.fail(f"Model save test failed for {timeframe} with exception: {e}")

if __name__ == '__main__':
    unittest.main()


// File: data\raw\btc_15m_data_2018_to_2024-2024-10-10.csv
// Snippet:
          Open time     Open     High      Low    Close     Volume              Close time  Quote asset volume  Number of trades  Taker buy base asset volume  Taker buy quote asset volume  Ignore
2018-01-01 00:00:00 13715.65 13715.65 13400.01 13556.15 123.616013 2018-01-01 00:14:59.999        1.675545e+06              1572                    63.227133                  8.576108e+05       0
2018-01-01 00:15:00 13533.75 13550.87 13402.00 13521.12  98.136430 2018-01-01 00:29:59.999        1.321757e+06              1461                    47.686389                  6.422812e+05       0
2018-01-01 00:30:00 13500.00 13545.37 13450.00 13470.41  79.904037 2018-01-01 00:44:59.999        1.078825e+06              1000                    43.710406                  5.900347e+05       0
2018-01-01 00:45:00 13494.65 13690.87 13450.00 13529.01 141.699719 2018-01-01 00:59:59.999        1.917783e+06              1195                    73.897993                  1.000614e+06       0
2018-01-01 01:00:00 13528.99 13571.74 13402.28 13445.63  72.537533 2018-01-01 01:14:59.999        9.778198e+05               898                    34.257652                  4.618369e+05       0

// File: data\raw\btc_1d_data_2018_to_2024-2024-10-10.csv
// Snippet:
 Open time     Open     High      Low    Close       Volume              Close time  Quote asset volume  Number of trades  Taker buy base asset volume  Taker buy quote asset volume  Ignore
2018-01-01 13715.65 13818.55 12750.00 13380.00  8609.915844 2018-01-01 23:59:59.999        1.147997e+08            105595                  3961.938946                  5.280975e+07       0
2018-01-02 13382.16 15473.49 12890.02 14675.11 20078.092111 2018-01-02 23:59:59.999        2.797171e+08            177728                 11346.326739                  1.580801e+08       0
2018-01-03 14690.00 15307.56 14150.00 14919.51 15905.667639 2018-01-03 23:59:59.999        2.361169e+08            162787                  8994.953566                  1.335873e+08       0
2018-01-04 14919.51 15280.00 13918.04 15059.54 21329.649574 2018-01-04 23:59:59.999        3.127816e+08            170310                 12680.812951                  1.861168e+08       0
2018-01-05 15059.56 17176.24 14600.00 16960.39 23251.491125 2018-01-05 23:59:59.999        3.693220e+08            192969                 13346.622293                  2.118299e+08       0

// File: data\raw\btc_1h_data_2018_to_2024-2024-10-10.csv
// Snippet:
          Open time     Open     High      Low    Close     Volume              Close time  Quote asset volume  Number of trades  Taker buy base asset volume  Taker buy quote asset volume  Ignore
2018-01-01 00:00:00 13715.65 13715.65 13400.01 13529.01 443.356199 2018-01-01 00:59:59.999        5.993910e+06              5228                   228.521921                  3.090541e+06       0
2018-01-01 01:00:00 13528.99 13595.89 13155.38 13203.06 383.697006 2018-01-01 01:59:59.999        5.154522e+06              4534                   180.840403                  2.430449e+06       0
2018-01-01 02:00:00 13203.00 13418.43 13200.00 13330.18 429.064572 2018-01-01 02:59:59.999        5.710192e+06              4887                   192.237935                  2.558505e+06       0
2018-01-01 03:00:00 13330.26 13611.27 13290.00 13410.03 420.087030 2018-01-01 03:59:59.999        5.657448e+06              4789                   137.918407                  1.858041e+06       0
2018-01-01 04:00:00 13434.98 13623.29 13322.15 13601.01 340.807329 2018-01-01 04:59:59.999        4.588047e+06              4563                   172.957635                  2.328058e+06       0

// File: data\raw\btc_4h_data_2018_to_2024-2024-10-10.csv
// Snippet:
          Open time     Open     High      Low    Close      Volume              Close time  Quote asset volume  Number of trades  Taker buy base asset volume  Taker buy quote asset volume  Ignore
2018-01-01 00:00:00 13715.65 13715.65 13155.38 13410.03 1676.204807 2018-01-01 03:59:59.999        2.251607e+07             19438                   739.518666                  9.937537e+06       0
2018-01-01 04:00:00 13434.98 13818.55 13322.15 13570.35 1302.214836 2018-01-01 07:59:59.999        1.770254e+07             18061                   588.516222                  7.998796e+06       0
2018-01-01 08:00:00 13569.98 13735.24 13001.13 13220.56 1319.755931 2018-01-01 11:59:59.999        1.765121e+07             17200                   582.668293                  7.792109e+06       0
2018-01-01 12:00:00 13220.56 13330.00 12750.00 13247.00 1831.933153 2018-01-01 15:59:59.999        2.391259e+07             21357                   830.314747                  1.083304e+07       0
2018-01-01 16:00:00 13247.00 13290.65 12940.00 13240.37 1092.337234 2018-01-01 19:59:59.999        1.434887e+07             13178                   557.453779                  7.324320e+06       0

