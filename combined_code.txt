**File Tree (Relevant Files Only)**
  .
    - lnn_module.py
    - preprocess.py
    - test.py
  data\raw
    - btc_15m_data_2018_to_2024-2024-10-10.csv
    - btc_1d_data_2018_to_2024-2024-10-10.csv
    - btc_1h_data_2018_to_2024-2024-10-10.csv
    - btc_4h_data_2018_to_2024-2024-10-10.csv

// File: lnn_module.py
# File: lnn_module.py

import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
import joblib
import os
from datetime import datetime
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
import multiprocessing
from torch.cuda.amp import GradScaler, autocast

# Define file paths for all timeframes
PROCESSED_DATA_DIR = r"data\final"
PROCESSED_DATA_PATHS = {
    "15m": os.path.join(PROCESSED_DATA_DIR, "processed_data_15m.csv.gz"),
    "1h": os.path.join(PROCESSED_DATA_DIR, "processed_data_1h.csv.gz"),
    "4h": os.path.join(PROCESSED_DATA_DIR, "processed_data_4h.csv.gz"),
    "1d": os.path.join(PROCESSED_DATA_DIR, "processed_data_1d.csv.gz")
}

# Parameters
TARGET_COLUMNS = {
    '15m': ['target_15m', 'target_1h', 'target_4h', 'target_1d'],
    '1h': ['target_1h', 'target_4h', 'target_1d'],
    '4h': ['target_4h', 'target_1d'],
    '1d': ['target_1d']
}
FEATURE_COLUMNS = {
    '15m': [
        'open', 'high', 'low', 'close', 'volume',
        'return_15m', 'return_1h', 'return_4h', 'return_1d',
        'ema_14', 'sma_14', 'rsi_14', 'stoch_k', 'stoch_d',
        'bb_mavg', 'bb_hband', 'bb_lband', 'bb_pband', 'bb_wband',
        'kc_hband', 'kc_lband', 'kc_mband', 'kc_pband', 'kc_wband',
        'atr_14', 'obv', 'macd', 'macd_signal', 'macd_diff',
        'adx', 'adx_pos', 'adx_neg', 'ulcer_index',
        'adi', 'cmf', 'eom', 'vpt'
    ],
    '1h': [
        'open', 'high', 'low', 'close', 'volume',
        'return_15m', 'return_1h', 'return_4h', 'return_1d',
        'ema_14', 'sma_14', 'rsi_14', 'stoch_k', 'stoch_d',
        'bb_mavg', 'bb_hband', 'bb_lband', 'bb_pband', 'bb_wband',
        'kc_hband', 'kc_lband', 'kc_mband', 'kc_pband', 'kc_wband',
        'atr_14', 'obv', 'macd', 'macd_signal', 'macd_diff',
        'adx', 'adx_pos', 'adx_neg', 'ulcer_index',
        'adi', 'cmf', 'eom', 'vpt'
    ],
    '4h': [
        'open', 'high', 'low', 'close', 'volume',
        'return_15m', 'return_1h', 'return_4h', 'return_1d',
        'ema_14', 'sma_14', 'rsi_14', 'stoch_k', 'stoch_d',
        'bb_mavg', 'bb_hband', 'bb_lband', 'bb_pband', 'bb_wband',
        'kc_hband', 'kc_lband', 'kc_mband', 'kc_pband', 'kc_wband',
        'atr_14', 'obv', 'macd', 'macd_signal', 'macd_diff',
        'adx', 'adx_pos', 'adx_neg', 'ulcer_index',
        'adi', 'cmf', 'eom', 'vpt'
    ],
    '1d': [
        'open', 'high', 'low', 'close', 'volume',
        'return_15m', 'return_1h', 'return_4h', 'return_1d',
        'ema_14', 'sma_14', 'rsi_14', 'stoch_k', 'stoch_d',
        'bb_mavg', 'bb_hband', 'bb_lband', 'bb_pband', 'bb_wband',
        'kc_hband', 'kc_lband', 'kc_mband', 'kc_pband', 'kc_wband',
        'atr_14', 'obv', 'macd', 'macd_signal', 'macd_diff',
        'adx', 'adx_pos', 'adx_neg', 'ulcer_index',
        'adi', 'cmf', 'eom', 'vpt'
    ]
}
SEQ_LENGTH = 10
BATCH_SIZE = 256  # Adjust based on GPU memory
NUM_WORKERS = multiprocessing.cpu_count()
SCALER_DIR = 'scalers'
MODEL_DIR = 'models'
NUM_EPOCHS = 50
LEARNING_RATE = 0.0001

# Ensure directories exist
os.makedirs(SCALER_DIR, exist_ok=True)
os.makedirs(MODEL_DIR, exist_ok=True)

# Enable CUDA benchmark for optimized performance on fixed input sizes
torch.backends.cudnn.benchmark = True

# Check device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def load_and_preprocess(processed_path, feature_cols, target_cols, scaler_path):
    """
    Loads and preprocesses the data.

    Parameters:
    - processed_path: Path to the processed CSV file.
    - feature_cols: List of feature column names.
    - target_cols: List of target column names.
    - scaler_path: Path to save/load the scaler.

    Returns:
    - df_scaled: DataFrame with scaled features and targets.
    """
    # Load processed data
    df_processed = pd.read_csv(
        processed_path,
        parse_dates=['open time'],
        dtype={col: np.float32 for col in feature_cols},
        compression='gzip'
    )

    # Handle missing values using forward and backward fill
    df_processed.ffill(inplace=True)
    df_processed.bfill(inplace=True)

    # Feature scaling
    scaler = StandardScaler()
    scaled_features = scaler.fit_transform(df_processed[feature_cols].astype(np.float32))
    df_scaled = pd.DataFrame(scaled_features, columns=feature_cols, index=df_processed.index)
    df_scaled['open time'] = df_processed['open time']
    for target in target_cols:
        df_scaled[target] = df_processed[target].astype(np.float32)

    # Save the scaler
    joblib.dump(scaler, scaler_path)

    return df_scaled

def create_sequences(data, seq_length, feature_cols, target_cols):
    """
    Creates input sequences and corresponding targets.

    Parameters:
    - data: DataFrame containing features and targets.
    - seq_length: Number of time steps in each input sequence.
    - feature_cols: List of feature column names.
    - target_cols: List of target column names.

    Returns:
    - sequences: Numpy array of shape (num_samples, seq_length, num_features)
    - targets: Dictionary of Numpy arrays for each target
    """
    sequences = []
    targets = {target: [] for target in target_cols}

    for i in range(len(data) - seq_length):
        seq = data[feature_cols].iloc[i:i+seq_length].values
        sequences.append(seq)
        for target in target_cols:
            targets[target].append(data[target].iloc[i+seq_length])

    sequences = np.array(sequences, dtype=np.float32)
    for target in target_cols:
        targets[target] = np.array(targets[target], dtype=np.float32)

    return sequences, targets

class FinancialDataset(Dataset):
    """
    Custom Dataset for financial data sequences.
    """
    def __init__(self, sequences, targets):
        self.sequences = torch.tensor(sequences, dtype=torch.float32)
        self.targets = {key: torch.tensor(val, dtype=torch.float32).unsqueeze(1) for key, val in targets.items()}

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        sequence = self.sequences[idx]
        target = {key: self.targets[key][idx] for key in self.targets}
        return sequence, target

class LNNModel(nn.Module):
    """
    Optimized Liquid Neural Network Model using LSTM layers with dropout and batch normalization.
    """
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.3):
        super(LNNModel, self).__init__()
        self.lstm = nn.LSTM(
            input_size,
            hidden_size,
            num_layers,
            batch_first=True,
            dropout=dropout,
            bidirectional=True
        )
        self.batch_norm = nn.BatchNorm1d(hidden_size * 2)
        self.fc = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, output_size)
        )

    def forward(self, x):
        # Forward propagate LSTM
        out, _ = self.lstm(x)

        # Apply batch normalization on the last time step
        out = self.batch_norm(out[:, -1, :])

        # Decode the hidden state of the last time step
        out = self.fc(out)
        return out

def train_model(timeframe):
    """
    Trains the LNN model for a specific timeframe.

    Parameters:
    - timeframe: Timeframe identifier (e.g., '15m', '1h').

    Returns:
    - None
    """
    processed_path = PROCESSED_DATA_PATHS[timeframe]
    feature_cols = FEATURE_COLUMNS[timeframe]
    target_cols = TARGET_COLUMNS[timeframe]
    scaler_path = os.path.join(SCALER_DIR, f"scaler_{timeframe}.save")
    model_save_path = os.path.join(MODEL_DIR, f"lnn_model_{timeframe}.pth")

    # Load and preprocess data
    df_scaled = load_and_preprocess(processed_path, feature_cols, target_cols, scaler_path)

    # Create sequences and targets
    sequences, targets = create_sequences(df_scaled, SEQ_LENGTH, feature_cols, target_cols)

    # For simplicity, let's train to predict the first target in target_cols
    primary_target = target_cols[0]
    dataset = FinancialDataset(sequences, {primary_target: targets[primary_target]})
    data_loader = DataLoader(
        dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=NUM_WORKERS,
        pin_memory=True
    )

    # Initialize model, loss function, and optimizer
    input_size = len(feature_cols)
    hidden_size = 256
    num_layers = 3
    output_size = 1

    model = LNNModel(input_size, hidden_size, num_layers, output_size).to(device)
    criterion = nn.MSELoss()
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)

    # Initialize GradScaler for mixed precision training
    scaler = GradScaler()

    # Learning rate scheduler
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)

    # Training loop
    for epoch in range(NUM_EPOCHS):
        model.train()
        epoch_loss = 0
        progress = tqdm(
            data_loader,
            desc=f"{timeframe} Epoch {epoch+1}/{NUM_EPOCHS}",
            leave=False,
            ncols=100
        )

        for sequences_batch, targets_batch in progress:
            sequences_batch = sequences_batch.to(device, non_blocking=True)
            targets_batch = targets_batch.to(device, non_blocking=True)

            optimizer.zero_grad()

            # Mixed precision forward pass
            with autocast():
                outputs = model(sequences_batch)
                loss = criterion(outputs, targets_batch)

            # Scales loss and backpropagates
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            epoch_loss += loss.item()
            progress.set_postfix(loss=loss.item())

        avg_loss = epoch_loss / len(data_loader)
        print(f"{timeframe} Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {avg_loss:.6f}")

        # Step the scheduler
        scheduler.step(avg_loss)

    # Save the trained model
    torch.save(model.state_dict(), model_save_path)
    print(f"{timeframe} Model training complete and saved to {model_save_path}")

def main():
    """
    Main function to train models for all timeframes in parallel.
    """
    timeframes = list(PROCESSED_DATA_PATHS.keys())
    with Pool(processes=cpu_count()) as pool:
        pool.map(train_model, timeframes)

if __name__ == "__main__":
    main()


// File: preprocess.py
# File: preprocess.py

import pandas as pd
import numpy as np
import os
from ta import trend, momentum, volatility, volume
from multiprocessing import Pool, cpu_count
from functools import partial
from tqdm import tqdm

# Define file paths for all timeframes
RAW_DATA_PATHS = {
    "15m": r"data\raw\btc_15m_data_2018_to_2024-2024-10-10.csv",
    "1h": r"data\raw\btc_1h_data_2018_to_2024-2024-10-10.csv",
    "4h": r"data\raw\btc_4h_data_2018_to_2024-2024-10-10.csv",
    "1d": r"data\raw\btc_1d_data_2018_to_2024-2024-10-10.csv"
}

PROCESSED_DATA_DIR = r"data\final"

# Parameters
FEATURE_COLUMNS = [
    'open', 'high', 'low', 'close', 'volume'
]
TARGET_COLUMNS = {
    '15m': ['target_15m', 'target_1h', 'target_4h', 'target_1d'],
    '1h': ['target_1h', 'target_4h', 'target_1d'],
    '4h': ['target_4h', 'target_1d'],
    '1d': ['target_1d']
}
SEQ_LENGTH = 10
CHUNKSIZE = 500000  # Adjust based on memory constraints

def preprocess_chunk(df_chunk, timeframe):
    """
    Preprocesses a chunk of the dataframe by calculating technical indicators.

    Parameters:
    - df_chunk: Pandas DataFrame chunk.
    - timeframe: Timeframe identifier (e.g., '15m', '1h').

    Returns:
    - df_chunk: Processed DataFrame chunk with technical indicators.
    """
    # Ensure column names are lowercase
    df_chunk.columns = df_chunk.columns.str.lower()

    # Sort by time to ensure chronological order
    df_chunk.sort_values('open time', inplace=True)
    df_chunk.reset_index(drop=True, inplace=True)

    # Handle missing values using forward and backward fill
    df_chunk.ffill(inplace=True)
    df_chunk.bfill(inplace=True)

    # Feature Engineering
    # Returns
    df_chunk['return_15m'] = df_chunk['close'].pct_change(periods=1)
    df_chunk['return_1h'] = df_chunk['close'].pct_change(periods=4)  # 1 hour = 4 * 15m
    df_chunk['return_4h'] = df_chunk['close'].pct_change(periods=16)  # 4 hours = 16 * 15m
    df_chunk['return_1d'] = df_chunk['close'].pct_change(periods=96)  # 1 day = 96 * 15m

    # Technical Indicators
    # EMA and SMA
    df_chunk['ema_14'] = trend.EMAIndicator(close=df_chunk['close'], window=14).ema_indicator()
    df_chunk['sma_14'] = trend.SMAIndicator(close=df_chunk['close'], window=14).sma_indicator()

    # RSI
    df_chunk['rsi_14'] = momentum.RSIIndicator(close=df_chunk['close'], window=14).rsi()

    # Stochastic Oscillator
    stochastic = momentum.StochasticOscillator(
        high=df_chunk['high'],
        low=df_chunk['low'],
        close=df_chunk['close'],
        window=14,
        smooth_window=3
    )
    df_chunk['stoch_k'] = stochastic.stoch()
    df_chunk['stoch_d'] = stochastic.stoch_signal()

    # Bollinger Bands
    bollinger = volatility.BollingerBands(close=df_chunk['close'], window=20, window_dev=2)
    df_chunk['bb_mavg'] = bollinger.bollinger_mavg()
    df_chunk['bb_hband'] = bollinger.bollinger_hband()
    df_chunk['bb_lband'] = bollinger.bollinger_lband()
    df_chunk['bb_pband'] = bollinger.bollinger_pband()
    df_chunk['bb_wband'] = bollinger.bollinger_wband()

    # Keltner Channels
    keltner = volatility.KeltnerChannel(
        high=df_chunk['high'],
        low=df_chunk['low'],
        close=df_chunk['close'],
        window=20,
        window_atr=10,
        fillna=True
    )
    df_chunk['kc_hband'] = keltner.keltner_channel_hband()
    df_chunk['kc_lband'] = keltner.keltner_channel_lband()
    df_chunk['kc_mband'] = keltner.keltner_channel_mband()
    df_chunk['kc_pband'] = keltner.keltner_channel_pband()
    df_chunk['kc_wband'] = keltner.keltner_channel_wband()

    # ATR
    df_chunk['atr_14'] = volatility.AverageTrueRange(
        high=df_chunk['high'],
        low=df_chunk['low'],
        close=df_chunk['close'],
        window=14
    ).average_true_range()

    # OBV
    df_chunk['obv'] = volume.OnBalanceVolumeIndicator(
        close=df_chunk['close'],
        volume=df_chunk['volume']
    ).on_balance_volume()

    # MACD
    macd = trend.MACD(close=df_chunk['close'])
    df_chunk['macd'] = macd.macd()
    df_chunk['macd_signal'] = macd.macd_signal()
    df_chunk['macd_diff'] = macd.macd_diff()

    # ADX
    adx_indicator = trend.ADXIndicator(
        high=df_chunk['high'],
        low=df_chunk['low'],
        close=df_chunk['close'],
        window=14
    )
    df_chunk['adx'] = adx_indicator.adx()
    df_chunk['adx_pos'] = adx_indicator.adx_pos()
    df_chunk['adx_neg'] = adx_indicator.adx_neg()

    # Ulcer Index
    df_chunk['ulcer_index'] = ulcer_index(df_chunk['close'], window=14)

    # Accumulation/Distribution Indicator (ADI)
    df_chunk['adi'] = volume.AccDistIndexIndicator(
        high=df_chunk['high'],
        low=df_chunk['low'],
        close=df_chunk['close'],
        volume=df_chunk['volume']
    ).acc_dist_index()

    # Chaikin Money Flow (CMF)
    df_chunk['cmf'] = volume.ChaikinMoneyFlowIndicator(
        high=df_chunk['high'],
        low=df_chunk['low'],
        close=df_chunk['close'],
        volume=df_chunk['volume'],
        window=20
    ).chaikin_money_flow()

    # Ease of Movement (EOM)
    df_chunk['eom'] = ease_of_movement(
        high=df_chunk['high'],
        low=df_chunk['low'],
        volume=df_chunk['volume'],
        window=14
    )

    # Volume Price Trend (VPT)
    df_chunk['vpt'] = volume.VolumePriceTrendIndicator(
        close=df_chunk['close'],
        volume=df_chunk['volume']
    ).volume_price_trend()

    return df_chunk

def ulcer_index(series, window=14):
    """
    Calculates the Ulcer Index for a given series.

    Parameters:
    - series: Pandas Series of prices.
    - window: Rolling window size.

    Returns:
    - ulcer: Pandas Series of Ulcer Index values.
    """
    drawdown = series.cummax() - series
    ulcer = np.sqrt((drawdown**2).rolling(window=window).mean())
    return ulcer

def ease_of_movement(high, low, volume, window=14):
    """
    Calculates the Ease of Movement (EOM) indicator.

    Parameters:
    - high: Pandas Series of high prices.
    - low: Pandas Series of low prices.
    - volume: Pandas Series of volume.
    - window: Rolling window size.

    Returns:
    - eom: Pandas Series of EOM values.
    """
    box_ratio = ((high + low) / 2 - (high.shift(1) + low.shift(1)) / 2) / volume
    box_ratio = box_ratio.fillna(0)
    eom = box_ratio.rolling(window=window).sum()
    return eom

def add_target_variables(df, timeframe):
    """
    Adds target variables based on the timeframe.

    Parameters:
    - df: Processed DataFrame.
    - timeframe: Timeframe identifier (e.g., '15m', '1h').

    Returns:
    - df: DataFrame with added target variables.
    """
    if timeframe == '15m':
        df['target_15m'] = df['return_15m'].shift(-1)
        df['target_1h'] = df['return_1h'].shift(-4)  # 1 hour ahead
        df['target_4h'] = df['return_4h'].shift(-16)  # 4 hours ahead
        df['target_1d'] = df['return_1d'].shift(-96)  # 1 day ahead
    elif timeframe == '1h':
        df['target_1h'] = df['return_1h'].shift(-1)
        df['target_4h'] = df['return_4h'].shift(-4)  # 4 hours ahead
        df['target_1d'] = df['return_1d'].shift(-16)  # 16 hours ahead
    elif timeframe == '4h':
        df['target_4h'] = df['return_4h'].shift(-1)
        df['target_1d'] = df['return_1d'].shift(-4)  # 4 days ahead
    elif timeframe == '1d':
        df['target_1d'] = df['return_1d'].shift(-1)
    
    # Drop rows with NaN targets
    df.dropna(subset=TARGET_COLUMNS[timeframe], inplace=True)
    df.reset_index(drop=True, inplace=True)
    
    return df

def preprocess_file(timeframe, raw_path, processed_path, chunksize=CHUNKSIZE):
    """
    Processes a single raw data file and saves the processed data.

    Parameters:
    - timeframe: Timeframe identifier (e.g., '15m', '1h').
    - raw_path: Path to the raw CSV file.
    - processed_path: Path to save the processed CSV file.
    - chunksize: Number of rows per chunk.

    Returns:
    - None
    """
    processed_chunks = []
    try:
        # Read raw data in chunks
        reader = pd.read_csv(raw_path, chunksize=chunksize, parse_dates=['Open time'])

        for chunk in tqdm(reader, desc=f"Processing {timeframe} data"):
            processed_chunk = preprocess_chunk(chunk, timeframe)
            processed_chunks.append(processed_chunk)

        # Concatenate all processed chunks
        df_processed = pd.concat(processed_chunks, ignore_index=True)

        # Add target variables
        df_processed = add_target_variables(df_processed, timeframe)

        # Save the processed data with gzip compression
        df_processed.to_csv(processed_path, index=False, compression='gzip')
        print(f"Processed data for {timeframe} saved to {processed_path}")

    except Exception as e:
        print(f"Error processing {timeframe} data: {e}")

def main():
    """
    Main function to preprocess all raw data files in parallel.
    """
    # Ensure the output directory exists
    os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)

    tasks = []
    for timeframe, raw_path in RAW_DATA_PATHS.items():
        processed_filename = f"processed_data_{timeframe}.csv.gz"
        processed_path = os.path.join(PROCESSED_DATA_DIR, processed_filename)
        tasks.append((timeframe, raw_path, processed_path))

    # Use multiprocessing Pool to process files in parallel
    with Pool(processes=cpu_count()) as pool:
        pool.starmap(preprocess_file, tasks)

if __name__ == "__main__":
    main()


// File: test.py
# File: test.py

import unittest
import pandas as pd
import numpy as np
import torch
from torch.utils.data import DataLoader
from sklearn.preprocessing import StandardScaler
from unittest.mock import patch, MagicMock
import joblib
import os
import shutil

# Import functions and classes from lnn_module.py
from lnn_module import (
    load_and_preprocess,
    FEATURE_COLUMNS,
    TARGET_COLUMNS,
    create_sequences,
    FinancialDataset,
    LNNModel,
    SEQ_LENGTH,
    BATCH_SIZE,
    SCALER_DIR,
    MODEL_DIR
)

# Define constants for testing
TEST_PROCESSED_DATA_DIR = 'test_data_final'
TEST_RAW_DATA_DIR = 'test_data_raw'
TEST_SCALER_DIR = 'test_scalers'
TEST_MODEL_DIR = 'test_models'

class TestLNNProgram(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        """
        Set up synthetic datasets for testing purposes.
        This method runs once before all tests.
        """
        os.makedirs(TEST_PROCESSED_DATA_DIR, exist_ok=True)
        os.makedirs(TEST_RAW_DATA_DIR, exist_ok=True)
        os.makedirs(TEST_SCALER_DIR, exist_ok=True)
        os.makedirs(TEST_MODEL_DIR, exist_ok=True)

        # Create synthetic raw data for each timeframe
        cls.synthetic_raw_data = {}
        timeframes = ['15m', '1h', '4h', '1d']
        for timeframe in timeframes:
            raw_filename = f"btc_{timeframe}_data_2018_to_2024-2024-10-10.csv"
            raw_path = os.path.join(TEST_RAW_DATA_DIR, raw_filename)
            freq = '15T' if timeframe == '15m' else 'H' if timeframe == '1h' else '4H' if timeframe == '4h' else 'D'
            data = {
                'Open time': pd.date_range(start='2022-01-01', periods=1000, freq=freq),
                'Open': np.random.rand(1000).astype(np.float32) * 10000,
                'High': np.random.rand(1000).astype(np.float32) * 10000,
                'Low': np.random.rand(1000).astype(np.float32) * 10000,
                'Close': np.random.rand(1000).astype(np.float32) * 10000,
                'Volume': np.random.rand(1000).astype(np.float32) * 100,
                'Close time': pd.date_range(start='2022-01-01', periods=1000, freq=freq),
                'Quote asset volume': np.random.rand(1000).astype(np.float32) * 1e6,
                'Number of trades': np.random.randint(100, 1000, size=1000),
                'Taker buy base asset volume': np.random.rand(1000).astype(np.float32) * 100,
                'Taker buy quote asset volume': np.random.rand(1000).astype(np.float32) * 1e5,
                'Ignore': [0]*1000
            }
            df_raw = pd.DataFrame(data)
            df_raw.to_csv(raw_path, index=False)
            cls.synthetic_raw_data[timeframe] = raw_path

        # Preprocess synthetic raw data
        from preprocess import preprocess_file  # Import the preprocess_file function

        for timeframe, raw_path in cls.synthetic_raw_data.items():
            processed_filename = f"processed_data_{timeframe}.csv.gz"
            processed_path = os.path.join(TEST_PROCESSED_DATA_DIR, processed_filename)
            preprocess_file(timeframe, raw_path, processed_path)

    @classmethod
    def tearDownClass(cls):
        """
        Clean up the synthetic datasets after all tests have run.
        """
        shutil.rmtree(TEST_PROCESSED_DATA_DIR, ignore_errors=True)
        shutil.rmtree(TEST_RAW_DATA_DIR, ignore_errors=True)
        shutil.rmtree(TEST_SCALER_DIR, ignore_errors=True)
        shutil.rmtree(TEST_MODEL_DIR, ignore_errors=True)

    def test_load_and_preprocess(self):
        """
        Test the load_and_preprocess function to ensure it correctly loads data,
        handles missing values, and scales features for all timeframes.
        """
        timeframes = ['15m', '1h', '4h', '1d']
        for timeframe in timeframes:
            with self.subTest(timeframe=timeframe):
                processed_path = os.path.join(TEST_PROCESSED_DATA_DIR, f"processed_data_{timeframe}.csv.gz")
                feature_cols = FEATURE_COLUMNS[timeframe]
                target_cols = TARGET_COLUMNS[timeframe]
                scaler_path = os.path.join(TEST_SCALER_DIR, f"scaler_{timeframe}.save")

                df_scaled = load_and_preprocess(processed_path, feature_cols, target_cols, scaler_path)

                # Check for missing values
                self.assertFalse(df_scaled.isnull().values.any(), f"Missing values in {timeframe} data.")

                # Check if scaler was saved
                self.assertTrue(os.path.exists(scaler_path), f"Scaler not saved for {timeframe} data.")

                # Load scaler and verify scaling
                scaler = joblib.load(scaler_path)
                original_features = pd.read_csv(processed_path, compression='gzip')[feature_cols].astype(np.float32)
                scaled_features = scaler.transform(original_features)
                np.testing.assert_array_almost_equal(
                    df_scaled[feature_cols].values,
                    scaled_features,
                    decimal=5,
                    err_msg=f"Feature scaling incorrect for {timeframe} data."
                )

    def test_create_sequences(self):
        """
        Test the create_sequences function to ensure it correctly creates input sequences and targets for all timeframes.
        """
        timeframes = ['15m', '1h', '4h', '1d']
        for timeframe in timeframes:
            with self.subTest(timeframe=timeframe):
                processed_path = os.path.join(TEST_PROCESSED_DATA_DIR, f"processed_data_{timeframe}.csv.gz")
                feature_cols = FEATURE_COLUMNS[timeframe]
                target_cols = TARGET_COLUMNS[timeframe]
                scaler_path = os.path.join(TEST_SCALER_DIR, f"scaler_{timeframe}.save")

                df_scaled = load_and_preprocess(processed_path, feature_cols, target_cols, scaler_path)
                sequences, targets = create_sequences(df_scaled, SEQ_LENGTH, feature_cols, target_cols)

                expected_num_sequences = len(df_scaled) - SEQ_LENGTH
                actual_num_sequences = sequences.shape[0]

                self.assertEqual(
                    actual_num_sequences,
                    expected_num_sequences,
                    f"Sequences count mismatch for {timeframe} data."
                )

                # Check shapes
                self.assertEqual(
                    sequences.shape[1],
                    SEQ_LENGTH,
                    f"Sequence length incorrect for {timeframe} data."
                )
                self.assertEqual(
                    sequences.shape[2],
                    len(feature_cols),
                    f"Number of features incorrect for {timeframe} data."
                )

                for target in target_cols:
                    self.assertEqual(
                        len(targets[target]),
                        expected_num_sequences,
                        f"Target length mismatch for {timeframe} data."
                    )

    def test_financial_dataset(self):
        """
        Test the FinancialDataset class to ensure it correctly retrieves sequences and targets for all timeframes.
        """
        timeframes = ['15m', '1h', '4h', '1d']
        for timeframe in timeframes:
            with self.subTest(timeframe=timeframe):
                processed_path = os.path.join(TEST_PROCESSED_DATA_DIR, f"processed_data_{timeframe}.csv.gz")
                feature_cols = FEATURE_COLUMNS[timeframe]
                target_cols = TARGET_COLUMNS[timeframe]
                scaler_path = os.path.join(TEST_SCALER_DIR, f"scaler_{timeframe}.save")

                df_scaled = load_and_preprocess(processed_path, feature_cols, target_cols, scaler_path)
                sequences, targets = create_sequences(df_scaled, SEQ_LENGTH, feature_cols, target_cols)
                dataset = FinancialDataset(sequences, {target_cols[0]: targets[target_cols[0]]})  # Testing first target

                # Test dataset length
                self.assertEqual(
                    len(dataset),
                    len(sequences),
                    f"Dataset length mismatch for {timeframe} data."
                )

                # Test getting an item
                sample_seq, sample_target = dataset[0]
                np.testing.assert_array_almost_equal(
                    sample_seq.numpy(),
                    sequences[0],
                    decimal=5,
                    err_msg=f"Sequence data incorrect for {timeframe} data."
                )
                self.assertAlmostEqual(
                    sample_target.item(),
                    targets[target_cols[0]][0],
                    places=6,
                    msg=f"Target value incorrect for {timeframe} data."
                )

    def test_lnn_model_forward(self):
        """
        Test the LNNModel's forward pass to ensure it produces outputs of the correct shape for all timeframes.
        """
        timeframes = ['15m', '1h', '4h', '1d']
        for timeframe in timeframes:
            with self.subTest(timeframe=timeframe):
                input_size = len(FEATURE_COLUMNS[timeframe])
                hidden_size = 128
                num_layers = 2
                output_size = 1

                model = LNNModel(input_size, hidden_size, num_layers, output_size)
                model.eval()  # Set model to evaluation mode

                # Create a dummy input tensor with batch_size=2
                dummy_input = torch.randn(2, SEQ_LENGTH, input_size)
                with torch.no_grad():
                    output = model(dummy_input)

                # Check output shape
                self.assertEqual(
                    output.shape,
                    (2, output_size),
                    f"Model output shape incorrect for {timeframe} data."
                )

    def test_full_pipeline(self):
        """
        Test the full data loading, preprocessing, dataset creation, and model initialization pipeline for all timeframes.
        This ensures that all components work together without errors.
        """
        timeframes = ['15m', '1h', '4h', '1d']
        for timeframe in timeframes:
            with self.subTest(timeframe=timeframe):
                try:
                    processed_path = os.path.join(TEST_PROCESSED_DATA_DIR, f"processed_data_{timeframe}.csv.gz")
                    feature_cols = FEATURE_COLUMNS[timeframe]
                    target_cols = TARGET_COLUMNS[timeframe]
                    scaler_path = os.path.join(TEST_SCALER_DIR, f"scaler_{timeframe}.save")
                    model_save_path = os.path.join(TEST_MODEL_DIR, f"lnn_model_{timeframe}.pth")

                    # Load and preprocess
                    df_scaled = load_and_preprocess(processed_path, feature_cols, target_cols, scaler_path)

                    # Create sequences and targets
                    sequences, targets = create_sequences(df_scaled, SEQ_LENGTH, feature_cols, target_cols)

                    # Create Dataset and DataLoader
                    dataset = FinancialDataset(sequences, {target_cols[0]: targets[target_cols[0]]})  # Using first target
                    data_loader = DataLoader(
                        dataset,
                        batch_size=BATCH_SIZE,
                        shuffle=True,
                        num_workers=NUM_WORKERS,
                        pin_memory=True
                    )

                    # Initialize model
                    input_size = len(feature_cols)
                    hidden_size = 128
                    num_layers = 2
                    output_size = 1

                    model = LNNModel(input_size, hidden_size, num_layers, output_size)
                    criterion = torch.nn.MSELoss()
                    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

                    # Perform a single training step
                    model.train()
                    for sequences_batch, targets_batch in data_loader:
                        # Forward pass
                        outputs = model(sequences_batch)
                        loss = criterion(outputs, targets_batch)

                        # Backward pass and optimization
                        optimizer.zero_grad()
                        loss.backward()
                        optimizer.step()
                        break  # Only one batch for testing

                    # Check if loss is a finite number
                    self.assertTrue(np.isfinite(loss.item()), f"Loss is not finite for {timeframe} data.")

                except Exception as e:
                    self.fail(f"Full pipeline test failed for {timeframe} with exception: {e}")

    @patch('joblib.dump')
    def test_scaler_save(self, mock_joblib_dump):
        """
        Test that the scaler is saved correctly during preprocessing for all timeframes.
        """
        timeframes = ['15m', '1h', '4h', '1d']
        for timeframe in timeframes:
            with self.subTest(timeframe=timeframe):
                processed_path = os.path.join(TEST_PROCESSED_DATA_DIR, f"processed_data_{timeframe}.csv.gz")
                feature_cols = FEATURE_COLUMNS[timeframe]
                target_cols = TARGET_COLUMNS[timeframe]
                scaler_path = os.path.join(TEST_SCALER_DIR, f"scaler_{timeframe}.save")

                load_and_preprocess(processed_path, feature_cols, target_cols, scaler_path)

                # Check that joblib.dump was called to save the scaler
                mock_joblib_dump.assert_called_with(
                    unittest.mock.ANY,  # The scaler object
                    scaler_path
                )

    def test_model_save(self):
        """
        Test that the model can be saved without errors for all timeframes.
        """
        timeframes = ['15m', '1h', '4h', '1d']
        for timeframe in timeframes:
            with self.subTest(timeframe=timeframe):
                model = LNNModel(
                    input_size=len(FEATURE_COLUMNS[timeframe]),
                    hidden_size=128,
                    num_layers=2,
                    output_size=1
                )
                model_save_path = os.path.join(TEST_MODEL_DIR, f"lnn_model_{timeframe}.pth")
                try:
                    torch.save(model.state_dict(), model_save_path)
                    self.assertTrue(os.path.exists(model_save_path), f"Model not saved correctly for {timeframe} data.")
                except Exception as e:
                    self.fail(f"Model save test failed for {timeframe} with exception: {e}")

if __name__ == '__main__':
    unittest.main()


// File: data\raw\btc_15m_data_2018_to_2024-2024-10-10.csv
// Snippet:
          Open time     Open     High      Low    Close     Volume              Close time  Quote asset volume  Number of trades  Taker buy base asset volume  Taker buy quote asset volume  Ignore
2018-01-01 00:00:00 13715.65 13715.65 13400.01 13556.15 123.616013 2018-01-01 00:14:59.999        1.675545e+06              1572                    63.227133                  8.576108e+05       0
2018-01-01 00:15:00 13533.75 13550.87 13402.00 13521.12  98.136430 2018-01-01 00:29:59.999        1.321757e+06              1461                    47.686389                  6.422812e+05       0
2018-01-01 00:30:00 13500.00 13545.37 13450.00 13470.41  79.904037 2018-01-01 00:44:59.999        1.078825e+06              1000                    43.710406                  5.900347e+05       0
2018-01-01 00:45:00 13494.65 13690.87 13450.00 13529.01 141.699719 2018-01-01 00:59:59.999        1.917783e+06              1195                    73.897993                  1.000614e+06       0
2018-01-01 01:00:00 13528.99 13571.74 13402.28 13445.63  72.537533 2018-01-01 01:14:59.999        9.778198e+05               898                    34.257652                  4.618369e+05       0

// File: data\raw\btc_1d_data_2018_to_2024-2024-10-10.csv
// Snippet:
 Open time     Open     High      Low    Close       Volume              Close time  Quote asset volume  Number of trades  Taker buy base asset volume  Taker buy quote asset volume  Ignore
2018-01-01 13715.65 13818.55 12750.00 13380.00  8609.915844 2018-01-01 23:59:59.999        1.147997e+08            105595                  3961.938946                  5.280975e+07       0
2018-01-02 13382.16 15473.49 12890.02 14675.11 20078.092111 2018-01-02 23:59:59.999        2.797171e+08            177728                 11346.326739                  1.580801e+08       0
2018-01-03 14690.00 15307.56 14150.00 14919.51 15905.667639 2018-01-03 23:59:59.999        2.361169e+08            162787                  8994.953566                  1.335873e+08       0
2018-01-04 14919.51 15280.00 13918.04 15059.54 21329.649574 2018-01-04 23:59:59.999        3.127816e+08            170310                 12680.812951                  1.861168e+08       0
2018-01-05 15059.56 17176.24 14600.00 16960.39 23251.491125 2018-01-05 23:59:59.999        3.693220e+08            192969                 13346.622293                  2.118299e+08       0

// File: data\raw\btc_1h_data_2018_to_2024-2024-10-10.csv
// Snippet:
          Open time     Open     High      Low    Close     Volume              Close time  Quote asset volume  Number of trades  Taker buy base asset volume  Taker buy quote asset volume  Ignore
2018-01-01 00:00:00 13715.65 13715.65 13400.01 13529.01 443.356199 2018-01-01 00:59:59.999        5.993910e+06              5228                   228.521921                  3.090541e+06       0
2018-01-01 01:00:00 13528.99 13595.89 13155.38 13203.06 383.697006 2018-01-01 01:59:59.999        5.154522e+06              4534                   180.840403                  2.430449e+06       0
2018-01-01 02:00:00 13203.00 13418.43 13200.00 13330.18 429.064572 2018-01-01 02:59:59.999        5.710192e+06              4887                   192.237935                  2.558505e+06       0
2018-01-01 03:00:00 13330.26 13611.27 13290.00 13410.03 420.087030 2018-01-01 03:59:59.999        5.657448e+06              4789                   137.918407                  1.858041e+06       0
2018-01-01 04:00:00 13434.98 13623.29 13322.15 13601.01 340.807329 2018-01-01 04:59:59.999        4.588047e+06              4563                   172.957635                  2.328058e+06       0

// File: data\raw\btc_4h_data_2018_to_2024-2024-10-10.csv
// Snippet:
          Open time     Open     High      Low    Close      Volume              Close time  Quote asset volume  Number of trades  Taker buy base asset volume  Taker buy quote asset volume  Ignore
2018-01-01 00:00:00 13715.65 13715.65 13155.38 13410.03 1676.204807 2018-01-01 03:59:59.999        2.251607e+07             19438                   739.518666                  9.937537e+06       0
2018-01-01 04:00:00 13434.98 13818.55 13322.15 13570.35 1302.214836 2018-01-01 07:59:59.999        1.770254e+07             18061                   588.516222                  7.998796e+06       0
2018-01-01 08:00:00 13569.98 13735.24 13001.13 13220.56 1319.755931 2018-01-01 11:59:59.999        1.765121e+07             17200                   582.668293                  7.792109e+06       0
2018-01-01 12:00:00 13220.56 13330.00 12750.00 13247.00 1831.933153 2018-01-01 15:59:59.999        2.391259e+07             21357                   830.314747                  1.083304e+07       0
2018-01-01 16:00:00 13247.00 13290.65 12940.00 13240.37 1092.337234 2018-01-01 19:59:59.999        1.434887e+07             13178                   557.453779                  7.324320e+06       0

