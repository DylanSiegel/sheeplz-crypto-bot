**File Tree (Relevant Files Only)**
  .
    - meta_sac.py
    - test.py
// File: meta_sac.py
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import random
import unittest
import os
import logging
from dataclasses import dataclass
from typing import Optional, Tuple, List, Dict, Any
from torch.utils.tensorboard import SummaryWriter

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Detect device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

@dataclass
class MetaSACConfig:
    """Configuration for MetaSAC agent and networks."""
    state_dim: int
    action_dim: int
    hidden_dim: int = 64
    attention_dim: int = 32
    meta_input_dim: int = 5
    time_encoding_dim: int = 10
    num_mlp_layers: int = 3
    dropout_rate: float = 0.1
    lr: float = 1e-3
    meta_lr: float = 1e-4
    alpha: float = 0.2
    gamma: float = 0.99
    tau: float = 0.005
    batch_size: int = 64 # Changed to 64 to be consistent with test
    max_grad_norm: float = 1.0
    epsilon: float = 1e-10
    device: torch.device = device
    replay_buffer_capacity: int = 1000000


# ------------------------- Helper Classes -------------------------
class APELU(nn.Module):
    """Advanced Parametric Exponential Linear Unit activation."""
    def __init__(self, alpha_init: float = 0.01, beta_init: float = 1.0):
        super().__init__()
        self.alpha = nn.Parameter(torch.tensor(alpha_init))
        self.beta = nn.Parameter(torch.tensor(beta_init))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return torch.where(x >= 0, x, self.alpha * x * torch.exp(self.beta * x))


# ------------------------- Modern MLP -------------------------
class ModernMLP(nn.Module):
    """Modern Multi-Layer Perceptron with layer normalization and dropout."""
    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, 
                 num_layers: int = 3, dropout_rate: float = 0.1):
        super().__init__()
        self.layers = nn.ModuleList()
        self.norms = nn.ModuleList()
        
        for i in range(num_layers):
            in_features = input_dim if i == 0 else hidden_dim
            out_features = output_dim if i == num_layers - 1 else hidden_dim
            self.layers.append(nn.Linear(in_features, out_features))
            if i != num_layers - 1:
                self.norms.append(nn.LayerNorm(hidden_dim))
        
        self.activation = APELU()
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        for i, layer in enumerate(self.layers):
            x = layer(x)
            if i != len(self.layers) - 1:
                x = self.norms[i](x)
                x = self.activation(x)
                x = self.dropout(x)
        return x
        
# ------------------------- Time Encoding -------------------------
class SinusoidalTimeEncoding(nn.Module):
    """Sinusoidal positional encoding for time."""
    def __init__(self, time_encoding_dim: int):
        super().__init__()
        self.time_encoding_dim = time_encoding_dim
        self.frequencies = 10 ** (torch.arange(0, time_encoding_dim // 2) * (-2 / (time_encoding_dim // 2)))

    def forward(self, time_step: torch.Tensor) -> torch.Tensor:
        time_step = time_step.float()  # Cast to float
        scaled_time = time_step.unsqueeze(-1) * self.frequencies.to(time_step.device)
        sin_encodings = torch.sin(scaled_time)
        cos_encodings = torch.cos(scaled_time)

        if self.time_encoding_dim % 2 == 0:
          encoding = torch.cat([sin_encodings, cos_encodings], dim=-1)
        else:
          encoding = torch.cat([sin_encodings, cos_encodings, torch.zeros_like(cos_encodings[:, :1])], dim=-1)

        return encoding

# ------------------------- Adaptive Modulation MLP -------------------------
class TimeAwareBias(nn.Module):
    """Time-aware bias module for temporal adaptation."""
    def __init__(self, input_dim: int, time_encoding_dim: int = 10, hidden_dim: int = 20):
        super().__init__()
        self.time_embedding = nn.Linear(time_encoding_dim, hidden_dim)
        self.time_projection = nn.Linear(hidden_dim, input_dim)
        self.activation = APELU()

    def forward(self, time_encoding: torch.Tensor) -> torch.Tensor:
        x = self.time_embedding(time_encoding)
        x = self.activation(x)
        return self.time_projection(x)


class AdaptiveModulationMLP(nn.Module):
    """MLP with adaptive modulation based on time encoding."""
    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int,
                 num_layers: int = 3, dropout_rate: float = 0.1, time_encoding_dim: int = 10):
        super().__init__()
        self.layers = nn.ModuleList()
        self.norms = nn.ModuleList()
        self.modulations = nn.ParameterList()
        self.time_biases = nn.ModuleList()
        self.sinusoidal_encoding = SinusoidalTimeEncoding(time_encoding_dim)
        for i in range(num_layers):
            in_features = input_dim if i == 0 else hidden_dim
            out_features = output_dim if i == num_layers - 1 else hidden_dim
            self.layers.append(nn.Linear(in_features, out_features))
            if i != num_layers - 1:
                self.norms.append(nn.LayerNorm(hidden_dim))
                self.modulations.append(nn.Parameter(torch.ones(hidden_dim)))
                self.time_biases.append(TimeAwareBias(hidden_dim, time_encoding_dim))
        
        self.activation = APELU()
        self.dropout = nn.Dropout(dropout_rate)
        self.num_layers = num_layers

    def forward(self, x: torch.Tensor, time_step: torch.Tensor) -> torch.Tensor:
        time_encoding = self.sinusoidal_encoding(time_step)
        for i, layer in enumerate(self.layers):
            x = layer(x)
            if i != self.num_layers - 1:
                modulation_factor = self.modulations[i] + self.time_biases[i](time_encoding)
                x = x * modulation_factor
                x = self.norms[i](x)
                x = self.activation(x)
                x = self.dropout(x)
        return x


# ------------------------- Attention Mechanism -------------------------
class Attention(nn.Module):
    """Multi-head attention mechanism."""
    def __init__(self, input_dim: int, attention_dim: int):
        super().__init__()
        self.query_proj = nn.Linear(input_dim, attention_dim)
        self.key_proj = nn.Linear(input_dim, attention_dim)
        self.value_proj = nn.Linear(input_dim, attention_dim)
        self.out_proj = nn.Linear(attention_dim, input_dim)
        
        # Initialize parameters with Xavier/Glorot initialization
        nn.init.xavier_uniform_(self.query_proj.weight)
        nn.init.xavier_uniform_(self.key_proj.weight)
        nn.init.xavier_uniform_(self.value_proj.weight)
        nn.init.xavier_uniform_(self.out_proj.weight)

    def forward(self, input: torch.Tensor) -> torch.Tensor:
        # [batch, seq_len, input_dim] -> [batch, seq_len, attention_dim]
        query = self.query_proj(input)
        key = self.key_proj(input)
        value = self.value_proj(input)

        # Scaled dot-product attention
        attention_weights = F.softmax(
            torch.matmul(query, key.transpose(-2, -1)) / (query.size(-1) ** 0.5),
            dim=-1
        )
        
        # Apply attention and project back
        context_vector = torch.matmul(attention_weights, value)
        return self.out_proj(context_vector)


# ------------------------- SAC Actor Network -------------------------
class MetaSACActor(nn.Module):
    """Meta Soft Actor-Critic actor network with attention."""
    def __init__(self, config: MetaSACConfig):
        super().__init__()
        self.attention = Attention(config.state_dim, config.attention_dim)
        self.mlp = AdaptiveModulationMLP(
            config.state_dim, 
            config.hidden_dim,
            2 * config.action_dim,
            config.num_mlp_layers,
            config.dropout_rate,
            config.time_encoding_dim
        )
        self.action_dim = config.action_dim

    def forward(self, x: torch.Tensor, time_step: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        if x.dim() == 2:
            x = x.unsqueeze(1)
        x = self.attention(x)
        x = x.squeeze(1)
        x = self.mlp(x, time_step)
        
        mu, log_sigma = x[:, :self.action_dim], x[:, self.action_dim:]
        
        # Add checks for NaN values
        if torch.isnan(mu).any() or torch.isnan(log_sigma).any():
            logger.warning("NaN detected in actor output (mu or log_sigma)")
            mu = torch.nan_to_num(mu)
            log_sigma = torch.nan_to_num(log_sigma)
            
        return torch.tanh(mu), log_sigma


# ------------------------- SAC Critic Network -------------------------
class MetaSACCritic(nn.Module):
    """Meta Soft Actor-Critic critic network with attention."""
    def __init__(self, config: MetaSACConfig):
        super().__init__()
        combined_dim = config.state_dim + config.action_dim
        self.attention = Attention(combined_dim, config.attention_dim)
        self.mlp = AdaptiveModulationMLP(
            combined_dim,
            config.hidden_dim,
            1,
            config.num_mlp_layers,
            config.dropout_rate,
            config.time_encoding_dim
        )

    def forward(self, state: torch.Tensor, action: torch.Tensor, time_step: torch.Tensor) -> torch.Tensor:
        x = torch.cat([state, action], dim=-1)
        if x.dim() == 2:
            x = x.unsqueeze(1)
        x = self.attention(x)
        x = x.squeeze(1)
        return self.mlp(x, time_step)


# ------------------------- Meta Controller -------------------------
class MetaController(nn.Module):
    """Meta-controller for adaptive temperature parameter."""
    def __init__(self, config: MetaSACConfig):
        super().__init__()
        self.mlp = ModernMLP(
            config.meta_input_dim + 2, # Added 2 for mean and variance
            config.hidden_dim,
            1,
            config.num_mlp_layers,
            config.dropout_rate
        )

    def forward(self, x: torch.Tensor, reward_stats: torch.Tensor) -> torch.Tensor:
        x = torch.cat([x, reward_stats], dim=-1)
        return self.mlp(x)


# ------------------------- Replay Buffer -------------------------
class ReplayBuffer:
    """Experience replay buffer with prioritized sampling."""
    def __init__(self, capacity: int):
        self.capacity = capacity
        self.buffer: List[Tuple] = []
        self.position = 0
        self.priorities = np.zeros(capacity, dtype=np.float32)
        self.alpha = 0.6  # Priority exponent
        self.beta = 0.4   # Importance sampling exponent
        self.epsilon = 1e-6  # Small constant to prevent zero priorities

    def push(self, state: np.ndarray, action: np.ndarray, reward: float,
             next_state: np.ndarray, done: bool) -> None:
        if len(self.buffer) < self.capacity:
            self.buffer.append(None)
        self.buffer[self.position] = (state, action, reward, next_state, done)
        self.priorities[self.position] = max(self.priorities.max(), self.epsilon)
        self.position = (self.position + 1) % self.capacity

    def sample(self, batch_size: int) -> Tuple[List, np.ndarray, np.ndarray]:
        if len(self.buffer) < batch_size:
            raise ValueError("Not enough experiences in buffer")
            
        probs = self.priorities[:len(self.buffer)] ** self.alpha
        probs /= probs.sum()
        
        indices = np.random.choice(len(self.buffer), batch_size, p=probs)
        weights = (len(self.buffer) * probs[indices]) ** (-self.beta)
        weights /= weights.max()
        
        batch = [self.buffer[idx] for idx in indices]
        return batch, indices, weights

    def update_priorities(self, indices: np.ndarray, priorities: np.ndarray) -> None:
        self.priorities[indices] = priorities + self.epsilon

    def __len__(self) -> int:
        return len(self.buffer)


# ------------------------- Meta-SAC Agent -------------------------
class MetaSACAgent(nn.Module):
    """Meta Soft Actor-Critic agent."""
    def __init__(self, config: MetaSACConfig):
        super().__init__()
        self.config = config
        self.device = config.device
        
        # Initialize networks
        self.actor = MetaSACActor(config).to(device)
        self.critic1 = MetaSACCritic(config).to(device)
        self.critic2 = MetaSACCritic(config).to(device)
        self.critic_target1 = MetaSACCritic(config).to(device)
        self.critic_target2 = MetaSACCritic(config).to(device)
        
        # Copy weights
        self.critic_target1.load_state_dict(self.critic1.state_dict())
        self.critic_target2.load_state_dict(self.critic2.state_dict())
        
        # Initialize meta-controller
        self.meta_controller = MetaController(config).to(device)
        
        # Setup optimizers with gradient clipping
        self.setup_optimizers()
        
        # Initialize alpha parameter
        self.alpha = nn.Parameter(torch.tensor(config.alpha, device=device))
        self.target_entropy = -torch.prod(torch.tensor(config.action_dim).float()).to(device)
        
        # Setup tensorboard writer
        self.writer = SummaryWriter()
        self.train_steps = 0
        self.reward_history = []

    def setup_optimizers(self) -> None:
        """Initialize optimizers with learning rate schedulers."""
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.config.lr)
        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=self.config.lr)
        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=self.config.lr)
        self.meta_optimizer = optim.Adam(self.meta_controller.parameters(), lr=self.config.meta_lr)
        
        # Learning rate schedulers
        self.actor_scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.actor_optimizer, mode='min', factor=0.5, patience=5
        )
        self.critic1_scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.critic1_optimizer, mode='min', factor=0.5, patience=5
        )
        self.critic2_scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.critic2_optimizer, mode='min', factor=0.5, patience=5
        )

    def select_action(self, state: np.ndarray, time_step: int, eval: bool = False) -> np.ndarray:
        """
        Select an action given the current state and time encoding.
        
        Args:
            state: Current state observation
            time_step: Current time step 
            eval: If True, use deterministic action selection
            
        Returns:
            Selected action
        """
        # Input validation
        if state.shape[-1] != self.config.state_dim:
            raise ValueError(f"Expected state dimension {self.config.state_dim}, got {state.shape[-1]}")
           
        # Handle NaN inputs
        if np.isnan(state).any():
            logger.warning("NaN detected in state input, replacing with zeros")
            state = np.nan_to_num(state)
           
        self.actor.eval() if eval else self.actor.train()
        
        with torch.no_grad():
            # Convert to tensors and move to device
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            time_tensor = torch.tensor(time_step).unsqueeze(0).to(self.device)

            mu, log_sigma = self.actor(state_tensor, time_tensor)
            
            if eval:
                action = mu
            else:
                sigma = torch.exp(log_sigma)
                dist = torch.distributions.Normal(mu, sigma)
                action = torch.tanh(dist.rsample())

            # Ensure actions are within bounds
            action = torch.clamp(action, -1.0, 1.0)
            return action.cpu().numpy()[0]

    def compute_q_targets(self, rewards: torch.Tensor, next_states: torch.Tensor, 
                            time_steps: torch.Tensor, dones: torch.Tensor) -> torch.Tensor:
        """
        Compute Q-value targets for critic update.
        
        Args:
            rewards: Batch of rewards
            next_states: Batch of next states
            time_steps: Batch of time steps
            dones: Batch of done flags
            
        Returns:
            Q-value targets
        """
        with torch.no_grad():
            next_actions = []
            next_log_probs = []
            
            # Process each state individually to avoid memory issues
            for ns, ts in zip(next_states, time_steps):
                action = self.select_action(ns.cpu().numpy(), ts.item(), eval=False)
                next_actions.append(action)
                
                # Compute log prob for the selected action
                mu, log_sigma = self.actor(ns.unsqueeze(0), ts.unsqueeze(0))
                sigma = torch.exp(log_sigma)
                dist = torch.distributions.Normal(mu, sigma)
                z = dist.rsample()
                action_tensor = torch.tanh(z)
                log_prob = dist.log_prob(z) - torch.log(1 - action_tensor.pow(2) + self.config.epsilon)
                next_log_probs.append(log_prob.sum(-1))

            next_actions = torch.FloatTensor(np.stack(next_actions)).to(self.device)
            next_log_probs = torch.stack(next_log_probs).unsqueeze(-1).to(self.device) # added unsqueeze to make the shape [batch_size,1]

            # Compute target Q-values
            q_target1 = self.critic_target1(next_states, next_actions, time_steps)
            q_target2 = self.critic_target2(next_states, next_actions, time_steps)
            q_target = torch.min(q_target1, q_target2)
            
            # Final target computation
            return rewards + (1.0 - dones) * self.config.gamma * (q_target - self.alpha * next_log_probs)

    def update_critics(self, states: torch.Tensor, actions: torch.Tensor, 
                      time_steps: torch.Tensor, q_targets: torch.Tensor) -> Tuple[float, float]:
        """
        Update critic networks.
        
        Returns:
            Tuple of critic losses
        """
        # Compute current Q-values
        q_value1 = self.critic1(states, actions, time_steps)
        q_value2 = self.critic2(states, actions, time_steps)

        # Compute critic losses
        critic1_loss = F.mse_loss(q_value1, q_targets)
        critic2_loss = F.mse_loss(q_value2, q_targets)

        # Update critic 1
        self.critic1_optimizer.zero_grad()
        critic1_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.critic1.parameters(), self.config.max_grad_norm)
        self.critic1_optimizer.step()

        # Update critic 2
        self.critic2_optimizer.zero_grad()
        critic2_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.critic2.parameters(), self.config.max_grad_norm)
        self.critic2_optimizer.step()

        return critic1_loss.item(), critic2_loss.item()

    def update_actor(self, states: torch.Tensor, time_steps: torch.Tensor) -> float:
        """
        Update actor network.
        
        Returns:
            Actor loss value
        """
        mu, log_sigma = self.actor(states, time_steps)
        sigma = torch.exp(log_sigma)
        dist = torch.distributions.Normal(mu, sigma)
        z = dist.rsample()
        actions = torch.tanh(z)
        log_probs = dist.log_prob(z) - torch.log(1 - actions.pow(2) + self.config.epsilon)
        log_probs = log_probs.sum(-1, keepdim=True)

        q_values = torch.min(
            self.critic1(states, actions, time_steps),
            self.critic2(states, actions, time_steps)
        )
        
        actor_loss = (self.alpha * log_probs - q_values).mean()

        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.config.max_grad_norm)
        self.actor_optimizer.step()

        return actor_loss.item()

    def update_meta_controller(self, meta_input: torch.Tensor, log_probs: torch.Tensor, rewards: torch.Tensor) -> float:
        """
        Update meta-controller network.
        
        Returns:
            Meta-controller loss value
        """
        # compute reward statistics
        mean = torch.mean(rewards, dim=0, keepdim=True)
        variance = torch.var(rewards, dim=0, keepdim=True)
        reward_stats = torch.cat([mean, variance], dim=-1).to(self.device)
        
        # Repeat the reward_stats along the batch size dimension to match meta_input
        batch_size = meta_input.size(0)
        reward_stats = reward_stats.repeat(batch_size, 1)
        
        meta_output = self.meta_controller(meta_input, reward_stats)
        alpha_target_loss = -(meta_output - torch.log(self.alpha)) * (log_probs + self.target_entropy).detach().mean()

        self.meta_optimizer.zero_grad()
        alpha_target_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.meta_controller.parameters(), self.config.max_grad_norm)
        self.meta_optimizer.step()

        return alpha_target_loss.item()

    def soft_update(self, target_network: nn.Module, source_network: nn.Module) -> None:
        """Soft update of target network parameters."""
        try:
            for target_param, param in zip(target_network.parameters(), source_network.parameters()):
                target_param.data.copy_(
                    target_param.data * (1.0 - self.config.tau) + param.data * self.config.tau
                )
        except Exception as e:
            logger.error(f"Failed to update target network: {str(e)}")
            raise

    def update_params(self, replay_buffer: ReplayBuffer, meta_input: np.ndarray, 
                     time_memory: List[int]) -> Dict[str, float]:
        """
        Update all network parameters.
        
        Returns:
            Dictionary containing all loss values
        """
        if len(replay_buffer) < self.config.batch_size:
            return {}

        # Sample from replay buffer
        try:
            batch, indices, weights = replay_buffer.sample(self.config.batch_size)
        except ValueError as e:
            logger.error(f"Failed to sample from replay buffer: {str(e)}")
            return {}

        # Prepare batch data
        states = torch.FloatTensor(np.stack([item[0] for item in batch])).to(self.device)
        actions = torch.FloatTensor(np.stack([item[1] for item in batch])).to(self.device)
        rewards = torch.FloatTensor(np.stack([item[2] for item in batch])).unsqueeze(1).to(self.device)
        next_states = torch.FloatTensor(np.stack([item[3] for item in batch])).to(self.device)
        dones = torch.FloatTensor(np.stack([item[4] for item in batch])).unsqueeze(1).to(self.device)
        meta_input = torch.FloatTensor(meta_input).unsqueeze(0).to(self.device)
        time_steps = torch.tensor(time_memory).to(self.device)
        
        # Compute targets for critics
        q_targets = self.compute_q_targets(rewards, next_states, time_steps, dones)

        # Update critics
        critic1_loss, critic2_loss = self.update_critics(states, actions, time_steps, q_targets)
        
        # Update actor
        actor_loss = self.update_actor(states, time_steps)

        # Update meta-controller and alpha
        mu, log_sigma = self.actor(states, time_steps)
        sigma = torch.exp(log_sigma)
        dist = torch.distributions.Normal(mu, sigma)
        z = dist.rsample()
        actions = torch.tanh(z)
        log_probs = dist.log_prob(z) - torch.log(1 - actions.pow(2) + self.config.epsilon)
        log_probs = log_probs.sum(-1, keepdim=True)
        meta_loss = self.update_meta_controller(meta_input, log_probs, rewards)  # Send the actual rewards

        # Update target networks
        self.soft_update(self.critic_target1, self.critic1)
        self.soft_update(self.critic_target2, self.critic2)

        # Update learning rates
        self.actor_scheduler.step(actor_loss)
        self.critic1_scheduler.step(critic1_loss)
        self.critic2_scheduler.step(critic2_loss)
            
        # Log to tensorboard
        self.train_steps += 1
        self.writer.add_scalar('Loss/actor', actor_loss, self.train_steps)
        self.writer.add_scalar('Loss/critic1', critic1_loss, self.train_steps)
        self.writer.add_scalar('Loss/critic2', critic2_loss, self.train_steps)
        self.writer.add_scalar('Loss/meta', meta_loss, self.train_steps)
        self.writer.add_scalar('Parameters/alpha', self.alpha.item(), self.train_steps)

        return {
            'actor_loss': actor_loss,
            'critic1_loss': critic1_loss,
            'critic2_loss': critic2_loss,
            'meta_loss': meta_loss,
            'alpha': self.alpha.item()
        }

    def save(self, path: str) -> None:
        """Save model state."""
        try:
            torch.save({
                'actor_state_dict': self.actor.state_dict(),
                'critic1_state_dict': self.critic1.state_dict(),
                'critic2_state_dict': self.critic2.state_dict(),
                'critic_target1_state_dict': self.critic_target1.state_dict(),
                'critic_target2_state_dict': self.critic_target2.state_dict(),
                'meta_controller_state_dict': self.meta_controller.state_dict(),
                'alpha': self.alpha.detach().cpu().numpy(),
                'config': self.config,
                'train_steps': self.train_steps
            }, path)
            logger.info(f"Model saved successfully to {path}")
        except Exception as e:
            logger.error(f"Failed to save model: {str(e)}")
            raise

    def load(self, path: str) -> None:
        """Load model state."""
        try:
            checkpoint = torch.load(path, map_location=self.device)
            self.actor.load_state_dict(checkpoint['actor_state_dict'])
            self.critic1.load_state_dict(checkpoint['critic1_state_dict'])
            self.critic2.load_state_dict(checkpoint['critic2_state_dict'])
            self.critic_target1.load_state_dict(checkpoint['critic_target1_state_dict'])
            self.critic_target2.load_state_dict(checkpoint['critic_target2_state_dict'])
            self.meta_controller.load_state_dict(checkpoint['meta_controller_state_dict'])
            self.alpha.data.copy_(torch.tensor(checkpoint['alpha'], device=self.device))
            self.train_steps = checkpoint['train_steps']
            logger.info(f"Model loaded successfully from {path}")
        except Exception as e:
            logger.error(f"Failed to load model: {str(e)}")
            raise

// File: test.py
import unittest
import numpy as np
import torch
import os
from typing import Tuple, List, Dict
import logging
from collections import deque
import matplotlib.pyplot as plt
from datetime import datetime

# Import your SAC implementation
from meta_sac import MetaSACAgent, MetaSACConfig, ReplayBuffer, SinusoidalTimeEncoding

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CustomEnvironment:
    """
    Custom environment for testing SAC agent.
    Simulates a simple control task where the goal is to move a point mass to a target.
    """
    def __init__(self, state_dim: int = 10, action_dim: int = 2):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.max_steps = 200
        self.target_state = np.zeros(state_dim)  # Target is at origin
        self.step_count = 0
        self.state = None
        self.action_scale = 2.0  # Scale factor for actions
        
        # Dynamic parameters
        self.inertia = 0.8  # How much previous state affects next state
        self.action_influence = 0.2  # How much actions affect the state
        self.noise_scale = 0.01  # Environmental noise

    def reset(self) -> np.ndarray:
        """Reset environment to initial state."""
        # Initialize state randomly but not too far from origin
        self.state = np.random.randn(self.state_dim)
        self.step_count = 0
        return self.state.copy()

    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, Dict]:
        """
        Take a step in environment given an action.
        
        Args:
            action: Array of action values in [-1, 1]
            
        Returns:
            (next_state, reward, done, info)
        """
        # Clip actions to valid range
        action = np.clip(action, -1, 1) * self.action_scale
        
        # Update state based on action and dynamics
        action_effect = np.zeros(self.state_dim)
        action_effect[:self.action_dim] = action
        
        self.state = (
            self.inertia * self.state + 
            self.action_influence * action_effect +
            self.noise_scale * np.random.randn(self.state_dim)
        )
        
        # Calculate reward based on distance to target and action magnitude
        distance_to_target = np.linalg.norm(self.state - self.target_state)
        action_penalty = 0.1 * np.linalg.norm(action)  # Small penalty for large actions
        reward = -distance_to_target - action_penalty
        
        # Update step count and check if episode is done
        self.step_count += 1
        done = self.step_count >= self.max_steps
        
        # Add some success and failure conditions
        if distance_to_target < 0.1:  # Success condition
            reward += 10
            done = True
        elif distance_to_target > 20:  # Failure condition
            reward -= 10
            done = True
            
        return self.state.copy(), reward, done, {
            'distance': distance_to_target,
            'step': self.step_count
        }

class TestSAC(unittest.TestCase):
    """Test suite for SAC agent."""
    
    def setUp(self):
        """Setup test environment and agent."""
        self.config = MetaSACConfig(
            state_dim=10,
            action_dim=2,
            hidden_dim=64,
            attention_dim=32,
            meta_input_dim=5,
            batch_size=64,
            lr=3e-4,
            meta_lr=1e-4,
            time_encoding_dim = 20
        )
        self.env = CustomEnvironment(self.config.state_dim, self.config.action_dim)
        self.agent = MetaSACAgent(self.config)
        self.replay_buffer = ReplayBuffer(100000)
        self.time_encoding_dim = 20
        
        # Training parameters
        self.num_eval_episodes = 5
        self.num_train_episodes = 50
        self.eval_interval = 10
        
        # Create directory for saving results
        self.results_dir = 'test_results'
        os.makedirs(self.results_dir, exist_ok=True)
        
    def generate_time_encoding(self) -> int:
        """Generate time encoding vector."""
        return np.random.randint(0, 1000)
    
    def evaluate_policy(self, num_episodes: int = 5) -> Tuple[float, List[float]]:
        """
        Evaluate current policy.
        
        Returns:
            Tuple of (mean reward, list of episode rewards)
        """
        episode_rewards = []
        
        for episode in range(num_episodes):
            state = self.env.reset()
            episode_reward = 0
            done = False
            time_step = 0
            while not done:
                action = self.agent.select_action(state, time_step, eval=True)
                next_state, reward, done, _ = self.env.step(action)
                episode_reward += reward
                state = next_state
                time_step+=1
            
            episode_rewards.append(episode_reward)
        
        return np.mean(episode_rewards), episode_rewards

    def test_basic_functionality(self):
        """Test basic environment and agent functionality."""
        # Test environment
        state = self.env.reset()
        self.assertEqual(state.shape, (self.config.state_dim,), "Environment state shape is incorrect")
        
        # Test agent action selection
        time_step = self.generate_time_encoding()
        action = self.agent.select_action(state, time_step)
        self.assertEqual(action.shape, (self.config.action_dim,), "Action shape is incorrect")
        self.assertTrue(np.all(action >= -1) and np.all(action <= 1), "Action values out of range")
    
    def test_time_encoding(self):
        """Test time encoding mechanism."""
        time_encoding_dim = self.config.time_encoding_dim
        time_encoding = SinusoidalTimeEncoding(time_encoding_dim)
        
        # Test for a specific time step
        time_step = torch.tensor([5])
        encoding = time_encoding(time_step)
        
        self.assertEqual(encoding.shape, torch.Size([1, time_encoding_dim]), "Incorrect time encoding shape")
        self.assertTrue(torch.all(encoding >= -1.0) and torch.all(encoding <= 1.0), "Time encoding values are out of range")

    def test_learning(self):
        """Test if agent can learn and improve."""
        initial_mean, initial_rewards = self.evaluate_policy()
        logger.info(f"Initial performance: {initial_mean:.2f}")
        
        eval_rewards = []
        training_rewards = []
        distances = []
        alphas = []
        critic1_losses = []
        critic2_losses = []
        actor_losses = []

        # Training loop
        for episode in range(self.num_train_episodes):
            state = self.env.reset()
            episode_reward = 0
            episode_distances = []
            done = False
            meta_input = np.random.randn(self.config.meta_input_dim)
            time_memory = []
            time_step = 0
            
            while not done:
                # Get action from agent
                time_memory.append(time_step)
                action = self.agent.select_action(state, time_step)
                
                # Take step in environment
                next_state, reward, done, info = self.env.step(action)
                episode_reward += reward
                episode_distances.append(info['distance'])
                
                # Store transition
                self.replay_buffer.push(state, action, reward, next_state, done)
                
                # Update agent if enough samples
                if len(self.replay_buffer) >= self.config.batch_size:
                    update_info = self.agent.update_params(
                        self.replay_buffer,
                        meta_input,
                        time_memory[-self.config.batch_size:]
                    )
                    
                    # Verify no NaN values in losses
                    for key, value in update_info.items():
                        self.assertFalse(np.isnan(value), f"NaN detected in {key}")
                    
                    # Store losses and alpha values
                    if "alpha" in update_info:
                      alphas.append(update_info["alpha"])
                    if "critic1_loss" in update_info:
                        critic1_losses.append(update_info["critic1_loss"])
                    if "critic2_loss" in update_info:
                        critic2_losses.append(update_info["critic2_loss"])
                    if "actor_loss" in update_info:
                         actor_losses.append(update_info["actor_loss"])
                
                state = next_state
                time_step += 1

            training_rewards.append(episode_reward)
            distances.append(np.mean(episode_distances))
            
            # Evaluate policy periodically
            if episode % self.eval_interval == 0:
                mean_reward, _ = self.evaluate_policy()
                eval_rewards.append(mean_reward)
                logger.info(f"Episode {episode}, Eval reward: {mean_reward:.2f}")
        
        final_mean, final_rewards = self.evaluate_policy()
        logger.info(f"Final performance: {final_mean:.2f}")
        
        # Plot results
        self._plot_results(training_rewards, eval_rewards, distances, alphas, critic1_losses, critic2_losses, actor_losses)
        
        # Assert improvement
        self.assertGreater(final_mean, initial_mean, 
                          "Agent failed to improve performance")
        
        #Assert that the alpha is decreasing
        if len(alphas) > 10:
            self.assertLess(np.mean(alphas[-10:]), np.mean(alphas[:10]), "Alpha not decreasing")

    def test_reproducibility(self):
        """Test if training is reproducible with same seed."""
        torch.manual_seed(42)
        np.random.seed(42)
        
        # Get results from two identical training runs
        results = []
        for _ in range(2):
            torch.manual_seed(42)
            np.random.seed(42)
            
            agent = MetaSACAgent(self.config)
            env = CustomEnvironment(self.config.state_dim, self.config.action_dim)
            
            state = env.reset()
            actions = []
            time_step = 0

            for _ in range(10):
                action = agent.select_action(state, time_step, eval=True)
                actions.append(action)
                state, _, done, _ = env.step(action)
                time_step+=1
                if done:
                    break
            
            results.append(actions)
        
        # Compare results
        for a1, a2 in zip(results[0], results[1]):
            np.testing.assert_array_almost_equal(a1, a2, err_msg="Actions are not reproducible")

    def test_deterministic_eval(self):
        """Test action selection is deterministic in eval mode."""
        state = self.env.reset()
        time_step = self.generate_time_encoding()
        
        actions = []
        for _ in range(10):
            action = self.agent.select_action(state, time_step, eval=True)
            actions.append(action)
        
        # Compare all actions to first action
        for idx, action in enumerate(actions[1:]):
            np.testing.assert_array_almost_equal(action, actions[0], err_msg=f"Deterministic eval failed on action {idx +1}")

    def test_save_load(self):
        """Test model saving and loading."""
        # Train agent briefly
        torch.manual_seed(42)
        np.random.seed(42)
        state = self.env.reset()
        initial_actions = []
        time_step = 0
        
        for _ in range(10):
            action = self.agent.select_action(state, time_step, eval=True)
            initial_actions.append(action)
            state, _, done, _ = self.env.step(action)
            time_step +=1
            if done:
                state = self.env.reset()
                time_step = 0
        
        # Save model
        save_path = os.path.join(self.results_dir, "test_model.pth")
        self.agent.save(save_path)
        
        # Create new agent and load saved model
        torch.manual_seed(42)
        np.random.seed(42)
        loaded_agent = MetaSACAgent(self.config)
        loaded_agent.load(save_path)
        
        # Compare actions
        state = self.env.reset()
        time_step = 0

        for initial_action in initial_actions:
            loaded_action = loaded_agent.select_action(state, time_step, eval=True)
            np.testing.assert_array_almost_equal(initial_action, loaded_action, err_msg="Action is different after loading")
            state, _, done, _ = self.env.step(loaded_action)
            time_step += 1
            if done:
                state = self.env.reset()
                time_step = 0

    def _plot_results(self, training_rewards: List[float], eval_rewards: List[float], 
                     distances: List[float], alphas: List[float], critic1_losses: List[float],
                     critic2_losses: List[float], actor_losses: List[float]) -> None:
        """Plot and save training results."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Plot training rewards
        plt.figure(figsize=(10, 5))
        plt.plot(training_rewards)
        plt.title('Training Rewards')
        plt.xlabel('Episode')
        plt.ylabel('Reward')
        plt.savefig(os.path.join(self.results_dir, f'training_rewards_{timestamp}.png'))
        plt.close()
        
        # Plot evaluation rewards
        plt.figure(figsize=(10, 5))
        plt.plot(range(0, self.num_train_episodes, self.eval_interval), eval_rewards)
        plt.title('Evaluation Rewards')
        plt.xlabel('Episode')
        plt.ylabel('Reward')
        plt.savefig(os.path.join(self.results_dir, f'eval_rewards_{timestamp}.png'))
        plt.close()
        
        # Plot distances to target
        plt.figure(figsize=(10, 5))
        plt.plot(distances)
        plt.title('Average Distance to Target')
        plt.xlabel('Episode')
        plt.ylabel('Distance')
        plt.savefig(os.path.join(self.results_dir, f'distances_{timestamp}.png'))
        plt.close()

        # Plot alpha
        plt.figure(figsize=(10,5))
        plt.plot(alphas)
        plt.title('Alpha over training')
        plt.xlabel("Training steps")
        plt.ylabel("Alpha")
        plt.savefig(os.path.join(self.results_dir, f"alpha_{timestamp}.png"))
        plt.close()

        # Plot critic 1 losses
        plt.figure(figsize=(10, 5))
        plt.plot(critic1_losses)
        plt.title('Critic 1 Loss')
        plt.xlabel('Training Step')
        plt.ylabel('Loss')
        plt.savefig(os.path.join(self.results_dir, f'critic1_loss_{timestamp}.png'))
        plt.close()

        # Plot critic 2 losses
        plt.figure(figsize=(10, 5))
        plt.plot(critic2_losses)
        plt.title('Critic 2 Loss')
        plt.xlabel('Training Step')
        plt.ylabel('Loss')
        plt.savefig(os.path.join(self.results_dir, f'critic2_loss_{timestamp}.png'))
        plt.close()

        # Plot actor losses
        plt.figure(figsize=(10, 5))
        plt.plot(actor_losses)
        plt.title('Actor Loss')
        plt.xlabel('Training Step')
        plt.ylabel('Loss')
        plt.savefig(os.path.join(self.results_dir, f'actor_loss_{timestamp}.png'))
        plt.close()

    def tearDown(self):
        """Cleanup after tests."""
        # Close any open plots
        plt.close('all')

if __name__ == '__main__':
    unittest.main()

