// File: clean.py
import subprocess
import os

def clean_project(project_dir):
  """
  Cleans a Python project by removing unused imports and variables using Autoflake.

  Args:
    project_dir: The path to the root directory of the Python project.
  """

  exclude_paths = [
      "venv",
      "migrations",
      ".mypy_cache",
      "__pycache__",
      # Add any other directories or file names to exclude
  ]

  for root, _, files in os.walk(project_dir):
      for file in files:
          if file.endswith(".py") and not any(exclude in root for exclude in exclude_paths):
              file_path = os.path.join(root, file)
              try:
                  subprocess.run(
                      [
                          "autoflake",
                          "--in-place",
                          "--remove-unused-variables",
                          "--remove-all-unused-imports",
                          file_path,
                      ],
                      check=True,
                  )
                  print(f"Cleaned: {file_path}")
              except subprocess.CalledProcessError as e:
                  print(f"Error cleaning {file_path}: {e}")

if __name__ == "__main__":
  project_directory = r"C:\Users\dylan\Desktop\sheeplz-crypto-bot"  # Your project directory
  clean_project(project_directory)

// File: config.py
import yaml
import logging
from typing import List, Dict, Any
from pydantic import BaseModel, field_validator, PositiveInt

class Config(BaseModel):
    """Configuration for DataIngestion and indicators."""

    symbol: str = "BTC_USDT"
    interval: str = "Min1"
    timeframes: List[str] = ["1m", "5m", "15m", "1h", "4h"]
    indicators: List[str] = ["price", "volume", "rsi", "macd", "fibonacci"]

    # GMN Parameters
    max_history_length: int = 1000

    # LNN Parameters
    lnn_model_path: str = "models/lnn/lnn_model.pth"
    lnn_hidden_size: int = 64
    lnn_training_epochs: int = 10
    training_history_length: int = 500
    lnn_learning_rate: float = 0.001

    # Agent Parameters
    threshold_buy: float = 0.7
    threshold_sell: float = 0.3

    # Risk Management
    risk_parameters: Dict[str, Any] = {}

    # Trade Execution
    trade_parameters: Dict[str, Any] = {}

    # System
    agent_loop_delay: PositiveInt = 1
    reconnect_delay: PositiveInt = 5
    log_level: str = "INFO"

    # Device
    device: str = "cpu"

    # Private Channels
    private_channels: List[str] = []

    @field_validator('timeframes', mode='before')
    def validate_timeframes(cls, v):
        if isinstance(v, str):
            return [tf.strip() for tf in v.split(',')]
        elif isinstance(v, list):
            return v
        else:
            raise ValueError("timeframes must be a comma-separated string or a list")

    def load_from_yaml(self, config_path: str):
        """Loads configuration from a YAML file."""
        try:
            with open(config_path, "r") as f:
                config_data: Dict[str, Any] = yaml.safe_load(f)
                for key, value in config_data.items():
                    if hasattr(self, key):
                        setattr(self, key, value)
        except FileNotFoundError:
            raise FileNotFoundError(f"Config file not found: {config_path}")
        except yaml.YAMLError as e:
            raise ValueError(f"Error parsing YAML config file: {e}")

// File: crypto_trading_bot.py
# crypto_trading_bot.py
import asyncio
import logging
import tracemalloc
import os
from typing import Tuple, Optional, Dict, List, Any
import torch
import numpy as np
import aiohttp
import joblib
from sklearn.preprocessing import MinMaxScaler

from data.mexc_data_ingestion import DataIngestion, Config as DataIngestionConfig
from models.agents.agent import TradingAgent
from models.gmn.gmn import CryptoGMN
from models.lnn.lnn_model import LiquidNeuralNetwork
from models.utils.risk_management import RiskManager
from models.utils.config import Config

# Load configuration
config = Config("configs/config.yaml")

# Configure logging
log_file_path = os.path.join(os.getcwd(), "logs", "trading_bot.log")
os.makedirs(os.path.dirname(log_file_path), exist_ok=True)  # Ensure log directory exists

logging.basicConfig(
    level=getattr(logging, config.log_level.upper(), logging.INFO),
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file_path),
        logging.StreamHandler()
    ]
)

# Initialize scaler path (should be defined before use in functions)
scaler_path = "models/lnn/scaler.joblib"

async def main():
    """Main function to run the crypto trading bot."""
    try:
        timeframes = config.timeframes
        indicators = config.indicators
        max_history_length = config.max_history_length

        # Initialize GMN
        gmn = CryptoGMN(timeframes, indicators, max_history_length=max_history_length)

        # Initialize Data Ingestion Config and DataIngestion
        data_ingestion_config = DataIngestionConfig(
            symbol=config.symbol,
            timeframes=config.timeframes,
            private_channels=config.private_channels,
            reconnect_delay=config.reconnect_delay,
            # ... add other data ingestion config parameters as needed
        )
        data_ingestion = DataIngestion(gmn, data_ingestion_config)

        # Initialize Risk Manager
        risk_manager = RiskManager(config.risk_parameters)

        # Load or initialize and fit scaler  (moved here)
        scaler = await load_or_fit_scaler(scaler_path, gmn, config.training_history_length) # Modified

        # Initialize or train LNN Model
        lnn_model_path = config.lnn_model_path
        model = await load_or_train_lnn(gmn, lnn_model_path, config, scaler) # Function handles both loading and training

        if model is None:  # Check if model loading/training failed
            logging.error("Failed to load or train LNN model. Exiting.")
            await shutdown(gmn=gmn, data_ingestion=data_ingestion, risk_manager=risk_manager, scaler=scaler)
            return

        # Initialize TradingAgent *after* model is successfully loaded or trained
        agent = TradingAgent(timeframes, indicators, model, config, risk_manager, scaler)

        tracemalloc.start()  # Start tracemalloc after objects are initialized
        tasks = [
            asyncio.create_task(data_ingestion.connect()),
            asyncio.create_task(agent_loop(agent, gmn))
        ]


        try:
            await asyncio.gather(*tasks)

        except asyncio.CancelledError:
            logging.info("Main tasks cancelled.")
        except Exception as e:
            logging.exception(f"Unhandled exception in main loop: {e}") # Log traceback
        finally:
            # Memory profiling
            snapshot = tracemalloc.take_snapshot()
            top_stats = snapshot.statistics('lineno')
            logging.info("Top 10 memory allocations:")
            for stat in top_stats[:10]:
                logging.info(stat)

            tracemalloc.stop()
            await shutdown(gmn=gmn, data_ingestion=data_ingestion, risk_manager=risk_manager, scaler=scaler, agent=agent)

    except KeyboardInterrupt:
        logging.info("Trading bot stopped by user.")
        # If tasks are running, cancel them here before shutdown.


async def agent_loop(agent: TradingAgent, gmn: CryptoGMN):
    """Agent loop."""
    while True:
        try: # Handle exceptions within the agent loop
            market_data = gmn.get_all_data() # More efficient

            if not all(market_data.values()): # Check if all timeframes have data
                await asyncio.sleep(config.agent_loop_delay)
                continue

            await agent.make_decision(market_data)

        except Exception as e:
            logging.error(f"Error in agent loop: {e}", exc_info=True)  # Include traceback
        await asyncio.sleep(config.agent_loop_delay)





async def load_or_train_lnn(gmn, model_path, config, scaler):
    """Loads or trains the LNN model."""
    try:
        model = LiquidNeuralNetwork(len(config.timeframes) * len(config.indicators), config.lnn_hidden_size, 1)
        model.load_state_dict(torch.load(model_path, map_location=config.device)) # Load on specified device
        model.to(config.device) # Move model to device after loading
        model.eval()
        logging.info("Loaded pre-trained LNN model.")
        return model
    except FileNotFoundError:
        logging.info("Pre-trained LNN model not found. Training a new model...")
        return await train_and_save_lnn(gmn, model_path, config, scaler)
    except Exception as e:
        logging.error(f"Error loading LNN model: {e}", exc_info=True)
        return None


async def train_and_save_lnn(gmn, model_path, config, scaler):
    """Trains and saves LNN model."""

    try:
        X_train, y_train = await prepare_lnn_training_data(gmn, config.training_history_length, scaler) # Pass scaler here

        if X_train is None or y_train is None:
            return None # Return None on failure


        # Convert to tensors
        X_train = torch.tensor(X_train, dtype=torch.float32, device=config.device) # Create on correct device
        y_train = torch.tensor(y_train, dtype=torch.float32, device=config.device) # Use config.device here

        input_size = X_train.shape[1]
        model = LiquidNeuralNetwork(input_size, config.lnn_hidden_size, 1).to(config.device)  # Send to device
        # ... (rest of training loop, including criterion, optimizer)


        # Save the trained model
        torch.save(model.state_dict(), model_path)  # Use state_dict
        model.eval()
        logging.info(f"LNN model trained and saved to {model_path}")

        return model

    except Exception as e:
        logging.error(f"Error during LNN training: {e}", exc_info=True) # Log traceback
        return None



async def prepare_lnn_training_data(gmn: CryptoGMN, history_length: int, scaler: MinMaxScaler) -> Tuple[np.ndarray, np.ndarray]: # Changed
    """Prepares training data."""
    try:
        market_data = gmn.get_all_data()
        # ... (data preparation logic, same as before)


        # Fit and Transform with the scaler
        X_scaled = scaler.fit_transform(X) # Fit the scaler here
        # ... (rest of the method)

    except Exception as e:
        logging.error(f"Error preparing LNN training data: {e}", exc_info=True)  # More detailed
        return None, None


async def load_or_fit_scaler(scaler_path: str, gmn: CryptoGMN, history_length: int) -> MinMaxScaler:  # Changed
    """Loads scaler or fits a new one."""
    try:
        scaler = joblib.load(scaler_path)
        logging.info(f"Loaded scaler from {scaler_path}")
    except FileNotFoundError:
        logging.info("Scaler file not found. Fitting a new scaler...")
        # Prepare data for scaler fitting
        X, _ = await prepare_lnn_training_data(gmn, history_length, None)  # Use unscaled data
        if X is None:
            raise ValueError("Failed to prepare data for scaler fitting.")

        scaler = MinMaxScaler()
        scaler.fit(X)
        joblib.dump(scaler, scaler_path)
        logging.info(f"New scaler fitted and saved to {scaler_path}")

    return scaler


async def shutdown(gmn: CryptoGMN = None, data_ingestion: DataIngestion = None, risk_manager: RiskManager = None, scaler: MinMaxScaler = None, agent: Optional[TradingAgent] = None):
    # ... (same as before)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logging.info("Trading bot stopped by user.")

// File: main.py
import asyncio
import logging
import tracemalloc
import os
from typing import Optional, List

import torch
import numpy as np
import joblib
from sklearn.preprocessing import MinMaxScaler

from data.ingestion.mexc_data_ingestion import DataIngestion
from data.config import Config as DataIngestionConfig
from models.agents.agent import TradingAgent
from models.gmn.gmn import CryptoGMN
from models.lnn.lnn_model import LiquidNeuralNetwork
from models.utils.risk_management import RiskManager
from models.utils.config import Config

config = Config("configs/config.yaml")

log_file_path = os.path.join(os.getcwd(), "logs", "trading_bot.log")
os.makedirs(os.path.dirname(log_file_path), exist_ok=True)

logging.basicConfig(
    level=getattr(logging, config.log_level.upper(), logging.INFO),
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file_path),
        logging.StreamHandler()
    ]
)

scaler_path = "models/lnn/scaler.joblib"

bot_tasks: List[asyncio.Task] = []

async def main():
    global bot_tasks

    try:
        timeframes = config.timeframes
        max_history_length = config.max_history_length

        gmn = CryptoGMN(timeframes, config.indicators, max_history_length=max_history_length, config=config)

        data_ingestion_config = DataIngestionConfig(
            symbol=config.symbol,
            timeframes=config.timeframes,
            private_channels=config.private_channels,
            reconnect_delay=config.reconnect_delay,
            max_retry_attempts=config.max_retry_attempts
        )
        data_ingestion = DataIngestion(gmn, data_ingestion_config)

        risk_manager = RiskManager(config.risk_parameters)
        scaler = await load_scaler(scaler_path)  # No need to pass gmn and history_length
        model = await load_or_train_lnn(gmn, config.lnn_model_path, config, scaler)

        if model is None:
            logging.error("Failed to load or train LNN model. Exiting.")
            await shutdown()  # Call simplified shutdown
            return

        agent = TradingAgent(timeframes, config.indicators, model, config, risk_manager, scaler)
        tracemalloc.start()
        bot_tasks = [
            asyncio.create_task(data_ingestion.connect()),
            asyncio.create_task(agent_loop(agent, gmn))
        ]

        try:
            await asyncio.gather(*bot_tasks)
        except asyncio.CancelledError:
            logging.info("Main tasks cancelled.")
        except Exception as e:
            logging.exception(f"Unhandled exception in main loop: {e}")
        finally:
            snapshot = tracemalloc.take_snapshot()
            top_stats = snapshot.statistics('lineno')
            logging.info("Top 10 memory allocations:")
            for stat in top_stats[:10]:
                logging.info(stat)

            tracemalloc.stop()
            await shutdown()

    except KeyboardInterrupt:
        logging.info("Trading bot stopped by user.")


async def agent_loop(agent: TradingAgent, gmn: CryptoGMN):
    while True:
        try:
            market_data = gmn.get_all_data()
            if not all(market_data.values()):
                await asyncio.sleep(config.agent_loop_delay)
                continue
            await agent.make_decision(market_data)
        except Exception as e:
            logging.error(f"Error in agent loop: {e}", exc_info=True)
        await asyncio.sleep(config.agent_loop_delay)


async def load_or_train_lnn(gmn, model_path, config, scaler):
    try:
        input_size = len(config.timeframes) * len(config.indicators)
        model = LiquidNeuralNetwork(input_size, config.lnn_hidden_size, 1).to(config.device)
        model.load_state_dict(torch.load(model_path, map_location=config.device))
        model.eval()
        logging.info("Loaded pre-trained LNN model.")
        return model
    except FileNotFoundError:
        logging.info("Pre-trained LNN model not found. Training a new model...")
        return await train_and_save_lnn(gmn, model_path, config, scaler)
    except Exception as e:
        logging.exception(f"Error loading LNN model: {e}")
        return None


async def train_and_save_lnn(gmn, model_path, config, scaler):
    try:
        X_train, y_train = await prepare_lnn_training_data(gmn, config.training_history_length, scaler)
        if X_train is None or y_train is None:
            return None
        X_train = torch.tensor(X_train, dtype=torch.float32, device=config.device)
        y_train = torch.tensor(y_train, dtype=torch.float32, device=config.device)
        input_size = X_train.shape[1]
        model = LiquidNeuralNetwork(input_size, config.lnn_hidden_size, 1).to(config.device)
        criterion = torch.nn.BCEWithLogitsLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=config.lnn_learning_rate)
        epochs = config.lnn_training_epochs
        batch_size = 32
        dataset = torch.utils.data.TensorDataset(X_train, y_train)
        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

        for epoch in range(epochs):
            model.train()
            epoch_loss = 0.0
            for batch_X, batch_y in dataloader:
                optimizer.zero_grad()
                outputs = model(batch_X.unsqueeze(1))
                loss = criterion(outputs.squeeze(), batch_y)
                loss.backward()
                optimizer.step()
                epoch_loss += loss.item()
            avg_loss = epoch_loss / len(dataloader)
            logging.info(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")

        torch.save(model.state_dict(), model_path)
        model.eval()
        logging.info(f"LNN model trained and saved to {model_path}")
        joblib.dump(scaler, scaler_path)
        logging.info(f"Scaler saved to {scaler_path}")
        return model
    except Exception as e:
        logging.error(f"Error during LNN training: {e}", exc_info=True)
        return None


async def prepare_lnn_training_data(gmn: CryptoGMN, history_length: int, scaler: MinMaxScaler):
    try:
        market_data = gmn.get_all_data()
        if not market_data or len(market_data['1m']['price']) < history_length + 1:
            logging.error("Not enough data to prepare training dataset.")
            return None, None

        X = []
        y = []
        for i in range(history_length, len(market_data['1m']['price']) - 1):
            features = []
            for timeframe in gmn.timeframes:
                for indicator in gmn.indicators:
                    data_series = market_data[timeframe].get(indicator)
                    if data_series and len(data_series) > i:
                        value = data_series[i]
                        if isinstance(value, dict):
                            features.extend(list(value.values()))
                        else:
                            features.append(value)
                    else:
                        features.append(0.0)

            future_price = market_data['1m']['price'][i + 1]
            current_price = market_data['1m']['price'][i]
            price_change = (future_price - current_price) / current_price
            y.append(1 if price_change > 0 else 0)
            X.append(features)

        X = np.array(X, dtype=np.float32)
        y = np.array(y, dtype=np.float32)
        X_scaled = scaler.fit_transform(X)  # Fit and transform inside the function
        return X_scaled, y

    except Exception as e:
        logging.error(f"Error preparing LNN training data: {e}", exc_info=True)
        return None, None


async def load_scaler(scaler_path: str) -> MinMaxScaler:
    """Loads a saved scaler or creates a new one."""
    try:
        scaler = joblib.load(scaler_path)
        logging.info(f"Loaded scaler from {scaler_path}")
        return scaler
    except FileNotFoundError:
        logging.warning(f"Scaler file not found at {scaler_path}. Initializing a new scaler.")
        return MinMaxScaler()


async def shutdown():
    """Shuts down the application gracefully."""
    global bot_tasks
    for task in bot_tasks:
        if task:
            task.cancel()
            try:
                await task
            except asyncio.CancelledError:
                pass
    logging.info("Shutdown complete.")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logging.info("Trading bot stopped by user. Shutting down...")
        asyncio.run(shutdown())

// File: data\config.py
from pydantic import BaseModel, Field, field_validator
from typing import List, Dict, Any, Union, Literal
import yaml


class Config(BaseModel):
    """Configuration for Data Ingestion and market data settings."""

    symbol: str = "BTC_USDT"
    interval: str = "1m"  # Default interval, but multiple timeframes are supported.
    timeframes: List[str] = ["1m", "5m", "15m", "1h", "4h"]
    indicators: List[str] = ["price", "volume", "rsi", "macd", "fibonacci"]

    # Websocket and Reconnection Settings
    ws_url: str = "wss://wbs.mexc.com/ws"  # Default; can be overridden by environment variable.
    reconnect_delay: float = 5.0        # Base delay for reconnection attempts
    max_reconnect_attempts: int = 10    # Maximum reconnection attempts
    max_reconnect_delay: float = 300.0  # Maximum delay between reconnection attempts
    backoff_factor: float = 2.0          # Exponential backoff factor

    # Batch Processing Parameters
    batch_size: int = 100               # Number of messages per batch
    batch_time_limit: float = 5.0       # Time limit (seconds) for batching

    # Private Channels (Optional)
    private_channels: List[str] = []

    @field_validator('timeframes')
    def validate_timeframes(cls, v):
        """Validates that 'timeframes' is either a comma-separated string or a list."""
        if isinstance(v, str):
            return [tf.strip() for tf in v.split(',')]
        return v

    def load_from_yaml(self, config_path: str) -> None:
        """Loads configuration from a YAML file."""
        try:
            with open(config_path, "r") as f:
                config_data = yaml.safe_load(f)
                for key, value in config_data.items():
                    if hasattr(self, key):
                        setattr(self, key, value)
        except FileNotFoundError:
            raise FileNotFoundError(f"Configuration file '{config_path}' not found.")
        except yaml.YAMLError as e:
            raise ValueError(f"Error parsing YAML: {e}")


// File: data\data_processor.py
import logging
from typing import Dict, Any, List, Union
import pandas as pd
from marshmallow import Schema, fields, validates, ValidationError
from data.config import Config
from data.indicator_calculations import IndicatorCalculator
from data.error_handler import ErrorHandler
from datetime import datetime, timezone


class KlineSchema(Schema):
    """Schema to validate kline data."""
    T = fields.Int(required=True, data_key='T', attribute="close_time")   # Close Time
    a = fields.Float(required=True, data_key='a', attribute="volume")     # Volume
    c = fields.Float(required=True, data_key='c', attribute="close")       # Close Price
    h = fields.Float(required=True, data_key='h', attribute="high")        # High Price
    i = fields.Str(required=True, data_key='i', attribute="timeframe")    # Interval (rename 'i' to 'timeframe')
    l = fields.Float(required=True, data_key='l', attribute="low")         # Low Price
    o = fields.Float(required=True, data_key='o', attribute="open")        # Open Price
    t = fields.Int(required=True, data_key='t', attribute="open_time")    # Open Time
    v = fields.Float(required=True, data_key='v', attribute="quantity")    # Quantity (Base Asset Volume)


class DataProcessor:

    def __init__(self, gmn, indicator_calculator: IndicatorCalculator, error_handler: ErrorHandler, config: Config):
        self.gmn = gmn
        self.indicator_calculator = indicator_calculator
        self.error_handler = error_handler
        self.logger = logging.getLogger(__name__)
        self.kline_schema = KlineSchema()  # Initialize the schema once
        self.config = config

    async def process_data(self, data_batch: List[Dict[str, Any]]) -> None:
        """
        Processes a batch of data messages.

        Args:
            data_batch (List[Dict[str, Any]]): List of raw data messages from WebSocket.
        """
        processed_data = {}  # Store kline data organized by timeframe

        for data in data_batch:
            try:
                if data.get('method') == 'PONG':  # Correct ping/pong handling
                    continue  # Heartbeat; ignore and proceed to the next message

                channel = data.get('c')  # Use 'c' for the channel
                if not channel or not channel.startswith("spot@public.kline.v3.api"):
                    continue  # Skip non-kline messages

                # Extract timeframe and other essential fields
                # Assuming the channel format is "spot@public.kline.v3.api@<symbol>@<timeframe>"
                parts = channel.split('@')
                if len(parts) < 4:
                    self.logger.warning(f"Invalid channel format: {channel}")
                    continue

                timeframe = parts[-1].split('.')[0]  # Extract timeframe
                if timeframe not in self.config.timeframes:
                    self.logger.warning(f"Received unsupported timeframe: {timeframe}")
                    continue

                kline_data = data.get('d', {}).get('k', {})  # Accessing the correct path for Kline data
                if not kline_data:  # Skip if Kline data is missing or empty
                    continue

                # Validate the Kline data
                try:
                    validated_kline = self.kline_schema.load(kline_data)

                except ValidationError as e:
                    self.error_handler.handle_error(
                        f"Kline validation error: {e.messages}",
                        data=data,
                        exc_info=True
                    )
                    continue  # Skip this invalid message

                # Organize klines by timeframe
                if timeframe not in processed_data:
                    processed_data[timeframe] = []
                processed_data[timeframe].append(validated_kline)  # Append the validated Kline data

            except Exception as e:
                self.error_handler.handle_error(
                    f"Error processing message: {e}",
                    data=data,
                    exc_info=True
                )

        if not processed_data:
            self.logger.info("No kline data to process in this batch.")
            return  # No kline data in this batch

        try:
            data_frames = {}
            for timeframe, klines in processed_data.items():
                df = pd.DataFrame(klines)
                df = self._add_data_lineage(df, timeframe)
                data_frames[timeframe] = df

            # Calculate indicators
            indicators = self.indicator_calculator.calculate_indicators("BTC_USDT", data_frames)  # Pass data_frames

            # Create unified feed
            unified_feed = self.create_unified_feed(data_frames, indicators)

            # Store unified feed
            await self.gmn.store_data(unified_feed)

        except Exception as e:
            self.error_handler.handle_error(f"Error processing batch data: {e}", exc_info=True)

    def _add_data_lineage(self, df: pd.DataFrame, timeframe: str) -> pd.DataFrame:
        """
        Adds data lineage and transforms data.

        Args:
            df (pd.DataFrame): DataFrame containing kline data.
            timeframe (str): The timeframe of the kline data.

        Returns:
            pd.DataFrame: Transformed DataFrame with added metadata.
        """
        df['source'] = 'mexc_websocket'
        df['symbol'] = 'BTC_USDT'  # Or get from config if needed
        df['timeframe'] = timeframe
        df['processed_at'] = datetime.now(timezone.utc)
        # Rename columns to more descriptive names if needed
        df = df.rename(columns={
            'c': 'close',
            'v': 'volume',
            'o': 'open',
            'h': 'high',
            'l': 'low',
            'T': 'close_time',
            't': 'open_time',
            'a': 'quantity'
        })
        return df

    def create_unified_feed(self, klines: Dict[str, pd.DataFrame], indicators: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """
        Combines the kline data and indicators into a single unified feed for GMN.

        Args:
            klines (Dict[str, pd.DataFrame]): Dictionary of DataFrames keyed by timeframe.
            indicators (Dict[str, Dict[str, Any]]): Dictionary of indicators keyed by timeframe.

        Returns:
            Dict[str, Any]: Unified data feed containing all relevant data and indicators.
        """
        unified_feed = {}
        for timeframe, df in klines.items():
            unified_feed[timeframe] = {
                'price': df['close'].tolist(),
                'volume': df['volume'].tolist(),
                'open': df['open'].tolist(),
                'high': df['high'].tolist(),
                'low': df['low'].tolist(),
                'close_time': df['close_time'].tolist(),
                'open_time': df['open_time'].tolist(),
                'quantity': df['quantity'].tolist(),
                'indicators': indicators.get(timeframe, {})
            }
        return unified_feed


// File: data\error_handler.py
import logging
import traceback

# from .alerts import send_alert  # Uncomment if you want alert functionality

class ErrorHandler:
    def __init__(self, config=None):
        """
        Initializes the error handler.

        Args:
            config: Configuration settings, if needed for alerting or additional logging options.
        """
        self.logger = logging.getLogger(__name__)
        # self.config = config  # Uncomment if using config for alerting or other settings

    def handle_error(self, message: str, exc_info=False, alert=False, data=None):
        """
        Handles errors by logging the error and optionally sending alerts.

        Args:
            message (str): The error message to log.
            exc_info (bool): If True, includes the traceback in the log and prints it.
            alert (bool): If True, sends an alert notification (if implemented).
            data (Any): Optional additional data related to the error.
        """
        # 1. Log the error
        if data is not None:
            self.logger.error(f"{message} | Data: {data}", exc_info=exc_info)
        else:
            self.logger.error(message, exc_info=exc_info)  # Log the error message, with traceback if exc_info=True
        if exc_info:
            traceback.print_exc()  # Optionally print the full traceback to the console

        # 2. Optional: Send an alert (uncomment if needed)
        # if alert:  # Example alerting functionality (e.g., if config.alerts_enabled)
        #     try:
        #         send_alert(message)  # This would be your alerting function (e.g., email/SMS)
        #     except Exception as alert_error:
        #         self.logger.error(f"Failed to send alert: {alert_error}")


// File: data\indicator_calculations.py
import pandas as pd
from finta import TA
from .error_handler import ErrorHandler
from typing import Dict, Any, List

class IndicatorCalculator:
    def __init__(self, error_handler: ErrorHandler):
        """
        Initializes the IndicatorCalculator with an error handler.

        Args:
            error_handler (ErrorHandler): Instance to handle errors during calculations.
        """
        self.error_handler = error_handler

    def calculate_indicators(self, symbol: str, data: Dict[str, pd.DataFrame]) -> Dict[str, Dict[str, Any]]:
        """
        Calculates indicators for all timeframes.

        Args:
            symbol (str): The trading symbol (e.g., 'BTC_USDT').
            data (Dict[str, pd.DataFrame]): Dictionary where keys are timeframes and values are DataFrames.

        Returns:
            Dict[str, Dict[str, Any]]: Dictionary of indicators keyed by timeframe.
        """
        indicators = {}
        for timeframe, df in data.items():
            try:
                indicators[timeframe] = {}
                indicators[timeframe]['rsi'] = self.calculate_rsi(df)
                indicators[timeframe]['macd'] = self.calculate_macd(df)
                indicators[timeframe]['fibonacci'] = self.calculate_fibonacci(df)
            except Exception as e:
                self.error_handler.handle_error(
                    f"Error calculating indicators for {symbol} {timeframe}: {e}",
                    exc_info=True
                )
        return indicators

    def calculate_rsi(self, df: pd.DataFrame) -> List[float]:
        """
        Calculates the RSI (Relative Strength Index).

        Args:
            df (pd.DataFrame): DataFrame containing OHLC data.

        Returns:
            List[float]: RSI values.
        """
        try:
            rsi_values = TA.RSI(df).fillna(0).tolist()
            return rsi_values
        except Exception as e:
            self.error_handler.handle_error(f"Error calculating RSI: {e}", exc_info=True)
            return []

    def calculate_macd(self, df: pd.DataFrame) -> Dict[str, List[float]]:
        """
        Calculates the MACD (Moving Average Convergence Divergence).

        Args:
            df (pd.DataFrame): DataFrame containing OHLC data.

        Returns:
            Dict[str, List[float]]: MACD values, signal line, and histogram.
        """
        try:
            macd_values = TA.MACD(df).fillna(0)
            return {
                'macd': macd_values['MACD'].tolist(),
                'macd_signal': macd_values['SIGNAL'].tolist(),
                'macd_hist': macd_values['HISTOGRAM'].tolist()
            }
        except Exception as e:
            self.error_handler.handle_error(f"Error calculating MACD: {e}", exc_info=True)
            return {'macd': [], 'macd_signal': [], 'macd_hist': []}

    def calculate_fibonacci(self, df: pd.DataFrame) -> List[float]:
        """
        Calculates Fibonacci Retracement levels manually.

        Args:
            df (pd.DataFrame): DataFrame containing OHLC data.

        Returns:
            List[float]: Fibonacci retracement levels.
        """
        try:
            high = df['high'].max()
            low = df['low'].min()
            fib_levels = [high - ((high - low) * ratio) for ratio in [0.236, 0.382, 0.5, 0.618, 0.786]]
            return fib_levels
        except Exception as e:
            self.error_handler.handle_error(f"Error calculating Fibonacci Retracement: {e}", exc_info=True)
            return []


// File: data\mexc_websocket_connector.py
import asyncio
import json
import logging
import os
import time
from typing import Dict, Any, List
from asyncio import Queue
from dotenv import load_dotenv
import websockets
import requests
from data.config import Config
from data.websocket_manager import WebSocketManager
from data.error_handler import ErrorHandler

load_dotenv()

RECONNECT_JITTER = 0.2  # 20% jitter


class MexcRestAPI:
    """Handles REST API interactions with MEXC for listenKey management."""

    def __init__(self, api_key: str, api_secret: str):
        self.base_url = "https://api.mexc.com"
        self.api_key = api_key
        self.api_secret = api_secret

    def get_listen_key(self) -> str:
        """Fetch the listenKey from MEXC API to start the WebSocket connection."""
        url = f"{self.base_url}/api/v3/userDataStream"
        headers = {
            'X-MEXC-APIKEY': self.api_key,
        }
        response = requests.post(url, headers=headers)
        if response.status_code == 200:
            listen_key = response.json().get('listenKey')
            if listen_key:
                logging.info("Successfully obtained listenKey.")
                return listen_key
            else:
                raise Exception("Failed to obtain listenKey: No listenKey in response.")
        else:
            raise Exception(f"Failed to get listenKey: {response.text}")

    def refresh_listen_key(self, listen_key: str):
        """Refresh the listenKey before it expires (every 60 minutes)."""
        url = f"{self.base_url}/api/v3/userDataStream"
        headers = {
            'X-MEXC-APIKEY': self.api_key,
        }
        params = {"listenKey": listen_key}
        response = requests.put(url, headers=headers, params=params)
        if response.status_code == 200:
            logging.info("Successfully refreshed listenKey.")
        else:
            raise Exception(f"Failed to refresh listenKey: {response.text}")


class MexcWebsocketConnector:

    def __init__(self, config: Config, data_queue: Queue):
        """
        Initializes the WebSocket connector with configuration and data queue.

        Args:
            config (Config): Configuration settings.
            data_queue (Queue): Asynchronous queue to hold incoming data batches.
        """
        self.config = config
        self.data_queue = data_queue
        self.ws_url = config.ws_url  # Get ws_url from config
        self.api_key = os.getenv("MEXC_API_KEY")
        self.api_secret = os.getenv("MEXC_API_SECRET")
        self.websocket_manager = WebSocketManager(
            self.ws_url, self.api_key, self.api_secret, rate_limit=0, config=config  # rate_limit not needed here
        )

        self.reconnect_attempts = 0
        self._last_reconnect_time = 0
        self.connected_event = asyncio.Event()

        self.rest_api = MexcRestAPI(self.api_key, self.api_secret)
        self.listen_key = None
        self.listen_key_task = None

        # Initialize error handler
        self.error_handler = ErrorHandler()

    async def connect(self) -> None:
        """
        Connects to the WebSocket and handles reconnections with backoff.
        """
        try:
            # Get the initial listenKey
            self.listen_key = self.rest_api.get_listen_key()

            # Start the listenKey refresh task
            self.listen_key_task = asyncio.create_task(self._refresh_listen_key_periodically())

            while self.reconnect_attempts < self.config.max_reconnect_attempts:
                try:
                    await self._connect_and_receive()
                except websockets.exceptions.ConnectionClosedError as e:
                    logging.error(f"WebSocket connection error: {e}")
                except Exception as e:
                    logging.exception(f"An unexpected error occurred: {e}")
                finally:
                    self.connected_event.clear()

                await self._handle_reconnection_delay()

        except Exception as e:
            self.error_handler.handle_error(f"Failed to establish WebSocket connection: {e}", exc_info=True)

    async def _connect_and_receive(self):
        """
        Handles the WebSocket connection and data reception.
        """
        connection_url = f"{self.ws_url}?listenKey={self.listen_key}"
        async with websockets.connect(connection_url, ping_interval=20, ping_timeout=20) as ws:
            self.connected_event.set()
            logging.info("WebSocket connection established.")

            # Subscribe to multiple timeframes
            await self._subscribe(ws)

            # Start receiving data, batch process
            await self._receive_batched_klines(ws)

    async def _subscribe(self, ws):
        """
        Subscribe to the defined timeframes.

        Args:
            ws: WebSocket connection.
        """
        subscribe_message = {
            "method": "SUBSCRIBE",
            "params": [f"spot@public.kline.v3.api@BTCUSDT@{tf}" for tf in self.config.timeframes],
            "id": 1
        }
        await self.websocket_manager.send_message(json.dumps(subscribe_message), ws=ws)  # Added ws=ws
        logging.info(f"Subscribed to timeframes: {self.config.timeframes}")

    async def _receive_batched_klines(self, ws) -> None:
        """
        Receives data from WebSocket and processes multiple klines for all timeframes.

        Args:
            ws: WebSocket connection.
        """
        try:
            data_batch = []
            batch_size = self.config.batch_size
            batch_time_limit = self.config.batch_time_limit
            batch_start_time = time.time()

            while True:
                try:
                    message = await ws.recv()
                    data = json.loads(message)
                    data_batch.append(data)

                    current_time = time.time()
                    if len(data_batch) >= batch_size or (current_time - batch_start_time) >= batch_time_limit:
                        if data_batch:
                            await self.data_queue.put(data_batch.copy())
                            logging.debug(f"Batch of {len(data_batch)} klines put into queue.")
                            data_batch.clear()
                            batch_start_time = current_time
                except asyncio.CancelledError:
                    logging.info("Receive data loop cancelled.")
                    break
                except websockets.exceptions.ConnectionClosedOK as e:
                    logging.warning(f"WebSocket closed gracefully: {e.reason}")
                    break
                except websockets.exceptions.ConnectionClosedError as e:
                    logging.error(f"WebSocket closed with error: {e.reason}")
                    break
                except Exception as e:
                    self.error_handler.handle_error(f"Error in _receive_batched_klines: {e}", exc_info=True)
                    break

            # Put remaining data in queue
            if data_batch:
                await self.data_queue.put(data_batch.copy())
                logging.debug(f"Final batch of {len(data_batch)} messages put into queue.")

        except Exception as e:
            self.error_handler.handle_error(f"Exception in _receive_batched_klines: {e}", exc_info=True)

    async def _refresh_listen_key_periodically(self):
        """
        Periodically refreshes the listenKey before it expires.
        """
        try:
            while True:
                await asyncio.sleep(30 * 60)  # Refresh every 30 minutes
                try:
                    self.rest_api.refresh_listen_key(self.listen_key)
                except Exception as e:
                    self.error_handler.handle_error(f"Error refreshing listenKey: {e}", exc_info=True)
        except asyncio.CancelledError:
            logging.info("ListenKey refresh task cancelled.")

    async def _handle_reconnection_delay(self) -> None:
        """
        Handles delay between reconnection attempts with exponential backoff and jitter.
        """
        current_time = time.monotonic()
        backoff_delay = self.config.reconnect_delay * (self.config.backoff_factor ** self.reconnect_attempts)
        jitter = backoff_delay * RECONNECT_JITTER
        wait_time = min(backoff_delay + jitter, self.config.max_reconnect_delay)
        next_reconnect = self._last_reconnect_time + wait_time
        sleep_duration = max(0, next_reconnect - current_time)
        logging.info(f"Next reconnection attempt in {sleep_duration:.2f} seconds...")
        await asyncio.sleep(sleep_duration)
        self._last_reconnect_time = time.monotonic()
        self.reconnect_attempts += 1

    async def close(self) -> None:
        """
        Closes the WebSocket connection and cancels background tasks.
        """
        if self.listen_key_task:
            self.listen_key_task.cancel()
            try:
                await self.listen_key_task
            except asyncio.CancelledError:
                logging.info("ListenKey refresh task cancelled.")

        await self.websocket_manager.close()
        logging.info("WebSocket connection closed.")


// File: data\websocket_manager.py
import asyncio
import logging
import websockets


class WebSocketManager:
    """Manages the WebSocket connection."""

    def __init__(self, ws_url: str, api_key: str, api_secret: str, rate_limit: int, config=None):
        """
        Initializes the WebSocketManager.

        Args:
            ws_url (str): WebSocket URL.
            api_key (str): API key for authentication.
            api_secret (str): API secret for authentication.
            rate_limit (int): Rate limit for sending messages.
            config: Additional configuration if needed.
        """
        self.ws_url = ws_url
        self.api_key = api_key
        self.api_secret = api_secret
        self.rate_limit = rate_limit
        self.config = config
        self.ws = None  # The websocket connection object
        self.logger = logging.getLogger(__name__)

    async def __aenter__(self):
        """Establishes the WebSocket connection."""
        # The connector will handle setting the listen key in the ws_url now.
        self.ws = await websockets.connect(self.ws_url)
        return self.ws

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Closes the WebSocket connection."""
        await self.close()

    async def send_message(self, message: str, ws=None):
        """
        Sends a message over the WebSocket connection.

        Args:
            message (str): The message to send.
            ws: Optional WebSocket connection to use.
        """
        try:
            if ws:
                await ws.send(message)
            elif self.ws:
                await self.ws.send(message)  # Use self.ws if no ws argument is provided
            else:
                raise ValueError("No active WebSocket connection.")
        except websockets.exceptions.ConnectionClosed:
            self.logger.error("WebSocket connection closed when sending message.")
            raise  # Re-raise the exception to be handled by the connector
        except Exception as e:
            self.logger.exception(f"Error sending message: {e}")
            raise

    async def receive_message(self, ws):
        """
        Receives a message from the WebSocket.

        Args:
            ws: WebSocket connection to receive from.

        Returns:
            str: The received message.
        """
        try:
            message = await ws.recv()
            return message
        except websockets.exceptions.ConnectionClosed:
            self.logger.error("WebSocket connection closed when receiving message.")
            raise  # Re-raise the exception
        except Exception as e:
            self.logger.exception(f"Error receiving message: {e}")
            raise

    async def close(self):
        """Closes the WebSocket connection."""
        if self.ws and not self.ws.closed:
            await self.ws.close()
            self.ws = None  # Reset the websocket object
            self.logger.info("WebSocket connection closed.")


// File: data\__init__.py
from .config import Config
from .mexc_websocket_connector import MexcWebsocketConnector
from .data_processor import DataProcessor
from .websocket_manager import WebSocketManager
from .storage.data_storage import DataStorage

__all__ = ['Config', 'MexcWebsocketConnector', 'DataProcessor', 'WebSocketManager', 'DataStorage']


// File: data\storage\data_storage.py
import pandas as pd
import os
from typing import Dict, Any


class DataStorage:
    def __init__(self, storage_path: str = "data_storage"):
        """
        Initializes the DataStorage with a specified storage path.

        Args:
            storage_path (str): Directory path where CSV files will be saved.
        """
        self.data = {}
        self.storage_path = storage_path
        os.makedirs(self.storage_path, exist_ok=True)

    def store_data(self, unified_feed: Dict[str, Any]) -> None:
        """
        Stores the unified feed data for all timeframes.

        Args:
            unified_feed (Dict[str, Any]): The unified feed containing data and indicators for each timeframe.
        """
        symbol = "BTC_USDT"  # Assuming single symbol; adjust if multiple symbols

        for timeframe, content in unified_feed.items():
            if symbol not in self.data:
                self.data[symbol] = {}
            if timeframe not in self.data[symbol]:
                self.data[symbol][timeframe] = {}

            # Store price and volume
            self.data[symbol][timeframe]['price'] = content.get('price', [])
            self.data[symbol][timeframe]['volume'] = content.get('volume', [])
            self.data[symbol][timeframe]['open'] = content.get('open', [])
            self.data[symbol][timeframe]['high'] = content.get('high', [])
            self.data[symbol][timeframe]['low'] = content.get('low', [])
            self.data[symbol][timeframe]['close_time'] = content.get('close_time', [])
            self.data[symbol][timeframe]['open_time'] = content.get('open_time', [])
            self.data[symbol][timeframe]['quantity'] = content.get('quantity', [])

            # Store indicators
            self.data[symbol][timeframe]['indicators'] = content.get('indicators', {})

            # Save to file
            self.save_to_file(symbol, timeframe, content)

    def save_to_file(self, symbol: str, timeframe: str, content: Dict[str, Any]) -> None:
        """
        Saves the unified feed to a CSV file per timeframe.

        Args:
            symbol (str): The trading symbol.
            timeframe (str): The timeframe.
            content (Dict[str, Any]): The data and indicators.
        """
        # Prepare a DataFrame
        df = pd.DataFrame({
            'open': content.get('open', []),
            'high': content.get('high', []),
            'low': content.get('low', []),
            'close': content.get('price', []),
            'volume': content.get('volume', []),
            'quantity': content.get('quantity', []),
            'open_time': content.get('open_time', []),
            'close_time': content.get('close_time', []),
        })

        # Add indicators
        indicators = content.get('indicators', {})
        for indicator, values in indicators.items():
            if isinstance(values, dict):
                for sub_indicator, sub_values in values.items():
                    column_name = f"{indicator}_{sub_indicator}"
                    df[column_name] = sub_values
            else:
                df[indicator] = values

        # Define filename
        filename = os.path.join(self.storage_path, f"{symbol}_{timeframe}.csv")
        df.to_csv(filename, index=False)
        self._log_storage(filename)

    def _log_storage(self, filename: str):
        """Logs the storage action."""
        print(f"Data saved to {filename}")

    def get_data(self, symbol: str, timeframe: str) -> pd.DataFrame:
        """
        Retrieves stored data for a given symbol and timeframe.

        Args:
            symbol (str): The trading symbol.
            timeframe (str): The timeframe.

        Returns:
            pd.DataFrame: DataFrame containing the stored data.
        """
        if symbol in self.data and timeframe in self.data[symbol]:
            data = self.data[symbol][timeframe]
            df = pd.DataFrame({
                'open': data.get('open', []),
                'high': data.get('high', []),
                'low': data.get('low', []),
                'close': data.get('price', []),
                'volume': data.get('volume', []),
                'quantity': data.get('quantity', []),
                'open_time': data.get('open_time', []),
                'close_time': data.get('close_time', []),
            })
            # Add indicators
            indicators = data.get('indicators', {})
            for indicator, values in indicators.items():
                if isinstance(values, dict):
                    for sub_indicator, sub_values in values.items():
                        column_name = f"{indicator}_{sub_indicator}"
                        df[column_name] = sub_values
                else:
                    df[indicator] = values
            return df
        return pd.DataFrame()


// File: data\storage\__init__.py


// File: models\__init__.py


// File: models\agents\agent.py
# File: models/agents/agent.py

import logging
import numpy as np
import torch
from sklearn.preprocessing import MinMaxScaler
from models.utils.config import Config
from models.utils.risk_management import RiskManager
import os
import aiohttp  # Ensure aiohttp is imported
import hashlib
import hmac
import time

class TradingAgent:
    def __init__(
        self, 
        timeframes, 
        indicators, 
        model, 
        config: Config, 
        risk_manager: RiskManager, 
        scaler: MinMaxScaler
    ):
        self.timeframes = timeframes
        self.indicators = indicators
        self.model = model
        self.config = config
        self.threshold_buy = config.threshold_buy
        self.threshold_sell = config.threshold_sell
        self.risk_manager = risk_manager
        self.trade_parameters = config.trade_parameters
        self.leverage = self.trade_parameters.get("leverage", 20)
        self.position = None
        self.scaler = scaler
        self.api_key = os.getenv("MEXC_API_KEY")
        self.api_secret = os.getenv("MEXC_API_SECRET")
        self.base_url = 'https://contract.mexc.com/api/v1/'  # Update if necessary

        self.peak_portfolio_value = 1.0  # Initialize for drawdown calculations
        self.portfolio_value = 1.0  # Initialize portfolio value

        # Initialize aiohttp session
        self.session = aiohttp.ClientSession()

    async def make_decision(self, market_data):
        """Processes market data, makes predictions, and executes trades based on the model's output."""
        try:
            input_vector = self._prepare_input(market_data)
            if input_vector is None:
                logging.warning("Input vector is None. Skipping decision.")
                return

            # Model expects input in float32
            input_tensor = torch.tensor([input_vector], dtype=torch.float32)

            with torch.no_grad():
                self.model.eval()
                prediction = self.model(input_tensor)
                prediction = torch.sigmoid(prediction)  # Apply sigmoid to get probability
                prediction_value = prediction.item()

            logging.info(f"Model Prediction Probability: {prediction_value:.4f}")

            current_drawdown = self.calculate_current_drawdown()

            if self.risk_manager.check_risk(current_drawdown, self.position, market_data):
                if prediction_value >= self.threshold_buy and self.position != 'long':
                    await self._execute_trade('buy')
                    self.position = 'long'
                elif prediction_value <= self.threshold_sell and self.position != 'short':
                    await self._execute_trade('sell')
                    self.position = 'short'
                elif self.threshold_sell < prediction_value < self.threshold_buy:
                    if self.position is not None:
                        await self._execute_trade('close')
                        self.position = None
            else:
                logging.warning("Risk management check failed. Not executing trade.")

        except Exception as e:
            logging.error(f"Error in make_decision: {e}")

    def _prepare_input(self, market_data):
        """Prepares and scales the input vector for the LNN model."""
        input_vector = []
        for timeframe in self.timeframes:
            for indicator in self.indicators:
                data = market_data.get(timeframe, {}).get(indicator)
                if data is None or len(data) == 0:
                    logging.warning(f"Missing data for {timeframe} {indicator}. Skipping.")
                    return None

                if isinstance(data[-1], dict):
                    values = [v for v in data[-1].values() if isinstance(v, (int, float))]
                    input_vector.extend(values)
                else:
                    input_vector.append(data[-1])

        if not input_vector:
            logging.warning("Input vector is empty. No data available for making a decision.")
            return None

        input_vector = np.array([input_vector], dtype=np.float32)
        input_vector = self.scaler.transform(input_vector).astype(np.float32)
        return input_vector.flatten()

    async def _execute_trade(self, action, symbol="BTC_USDT"):
        """Executes a trade action (buy, sell, close) via the MEXC API using aiohttp."""
        try:
            timestamp = int(time.time() * 1000)
            params = {
                "symbol": symbol,
                "timestamp": timestamp
            }

            # Determine trade parameters based on action
            if action == 'buy':
                side = 'OPEN_LONG'
                quantity = self.trade_parameters.get("volume", 1)
            elif action == 'sell':
                side = 'OPEN_SHORT'
                quantity = self.trade_parameters.get("volume", 1)
            elif action == 'close':
                if self.position == 'long':
                    side = 'CLOSE_LONG'
                elif self.position == 'short':
                    side = 'CLOSE_SHORT'
                else:
                    logging.warning("No position to close.")
                    return
                quantity = self.trade_parameters.get("volume", 1)
            else:
                logging.warning(f"Invalid trade action: {action}")
                return

            # Set additional parameters
            params.update({
                "price": '',  # Empty for market orders
                "vol": quantity,
                "side": side,
                "type": self.trade_parameters.get("order_type", 1),  # 1: Market order
                "leverage": self.leverage,
                "openType": self.trade_parameters.get("open_type", 1),  # 1: Isolated margin
            })

            # Generate signature
            query_string = '&'.join([f"{key}={params[key]}" for key in sorted(params)])
            signature = hmac.new(
                self.api_secret.encode('utf-8'),
                query_string.encode('utf-8'),
                hashlib.sha256
            ).hexdigest()
            params['sign'] = signature

            # Send request using aiohttp
            url = self.base_url + 'order/submit'
            async with self.session.post(url, params=params) as response:
                response_data = await response.json()
                if response.status == 200 and response_data.get("success", False):
                    logging.info(f"Successfully executed {action} order: {response_data}")
                    # Update portfolio value based on trade execution
                    self.update_portfolio(action, response_data)
                else:
                    logging.error(f"Failed to execute {action} order: {response_data}")

        except Exception as e:
            logging.error(f"Error executing trade: {e}")

    def calculate_current_drawdown(self):
        """Calculates the current drawdown based on portfolio value."""
        self.peak_portfolio_value = max(self.peak_portfolio_value, self.portfolio_value)
        drawdown = (self.peak_portfolio_value - self.portfolio_value) / self.peak_portfolio_value
        logging.info(f"Current Drawdown: {drawdown:.4f}")
        return drawdown

    def update_portfolio(self, action: str, response_data: Dict):
        """
        Updates the portfolio value based on the executed trade.
        This is a placeholder function. You need to implement actual portfolio management logic.
        """
        # Example: Update portfolio based on the price and quantity
        try:
            price = float(response_data.get('data', {}).get('price', self.portfolio_value))
            quantity = float(response_data.get('data', {}).get('vol', 0))
            if action == 'buy':
                # Example logic: Increase portfolio value
                self.portfolio_value += price * quantity
            elif action == 'sell':
                # Example logic: Decrease portfolio value
                self.portfolio_value -= price * quantity
            elif action == 'close':
                # Example logic: Neutralize position
                pass
            logging.info(f"Portfolio updated after {action}: {self.portfolio_value}")
        except Exception as e:
            logging.error(f"Error updating portfolio: {e}")

    async def close(self):
        """Closes the aiohttp session."""
        await self.session.close()

// File: models\agents\rl_agent.py
# models/agents/rl_agent.py

import gym
import numpy as np
from stable_baselines3 import PPO

class TradingEnvironment(gym.Env):
    def __init__(self, market_data):
        super(TradingEnvironment, self).__init__()
        self.market_data = market_data
        self.current_step = 0
        self.action_space = gym.spaces.Discrete(3)  # Buy, Sell, Hold
        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(OBSERVATION_SPACE_SIZE,), dtype=np.float32)

    def reset(self):
        self.current_step = 0
        return self._next_observation()

    def _next_observation(self):
        # Return the next market observation
        obs = self.market_data.iloc[self.current_step]
        return obs.values

    def step(self, action):
        # Execute one time step within the environment
        self.current_step += 1

        reward = self._calculate_reward(action)
        done = self.current_step >= len(self.market_data) - 1
        obs = self._next_observation() if not done else np.zeros(self.observation_space.shape)

        return obs, reward, done, {}

    def _calculate_reward(self, action):
        # Implement reward calculation
        return reward

def train_rl_agent():
    # Load market data
    market_data = pd.read_csv("data/distilled/distilled_data.csv")
    env = TradingEnvironment(market_data)
    model = PPO("MlpPolicy", env, verbose=1)
    model.learn(total_timesteps=10000)
    model.save("models/agents/ppo_agent")

if __name__ == "__main__":
    train_rl_agent()


// File: models\agents\__init__.py


// File: models\gmn\config.py
import os
from pydantic import BaseModel, Field, validator
from typing import List

class CryptoGMNConfig(BaseModel):
    timeframes: List[str] = ["1m", "5m"]
    indicators: List[str] = ["price", "volume", "rsi", "macd", "fibonacci"]
    max_history: int = 1000
    executor_workers: int = 5
    cache_size: int = 100
    db_path: str = "market_data.db"
    performance_threshold: float = 0.1
    
    @validator('timeframes', 'indicators', pre=True)
    def split_comma_separated(cls, v):
        if isinstance(v, str):
            return [item.strip() for item in v.split(",")]
        return v

def load_config():
    env = os.getenv("ENVIRONMENT", "DEFAULT").upper()
    config_file = f"config_{env.lower()}.ini" if env != "DEFAULT" else "config.ini"
    
    # Here you would typically load from the config file
    # For simplicity, we're using default values
    return CryptoGMNConfig()

// File: models\gmn\crypto_gmn.py
import asyncio
from collections import deque
from concurrent.futures import ThreadPoolExecutor
import logging
from typing import Any, Dict, List, Optional, Union
import numpy as np
from pydantic import BaseModel, ValidationError

logger = logging.getLogger(__name__)

class CryptoGMNError(Exception):
    """Base exception for CryptoGMN."""
    pass

class DataValidationError(CryptoGMNError):
    """Exception raised when data validation fails."""
    pass

class DataModel(BaseModel):
    c: float
    v: Optional[float] = 0.0

    class Config:
        arbitrary_types_allowed = True

class CryptoGMN:
    def __init__(self, timeframes: List[str], max_history_length: int, executor_workers: int):
        self.timeframes = timeframes
        self.max_history_length = max_history_length
        self.market_data: Dict[str, Dict[str, deque]] = {
            timeframe: {'price': deque(maxlen=max_history_length), 'volume': deque(maxlen=max_history_length)}
            for timeframe in timeframes
        }
        self.locks: Dict[str, asyncio.Lock] = {timeframe: asyncio.Lock() for timeframe in timeframes}
        self._executor_lock = asyncio.Lock()
        self._executor: Optional[ThreadPoolExecutor] = None
        self.executor_workers = executor_workers

    async def __aenter__(self):
        async with self._executor_lock:
            if self._executor is None:
                self._executor = ThreadPoolExecutor(max_workers=self.executor_workers)
                logger.info(f"ThreadPoolExecutor initialized with {self.executor_workers} workers.")
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.shutdown()

    async def update_graph(self, new_data_items: List[Dict[str, Any]]) -> None:
        try:
            await asyncio.gather(*(self._update_single_data(data) for data in new_data_items))
        except Exception as e:
            logger.error(f"Error updating graph: {e}", exc_info=True)
            raise CryptoGMNError("Failed to update graph") from e

    async def _update_single_data(self, new_data: Dict[str, Any]) -> None:
        try:
            validated_data = DataModel(**new_data)
            price = validated_data.c
            volume = validated_data.v
        except ValidationError as e:
            logger.error(f"Data validation error: {e}. Data: {new_data}")
            raise DataValidationError(f"Invalid data format: {e}") from e

        try:
            await asyncio.gather(*(self._update_timeframe(tf, price, volume) for tf in self.timeframes))
        except Exception as e:
            logger.error(f"Error updating timeframes: {e}", exc_info=True)
            raise CryptoGMNError("Failed to update timeframes") from e

    async def _update_timeframe(self, timeframe: str, price: float, volume: float) -> None:
        async with self.locks[timeframe]:
            try:
                data = self.market_data[timeframe]
                data['price'].append(price)
                data['volume'].append(volume)
            except Exception as e:
                logger.error(f"Error updating timeframe {timeframe}: {e}", exc_info=True)
                raise CryptoGMNError(f"Failed to update timeframe {timeframe}") from e

    def get_data(self, timeframe: str, data_type: str) -> Optional[List[Union[float]]]:
        try:
            return list(self.market_data[timeframe][data_type])
        except KeyError:
            logger.warning(f"No data found for timeframe '{timeframe}' and data type '{data_type}'.")
            return None

    def get_all_data(self) -> Dict[str, Dict[str, List[Union[float]]]]:
        return {
            timeframe: {
                data_type: list(data)
                for data_type, data in indicators.items()
            }
            for timeframe, indicators in self.market_data.items()
        }

    async def shutdown(self) -> None:
        if self._executor:
            self._executor.shutdown(wait=True)
            self._executor = None
        logger.info("CryptoGMN has been shut down gracefully.")

// File: models\gmn\data_model.py
from pydantic import BaseModel, validator
from typing import Optional

class DataModel(BaseModel):
    c: float  # Close price
    v: Optional[float] = 0.0  # Volume

    @validator('c', 'v', pre=True)
    def validate_numeric(cls, v):
        if isinstance(v, (int, float)):
            return float(v)
        try:
            return float(v)
        except (ValueError, TypeError):
            raise ValueError(f"Value '{v}' is not a valid float.")

// File: models\gmn\gmn.py
import asyncio
from collections import deque
from concurrent.futures import ThreadPoolExecutor
from typing import Any, Dict, List, Optional, Union, Callable, cast
import numpy as np
import talib  # type: ignore  # Suppress type errors for TA-Lib
import logging
from configparser import ConfigParser
from pydantic import BaseModel, ValidationError, validator

logger = logging.getLogger(__name__)

# Load configuration from external file (config.ini)
config = ConfigParser()
config.read("config.ini")  # Ensure a config.ini file exists in the same directory

# Default configuration values
DEFAULT_TIMEFRAMES = config.get("DEFAULT", "timeframes", fallback="1m,5m").split(",")  # Example: ['1m', '5m']
DEFAULT_INDICATORS = config.get("DEFAULT", "indicators", fallback="price,volume,rsi,macd,fibonacci").split(",")  # Example: ['price', 'volume', 'rsi', 'macd', 'fibonacci']
MAX_HISTORY = config.getint("DEFAULT", "max_history", fallback=1000)
EXECUTOR_WORKERS = config.getint("DEFAULT", "executor_workers", fallback=5)


class CryptoGMNError(Exception):
    """Base exception for CryptoGMN."""
    pass


class IndicatorCalculationError(CryptoGMNError):
    """Exception raised when an indicator calculation fails."""
    pass


class DataModel(BaseModel):
    """Pydantic model for validating incoming data."""
    c: float  # Close price
    v: Optional[float] = 0.0  # Volume

    @validator('c', 'v', pre=True)
    def validate_numeric(cls, v):
        if isinstance(v, (int, float)):
            return float(v)
        try:
            return float(v)
        except (ValueError, TypeError):
            raise ValueError(f"Value '{v}' is not a valid float.")


class CryptoGMN:
    """Manages cryptocurrency market data and technical indicators."""

    def __init__(self, timeframes: List[str] = DEFAULT_TIMEFRAMES,  # Default values
                 indicators: List[str] = DEFAULT_INDICATORS,
                 max_history_length: int = MAX_HISTORY,
                 executor_workers: int = EXECUTOR_WORKERS) -> None:
        """
        Initializes the CryptoGMN instance.

        :param timeframes: List of timeframes (e.g., ['1m', '5m', '1h'])
        :param indicators: List of indicators (e.g., ['price', 'volume', 'rsi', 'macd', 'fibonacci'])
        :param max_history_length: Maximum number of data points to store per indicator
        :param executor_workers: Number of worker threads for indicator calculations
        """
        self.timeframes = timeframes
        self.indicators = indicators
        self.max_history_length = max_history_length
        self.market_data: Dict[str, Dict[str, deque]] = {
            timeframe: {indicator: deque(maxlen=max_history_length) for indicator in indicators}
            for timeframe in timeframes
        }
        self.locks: Dict[str, asyncio.Lock] = {timeframe: asyncio.Lock() for timeframe in timeframes}
        self.executor: ThreadPoolExecutor  # Explicitly type hint
        # Asynchronous context manager for Executor
        self._executor_lock = asyncio.Lock()
        self._executor: Optional[ThreadPoolExecutor] = None

    async def __aenter__(self):
        async with self._executor_lock:
            if self._executor is None:
                self._executor = ThreadPoolExecutor(max_workers=self.executor_workers)  # Use the config value here
            self.executor = cast(ThreadPoolExecutor, self._executor)  # Cast to make mypy happy.
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.shutdown()

    async def update_graph(self, new_data_items: List[Dict[str, Any]]) -> None:
        """Updates market data with new items concurrently."""
        await asyncio.gather(*(self._update_single_data(data) for data in new_data_items))  # More concise

    async def _update_single_data(self, new_data: Dict[str, Any]) -> None:
        """Updates data for a single new item across all timeframes."""
        try:
            validated_data = DataModel(**new_data)
            price = validated_data.c
            volume = validated_data.v
        except ValidationError as e:
            logger.error(f"Data validation error: {e}. Data: {new_data}")
            return

        await asyncio.gather(*(self._update_timeframe(tf, price, volume) for tf in self.timeframes))

    async def _update_timeframe(self, timeframe: str, price: float, volume: float) -> None:
        """Updates data for a specific timeframe, including batching for high-frequency data."""
        async with self.locks[timeframe]:
            data = self.market_data[timeframe]
            data['price'].append(price)
            data['volume'].append(volume)

            prices_array = np.array(data['price'], dtype=np.float64)

            if len(prices_array) >= 14:
                await self._calculate_indicators(timeframe, prices_array)

    async def _calculate_indicators(self, timeframe: str, prices_array: np.ndarray):
        """Calculates all indicators for a timeframe concurrently."""
        tasks = []
        if 'rsi' in self.indicators:
            tasks.append(self._calculate_and_append(timeframe, 'rsi', talib.RSI, prices_array, {'timeperiod': 14}))
        if 'macd' in self.indicators:
            tasks.append(self._calculate_and_append(
                timeframe, 'macd',
                lambda prices: self._macd_wrapper(prices),  # Lambda for MACD extraction
                prices_array
            ))
        if 'fibonacci' in self.indicators:
            tasks.append(self._calculate_and_append_fibonacci(timeframe, prices_array))
        # Add other indicators similarly if needed
        if tasks:
            await asyncio.gather(*tasks)

    async def _calculate_and_append(self, timeframe: str, indicator: str,
                                    indicator_func: Callable, prices: np.ndarray,
                                    kwargs: Dict[str, Any] = {}) -> None:
        """
        Calculates an indicator and appends the result to the market data.

        :param timeframe: The timeframe for which to calculate the indicator
        :param indicator: The name of the indicator
        :param indicator_func: The TA-Lib function to calculate the indicator
        :param prices: NumPy array of prices
        :param kwargs: Additional keyword arguments for the indicator function
        """
        loop = asyncio.get_running_loop()
        try:
            result = await loop.run_in_executor(
                self.executor, indicator_func, prices, **kwargs  # Simplify executor call
            )
        except Exception as e:
            logger.error(f"Error running indicator '{indicator}' for timeframe '{timeframe}': {e}")
            return

        if result is not None:
            try:
                latest = result[-1]
                if isinstance(latest, np.ndarray):
                    latest = latest.item()  # Convert numpy scalar to Python float
                if np.isnan(latest):
                    logger.warning(f"{indicator.upper()} returned NaN or empty result for timeframe {timeframe}")
                    return
                self.market_data[timeframe][indicator].append(float(latest))
            except IndexError:
                logger.warning(f"{indicator.upper()} returned NaN or empty result for timeframe {timeframe}")

    async def _calculate_and_append_fibonacci(self, timeframe: str, prices: np.ndarray, lookback: int = 14) -> None:
        """Calculates and appends Fibonacci levels."""
        loop = asyncio.get_running_loop()
        try:
            fibonacci_levels = await loop.run_in_executor(
                self.executor, self._calculate_fibonacci, prices, lookback
            )
        except Exception as e:
            logger.error(f"Error calculating Fibonacci for timeframe '{timeframe}': {e}")
            return

        if fibonacci_levels:
            self.market_data[timeframe]['fibonacci'].append(fibonacci_levels)

    def _macd_wrapper(self, prices: np.ndarray, fastperiod: int = 12, slowperiod: int = 26, signalperiod: int = 9) -> Optional[float]:
        """Wrapper for talib.MACD to return only the MACD line."""
        try:
            macd, _, _ = talib.MACD(prices, fastperiod=fastperiod, slowperiod=slowperiod, signalperiod=signalperiod)
            if macd is None or len(macd) == 0:
                logger.warning(f"MACD calculation returned no data for timeframe.")
                return None
            latest_macd = macd[-1]
            if np.isnan(latest_macd):
                logger.warning(f"MACD returned NaN for timeframe.")
                return None
            return float(latest_macd)
        except Exception as e:
            logger.error(f"Error in _macd_wrapper: {e}")
            return None

    def _calculate_fibonacci(self, prices: np.ndarray, lookback: int) -> Optional[Dict[str, Union[float, bool]]]:
        """Calculates Fibonacci retracement levels and indicates the closest level."""
        try:
            if len(prices) < lookback:
                logger.warning("Not enough data to calculate Fibonacci retracement.")
                return None
            recent_prices = prices[-lookback:]
            high = np.max(recent_prices)
            low = np.min(recent_prices)
            close = recent_prices[-1]
            diff = high - low
            if diff == 0:
                logger.warning("High and low prices are the same; cannot calculate Fibonacci levels.")
                return None  # Avoid division by zero

            levels = {
                "23.6%": high - 0.236 * diff,
                "38.2%": high - 0.382 * diff,
                "50%": high - 0.5 * diff,
                "61.8%": high - 0.618 * diff,
                "78.6%": high - 0.786 * diff,
                "100%": low,
            }
            closest_level_key = min(
                levels.keys(), key=lambda k: abs(levels[k] - close)
            )
            for k in levels:
                levels[k] = {"value": levels[k], "is_closest": k == closest_level_key}  # Indicate closest

            return levels
        except Exception as e:
            logger.error(f"Error calculating Fibonacci retracement: {e}")
            return None

    def get_data(self, timeframe: str, indicator: str) -> Optional[List[Union[float, Dict[str, Union[float, bool]]]]]:
        """Retrieves the latest data for a specific timeframe and indicator."""
        try:
            return list(self.market_data[timeframe][indicator])
        except KeyError:
            logger.warning(
                f"No data found for timeframe '{timeframe}' and indicator '{indicator}'."
            )
            return None

    def get_all_data(self) -> Dict[str, Dict[str, List[Union[float, Dict[str, Union[float, bool]]]]]]:
        """Retrieves all market data across all timeframes and indicators."""
        return {
            timeframe: {
                indicator: list(data)
                for indicator, data in indicators.items()
            }
            for timeframe, indicators in self.market_data.items()
        }

    async def shutdown(self) -> None:
        """Shuts down the executor gracefully."""
        if self._executor:
            self._executor.shutdown(wait=True)
            self._executor = None  # Important for proper cleanup
        logger.info("CryptoGMN has been shut down gracefully.")


// File: models\gmn\logger.py
import logging

class CryptoGMNLogger:
    def __init__(self, name: str, level: int = logging.INFO):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(level)
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        
        file_handler = logging.FileHandler('cryptogmn.log')
        file_handler.setFormatter(formatter)
        self.logger.addHandler(file_handler)
        
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        self.logger.addHandler(console_handler)

logger = CryptoGMNLogger("CryptoGMN").logger

// File: models\gmn\main.py
import asyncio
import logging
import os
from crypto_gmn import CryptoGMN
from config import load_config

async def main():
    # Optionally set the environment variable before running
    # os.environ["ENVIRONMENT"] = "PRODUCTION"

    config = load_config()
    
    async with CryptoGMN(config) as gmn:
        # Example batch of new data points
        new_data = [
            {'c': 150.0, 'v': 1000},
            {'c': 155.0, 'v': 1200},
            {'c': 160.0, 'v': 1500},
            {'c': 158.0, 'v': 1100},
            {'c': 162.0, 'v': 1300},
        ]

        # Update the graph with new data
        await gmn.update_graph(new_data)

        # Retrieve specific indicator data
        rsi_data = gmn.get_data('1m', 'rsi')
        macd_data = gmn.get_data('1m', 'macd')
        fibonacci_data = gmn.get_data('1m', 'fibonacci')

        print(f"RSI Data for 1m: {rsi_data}")
        print(f"MACD Data for 1m: {macd_data}")
        print(f"Fibonacci Data for 1m: {fibonacci_data}")

        # Retrieve all market data
        all_data = gmn.get_all_data()
        print(f"All Market Data: {all_data}")

        # Dynamically add a new indicator (e.g., EMA)
        ema_func = gmn.indicator_factory.create_ema(timeperiod=50)
        gmn.add_indicator('1m', 'ema', ema_func)

        # Update with new data to trigger EMA calculation
        new_data = [
            {'c': 165.0, 'v': 1400},
            {'c': 170.0, 'v': 1600},
        ]
        await gmn.update_graph(new_data)

        ema_data = gmn.get_data('1m', 'ema')
        print(f"EMA Data for 1m: {ema_data}")

        # Fetch real-time data from Binance
        await gmn.fetch_real_time_data(exchange='binance', symbol='BTCUSDT')

        # Access cached indicator data
        cached_rsi = gmn.get_cached_indicator('1m', 'rsi', window=10)
        print(f"Cached RSI Data for 1m (last 10): {cached_rsi}")

if __name__ == "__main__":
    asyncio.run(main())

// File: models\gmn\performance_monitor.py
from collections import deque

class PerformanceMonitor:
    def __init__(self, window_size: int = 100):
        self.processing_times = deque(maxlen=window_size)

    def record(self, start_time: float, end_time: float):
        self.processing_times.append(end_time - start_time)

    @property
    def average_processing_time(self):
        return sum(self.processing_times) / len(self.processing_times) if self.processing_times else 0

// File: models\gmn\utils.py
import functools
from logger import logger

def retry(exceptions, tries=3, delay=1.0):
    def decorator(func):
        @functools.wraps(func)
        async def wrapper(*args, **kwargs):
            for attempt in range(tries):
                try:
                    return await func(*args, **kwargs)
                except exceptions as e:
                    logger.warning(f"Attempt {attempt + 1} failed with error: {e}")
                    if attempt < tries - 1:
                        await asyncio.sleep(delay)
                    else:
                        logger.error(f"All {tries} attempts failed for function {func.__name__}")
                        raise
        return wrapper
    return decorator

// File: models\gmn\__init__.py


// File: models\lnn\lnn_model.py
# File: models/lnn/lnn_model.py

import torch
import torch.nn as nn

class LiquidNeuralNetwork(nn.Module):
    def __init__(self, input_size: int, hidden_size: int, output_size: int, num_layers: int = 2, dropout: float = 0.2):
        super(LiquidNeuralNetwork, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, output_size)
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.ReLU()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass of the LNN.
        Args:
            x: Input tensor of shape (batch_size, seq_len, input_size)
        Returns:
            Output tensor of shape (batch_size, output_size)
        """
        out, _ = self.lstm(x)  # out: (batch_size, seq_len, hidden_size)
        out = self.dropout(out[:, -1, :])  # Take the output from the last time step
        out = self.relu(out)
        out = self.fc(out)
        return out


// File: models\lnn\train_lnn.py
# File: models/lnn/train_lnn.py

import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from models.lnn.lnn_model import LiquidNeuralNetwork
from models.utils.config import Config
from sklearn.preprocessing import MinMaxScaler
import logging


def train_lnn():
    """Trains the LNN model and saves it."""
    config = Config("configs/config.yaml")
    
    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )

    try:
        # Load and prepare your data
        # Replace 'your_training_data.csv' with your actual data source
        data = pd.read_csv("data/your_training_data.csv")  # Ensure this file exists and is correctly formatted
        X = data.iloc[:, :-1].values
        y = data.iloc[:, -1].values

        # Scale the input features
        scaler = MinMaxScaler()
        X_scaled = scaler.fit_transform(X)

        # Convert to tensors
        X_tensor = torch.tensor(X_scaled, dtype=torch.float32).unsqueeze(1)  # Shape: (batch_size, seq_len, input_size)
        y_tensor = torch.tensor(y, dtype=torch.float32)  # Shape: (batch_size,)

        # Create DataLoader
        dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)
        dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)

        # Initialize the model, loss function, and optimizer
        input_size = X_scaled.shape[1]
        model = LiquidNeuralNetwork(input_size, config.lnn_hidden_size, 1)
        criterion = nn.BCEWithLogitsLoss()  # Suitable for binary classification
        optimizer = optim.Adam(model.parameters(), lr=config.lnn_learning_rate)

        # Training loop
        epochs = config.lnn_training_epochs
        for epoch in range(epochs):
            model.train()
            epoch_loss = 0.0
            for batch_X, batch_y in dataloader:
                optimizer.zero_grad()
                outputs = model(batch_X.half()).squeeze()  # Forward pass
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                epoch_loss += loss.item()
            avg_loss = epoch_loss / len(dataloader)
            logging.info(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")

        # Save the trained model
        torch.save(model.state_dict(), config.lnn_model_path)
        logging.info(f"LNN model trained and saved to {config.lnn_model_path}")

    except FileNotFoundError:
        logging.error("Training data file not found. Please provide a valid CSV file.")
    except Exception as e:
        logging.error(f"Error during LNN training: {e}")


if __name__ == "__main__":
    train_lnn()


// File: models\lnn\__init__.py


// File: models\utils\config.py
# File: models/utils/config.py

import yaml
from typing import List, Dict, Any


class Config:
    def __init__(self, config_path: str):
        try:
            with open(config_path, "r") as f:
                self.config: Dict[str, Any] = yaml.safe_load(f)

                # Trading Parameters
                self.symbol: str = self.config.get("symbol", "BTC_USDT")
                self.interval: str = self.config.get("interval", "Min1")
                self.timeframes: List[str] = self.config.get("timeframes", ["1m", "5m", "15m", "1h", "4h"])
                self.indicators: List[str] = self.config.get("indicators", ["price", "volume", "rsi", "macd", "fibonacci"])

                # GMN Parameters
                self.max_history_length: int = self.config.get("max_history_length", 1000)

                # LNN Parameters
                self.lnn_model_path: str = self.config.get("lnn_model_path", "models/lnn/lnn_model.pth")
                self.lnn_hidden_size: int = self.config.get("lnn_hidden_size", 64)
                self.lnn_training_epochs: int = self.config.get("lnn_training_epochs", 10)
                self.training_history_length: int = self.config.get("training_history_length", 500)
                self.lnn_learning_rate: float = self.config.get("lnn_learning_rate", 0.001)

                # Agent Parameters
                self.threshold_buy: float = self.config.get("threshold_buy", 0.7)
                self.threshold_sell: float = self.config.get("threshold_sell", 0.3)

                # Risk Management
                self.risk_parameters: Dict[str, Any] = self.config.get("risk_parameters", {})

                # Trade Execution
                self.trade_parameters: Dict[str, Any] = self.config.get("trade_parameters", {})

                # System
                self.agent_loop_delay: int = self.config.get("agent_loop_delay", 1)
                self.reconnect_delay: int = self.config.get("reconnect_delay", 5)
                self.log_level = self.config.get("log_level", "INFO")

        except FileNotFoundError:
            raise FileNotFoundError(f"Config file not found: {config_path}")
        except yaml.YAMLError as e:
            raise ValueError(f"Error parsing YAML config file: {e}")


// File: models\utils\risk_management.py
# File: models/utils/risk_management.py

import logging
from typing import Dict, Any


class RiskManager:
    def __init__(self, risk_parameters: Dict[str, Any]):
        self.max_drawdown = risk_parameters.get("max_drawdown", 0.1)
        self.max_position_size = risk_parameters.get("max_position_size", 0.05)  # Example: 5% of portfolio
        # ... (initialize other risk parameters as needed)

    def check_risk(self, current_drawdown: float, current_position: str, market_data: Dict) -> bool:
        """
        Checks if the current trade action is within risk parameters.
        Args:
            current_drawdown (float): Current drawdown ratio.
            current_position (str): Current position ('long', 'short', or None).
            market_data (Dict): Latest market data for additional risk checks.
        Returns:
            bool: True if within risk parameters, False otherwise.
        """
        if current_drawdown > self.max_drawdown:
            logging.warning(
                f"Risk check failed: Drawdown ({current_drawdown:.2f}) exceeds maximum allowed ({self.max_drawdown:.2f})."
            )
            return False

        # Example: Prevent increasing position size beyond maximum allowed
        # Implement additional risk checks based on current_position and market_data
        # For instance, limit the number of concurrent positions, check volatility, etc.

        return True


// File: models\utils\__init__.py


// File: tests\test_processed_data.py
import pytest
from unittest.mock import AsyncMock, MagicMock
from data.data_processor import DataProcessor
from data.config import Config
from data.indicator_calculations import IndicatorCalculator
from data.error_handler import ErrorHandler


@pytest.mark.asyncio
async def test_processed_data():
    """
    Tests that DataProcessor correctly processes kline data, applies indicators,
    and creates a unified feed.
    """
    # Setup mock gmn with store_data as an AsyncMock
    mock_gmn = MagicMock()
    mock_gmn.store_data = AsyncMock()

    # Setup IndicatorCalculator with mock error_handler
    mock_error_handler = MagicMock(spec=ErrorHandler)
    indicator_calculator = IndicatorCalculator(mock_error_handler)

    # Initialize Config
    config = Config()

    # Initialize DataProcessor
    processor = DataProcessor(mock_gmn, indicator_calculator, mock_error_handler, config)

    # Sample kline data for multiple timeframes
    sample_kline_data = get_sample_kline_data()

    # Process the kline data
    await processor.process_data(sample_kline_data)

    # Ensure store_data was called
    assert mock_gmn.store_data.called, "store_data was not called"

    # Get the unified_feed argument
    unified_feed = mock_gmn.store_data.call_args[0][0]

    # Validate the unified feed
    validate_unified_feed(unified_feed)


def get_sample_kline_data():
    """Generates sample kline data for 1m and 5m timeframes."""
    return [
        {
            'method': 'some_method',
            'c': 'spot@public.kline.v3.api@1m',
            'd': {
                'k': {
                    'T': 1638316800000,
                    'a': 100.0,
                    'c': 30000.0,
                    'h': 30050.0,
                    'i': '1m',
                    'l': 29950.0,
                    'o': 29975.0,
                    't': 1638316740000,
                    'v': 50.0
                }
            }
        },
        {
            'method': 'some_method',
            'c': 'spot@public.kline.v3.api@5m',
            'd': {
                'k': {
                    'T': 1638317100000,
                    'a': 150.0,
                    'c': 30050.0,
                    'h': 30100.0,
                    'i': '5m',
                    'l': 30000.0,
                    'o': 30025.0,
                    't': 1638317040000,
                    'v': 75.0
                }
            }
        },
        {
            'method': 'PONG',  # Heartbeat message
            'c': 'some_other_channel',
            'd': {}
        }
    ]


def validate_unified_feed(unified_feed):
    """Helper function to validate the structure and content of the unified feed."""
    assert '1m' in unified_feed
    assert '5m' in unified_feed

    for timeframe in ['1m', '5m']:
        assert 'price' in unified_feed[timeframe]
        assert 'volume' in unified_feed[timeframe]
        assert 'open' in unified_feed[timeframe]
        assert 'high' in unified_feed[timeframe]
        assert 'low' in unified_feed[timeframe]
        assert 'close_time' in unified_feed[timeframe]
        assert 'open_time' in unified_feed[timeframe]
        assert 'quantity' in unified_feed[timeframe]
        assert 'indicators' in unified_feed[timeframe]

        # Validate indicators
        assert 'rsi' in unified_feed[timeframe]['indicators'], "Missing RSI indicator"
        assert 'macd' in unified_feed[timeframe]['indicators'], "Missing MACD indicator"
        assert 'fibonacci' in unified_feed[timeframe]['indicators'], "Missing Fibonacci indicator"

        # Validate the contents of '1m'
        if timeframe == '1m':
            assert unified_feed['1m']['price'] == [30000.0]
            assert unified_feed['1m']['volume'] == [50.0]
            assert unified_feed['1m']['open'] == [29975.0]
            assert unified_feed['1m']['high'] == [30050.0]
            assert unified_feed['1m']['low'] == [29950.0]
            assert unified_feed['1m']['quantity'] == [100.0]
            validate_indicators(unified_feed['1m']['indicators'])

        # Validate the contents of '5m'
        if timeframe == '5m':
            assert unified_feed['5m']['price'] == [30050.0]
            assert unified_feed['5m']['volume'] == [75.0]
            assert unified_feed['5m']['open'] == [30025.0]
            assert unified_feed['5m']['high'] == [30100.0]
            assert unified_feed['5m']['low'] == [30000.0]
            assert unified_feed['5m']['quantity'] == [150.0]
            validate_indicators(unified_feed['5m']['indicators'])


def validate_indicators(indicators):
    """Helper function to validate the indicator contents."""
    assert isinstance(indicators['rsi'], list), "RSI should be a list"
    assert isinstance(indicators['macd'], dict), "MACD should be a dictionary"
    assert 'macd' in indicators['macd'], "Missing MACD line"
    assert 'macd_signal' in indicators['macd'], "Missing MACD signal line"
    assert 'macd_hist' in indicators['macd'], "Missing MACD histogram"
    assert isinstance(indicators['fibonacci'], list), "Fibonacci should be a list"


// File: tests\__init__.py


