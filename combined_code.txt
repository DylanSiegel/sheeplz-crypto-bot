**File Tree (Relevant Files Only)**
  .
    - data_transformation.py
    - feature_engineering.py
    - logging_config.py
    - main.py
// File: data_transformation.py
# data_transformation.py
import pandas as pd
import numpy as np
import torch
import pywt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler, RobustScaler
from scipy.fft import fft
import logging
from typing import Optional

# Set logging configuration
# Moved to logging_config.py
from logging_config import setup_logging

# Setup logging
setup_logging()

def calculate_fft(series: pd.Series) -> pd.DataFrame:
    """Calculates FFT features, handling NaN/infinite values."""
    series = series.dropna()
    fft_values = fft(series.values)
    return pd.DataFrame({
        'FFT_Real': np.real(fft_values),
        'FFT_Imag': np.imag(fft_values),
        'FFT_Magnitude': np.abs(fft_values),
        'FFT_Phase': np.angle(fft_values)
    })

def perform_pca(df: pd.DataFrame, n_components: int = 5, scaler_type: str = 'standard') -> pd.DataFrame:
    """Performs PCA on numerical columns with optional scaling."""
    num_cols = df.select_dtypes(include=['number']).columns  # Use 'number' for broader type selection
    if not len(num_cols):
        return pd.DataFrame()  # Return empty DataFrame if no numerical columns

    scaler = RobustScaler() if scaler_type == 'robust' else StandardScaler()
    scaled_data = scaler.fit_transform(df[num_cols])

    pca = PCA(n_components=n_components)
    pca_result = pca.fit_transform(scaled_data)
    
    return pd.DataFrame(pca_result, columns=[f'PCA_{i+1}' for i in range(n_components)])


// File: feature_engineering.py
# feature_engineering.py
import ta
from functools import lru_cache
import logging
import pandas as pd
import torch
import pywt
from sklearn.manifold import TSNE
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans
import numpy as np
from typing import Dict

# logging config moved to logging_config.py
from logging_config import setup_logging

# Setup logging
setup_logging()

indicator_functions = {  # Use lowercase keys for consistency
    "rsi": {"func": ta.momentum.RSIIndicator, "params": {"window": 14}},
    "macd": {"func": ta.trend.MACD, "params": {}},
    # ... (rest of the indicators)
}

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

@lru_cache(maxsize=None)
def calculate_wavelet_features(series: np.ndarray, wavelet: str = 'db4', level: int = 4) -> np.ndarray:
    """Calculates wavelet features and returns a NumPy array."""
    try:
        tensor = torch.tensor(series, device=device)
        coeffs = pywt.wavedec(tensor.cpu().numpy(), wavelet, level=level)  # Move back to CPU for pywt
        return np.concatenate(coeffs)
    except Exception as e:
        logging.error(f"Error in calculate_wavelet_features: {e}", exc_info=True)
        return np.array([])

def calculate_indicators(df: pd.DataFrame) -> pd.DataFrame:
    """Calculates technical indicators and their rolling statistics."""

    close = df['Close']
    high = df['High']
    low = df['Low']
    volume = df['Volume']

    calculated_indicators = {}
    for name, indicator_data in indicator_functions.items():
        indicator = indicator_data["func"](high=high, low=low, close=close, volume=volume, **indicator_data["params"])
        calculated_indicators[name] = getattr(indicator, name, None)  # Simplified attribute access

    if calculated_indicators["rsi"] is not None:
        df['RSI'] = calculated_indicators['rsi']
    if calculated_indicators["macd"] is not None:
        df['MACD'] = calculated_indicators["macd"].macd()  # added .macd()
    # Add other indicators if not None

    # Rolling Statistics (optimized)
    core_features = [
        feature for feature in [
            'Close', 'Volume', 'RSI', 'MACD', 'BollingerBands', 'ATR', 'OBV',
            'StochasticOscillator', 'ADX', 'CCI', 'EMA', 'Cluster', 'TSNE1', 'TSNE2'
        ] if feature in df.columns
    ]  # adding conditional to prevent errors

    for window in [3, 7, 14, 21]:
        rolled = df[core_features].rolling(window)
        df = df.join(rolled.agg(['mean', 'std', 'min', 'max']).add_prefix(f'rolling{window}_'))  # More efficient rolling

    scaler = MinMaxScaler()
    df[core_features] = scaler.fit_transform(df[core_features].fillna(0))

    return df


// File: logging_config.py
# logging_config.py

import logging

def setup_logging(log_file: str = 'data_processing.log') -> None:
    """
    Configures the logging settings for the application.
    
    Args:
        log_file (str): Path to the log file. Defaults to 'data_processing.log'.
    """
    logging.basicConfig(
        filename=log_file,
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        filemode='a'  # Append mode to avoid overwriting existing logs
    )


// File: main.py
# main.py
import time
import logging
from logging_config import setup_logging  # Import the setup_logging function
import glob
import os
import dask.dataframe as dd
from dask.distributed import Client, LocalCluster
import pandas as pd  # Import pandas
from data_transformation import calculate_fft, perform_pca
from feature_engineering import calculate_indicators



def process_chunk(chunk):
    try:
        # Use dask.dataframe to read CSV files
        df = dd.read_csv(chunk).compute()  # compute to pandas df
        df = pd.DataFrame(df)  # Convert to DataFrame

        # Calculate technical indicators
        df = calculate_indicators(df.to_pandas())  # Convert back to pandas for feature engineering
        df = pd.DataFrame(df)  # Convert back to DataFrame

        # Perform FFT
        fft_features = calculate_fft(df['Close'])
        df = df.join(fft_features)

        # Perform PCA
        pca_result = perform_pca(df, n_components=5)
        df = df.join(pd.DataFrame(pca_result))

        return df
    except FileNotFoundError:
        logging.error(f"Error: One or more files in the chunk not found: {chunk}")
        return pd.DataFrame()  # Return empty DataFrame in case of error
    except Exception as e:
        logging.error(f"Error processing chunk {chunk}: {e}")
        return pd.DataFrame()


def optimize_dataframe(df):
    # Example optimization: drop unnecessary columns and convert data types
    df = df.dropna()  # Drop rows with NaN values
    df = df.astype({col: 'float32' for col in df.columns if df[col].dtype == 'float64'})  # Convert float64 to float32
    return df


def validate_dataframe(df):
    # Example validation: ensure no NaN values and required columns exist
    required_columns = ['Close', 'Volume', 'RSI', 'MACD']
    for col in required_columns:
        if col not in df.columns or df[col].isnull().any():
            logging.error(f"Validation failed: Missing or NaN values in column {col}")
            return False
    return True


def main():
    setup_logging()
    start_time = time.time()

    try:
        cluster = LocalCluster(n_workers=min(8, os.cpu_count()), threads_per_worker=2, processes=True, memory_limit='16GB')
        client = Client(cluster)  # Connect to the cluster
        logging.info("Dask cluster initialized.")

        RAW_DATA_PATH = r"C:\Users\dylan\Desktop\DATA-LAKE\data\raw\*.csv"
        PROCESSED_DATA_PATH = r"C:\Users\dylan\Desktop\DATA-LAKE\data\processed\btc_merged_advanced_features.parquet"

        file_paths = glob.glob(RAW_DATA_PATH)
        output_path = PROCESSED_DATA_PATH

        # Process and write data in chunks
        chunk_size = 1000  # Increased chunk size for better efficiency
        for i in range(0, len(file_paths), chunk_size):
            chunk = file_paths[i:i + chunk_size]

            # Using dask.dataframe for efficient CSV reading
            futures = client.map(process_chunk, chunk)
            dfs = client.gather(futures)
            dfs = [df for df in dfs if not df.empty]  # Prevent errors
            if len(dfs) > 0:  # Prevent errors
                combined = pd.concat(dfs, ignore_index=True)

                # Optimize DataFrame
                combined = optimize_dataframe(combined)

                # Validate DataFrame
                if validate_dataframe(combined.to_pandas()):
                    combined.to_parquet(output_path, engine='pyarrow', compression='snappy', append=os.path.exists(output_path))
                    logging.info(f"Processed and written chunk {i // chunk_size + 1} of {len(file_paths) // chunk_size + 1}")
                else:
                    logging.error(f"Validation failed for chunk {i // chunk_size + 1}")

    except Exception as e:
        logging.error(f"Error in main processing loop: {e}", exc_info=True)
    finally:
        try:
            client.close()
            cluster.close()  # Close the cluster as well.
            logging.info("Dask cluster and client closed.")
        except:
            logging.warning("Dask client/cluster was not initialized or already closed.")

    end_time = time.time()
    elapsed_time = end_time - start_time
    logging.info(f"Total processing time: {elapsed_time:.2f} seconds")


if __name__ == "__main__":
    main()


