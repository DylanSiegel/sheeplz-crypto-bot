**File Tree (Relevant Files Only)**
  src
    - main.py
  src\config
    - config.py
    - __init__.py
  src\data
    - base_preprocessor.py
    - kline_preprocessor.py
    - market_preprocessor.py
    - __init__.py
  src\data\raw
    - btc_usdt_1m_klines.csv
  src\pipeline
    - augmenters.py
    - datasets.py
    - handlers.py
    - market_indicators.py
    - metrics.py
    - normalizers.py
    - pipeline.py
    - __init__.py
  src\shared
    - exceptions.py
    - types.py
  src\utils
    - logging.py
    - system_monitor.py
    - __init__.py
// File: src\main.py
# src/main.py

from pathlib import Path
import torch
from datetime import datetime
import sys
from typing import Tuple
import pandas as pd

from config.config import TradingConfig, TargetType, NormalizationType
from pipeline.pipeline import TradingDataPipeline
from pipeline.handlers import DirectionTargetCalculator
from utils.logging import custom_logger
from pipeline.metrics import PipelineMetrics
from pipeline.market_indicators import TechnicalIndicators, IndicatorConfig
from data.kline_preprocessor import KlinePreprocessor
from utils.system_monitor import SystemMonitor

def create_config() -> TradingConfig:
    """Create and initialize configuration"""
    return TradingConfig(
        # Data Parameters
        sequence_length=60,            # 1-hour lookback
        prediction_horizon=5,          # 5-minute prediction
        batch_size=4096,              # Optimized for 8GB VRAM
        
        # Hardware Optimization
        num_workers=6,                # Half of Ryzen 9 7900X cores
        pin_memory=True,
        
        # Processing Parameters
        outlier_std_threshold=4.0,
        volume_percentile_threshold=0.9995,
        use_robust_scaling=True,
        
        # Target Configuration
        target_type=TargetType.DIRECTION,
        target_normalization=NormalizationType.ROBUST
    )

def process_and_save_data(
    config: TradingConfig,
    input_path: Path,
    output_path: Path,
    metrics: PipelineMetrics
) -> Tuple[torch.Tensor, torch.Tensor]:
    """Process kline data and save results"""
    system_monitor = SystemMonitor()
    
    try:
        # Initialize preprocessor
        preprocessor = KlinePreprocessor(config)
        
        # Load and process data
        custom_logger.logger.info(f"Loading data from {input_path}")
        raw_data = pd.read_parquet(input_path) if input_path.suffix == '.parquet' else pd.read_csv(input_path)
        
        # Process features and targets
        custom_logger.logger.info("Processing kline data...")
        features, targets = preprocessor.process_klines(raw_data)
        
        # Log processing metrics
        current_metrics = system_monitor.get_metrics()
        metrics.update('processing_time', current_metrics.cpu_percent)
        metrics.update('memory_usage', current_metrics.memory_percent)
        
        custom_logger.log_metrics({
            'cpu_usage': current_metrics.cpu_percent,
            'memory_usage': current_metrics.memory_percent,
            'gpu_utilization': current_metrics.gpu_utilization,
            'data_samples': len(features)
        }, step=0)
        
        # Save processed data
        custom_logger.logger.info(f"Saving processed data to {output_path}")
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        torch.save({
            'features': features,
            'targets': targets,
            'config': config.to_dict(),
            'processing_timestamp': datetime.now().isoformat()
        }, output_path)
        
        # Log summary statistics
        custom_logger.logger.info(f"Processed {len(features):,} samples")
        custom_logger.logger.info(f"Feature shape: {features.shape}")
        custom_logger.logger.info(f"Target shape: {targets.shape}")
        
        return features, targets
        
    except Exception as e:
        custom_logger.logger.error(f"Error processing data: {str(e)}", exception=True)
        raise

def main():
    try:
        # Initialize configuration
        config = create_config()
        custom_logger.logger.info("Configuration initialized")
        
        # Initialize metrics tracker
        metrics = PipelineMetrics()
        
        # Define paths
        input_path = Path("src/data/raw/btc_usdt_1m_klines.parquet")
        output_path = Path("src/data/processed/kline_features.pt")
        
        # Process and save data
        features, targets = process_and_save_data(
            config,
            input_path,
            output_path,
            metrics
        )
        
        # Log final metrics
        metrics.log_summary()
        custom_logger.logger.info("Data processing completed successfully")
        
        # Optional: Create training/validation splits
        if features is not None and targets is not None:
            split_idx = int(len(features) * 0.8)
            
            split_data = {
                'train_features': features[:split_idx],
                'train_targets': targets[:split_idx],
                'val_features': features[split_idx:],
                'val_targets': targets[split_idx:],
                'config': config.to_dict()
            }
            
            split_path = output_path.parent / 'train_val_split.pt'
            torch.save(split_data, split_path)
            
            custom_logger.logger.info(
                f"Train/validation split saved - "
                f"Train: {len(split_data['train_features']):,} samples, "
                f"Val: {len(split_data['val_features']):,} samples"
            )
        
    except Exception as e:
        custom_logger.logger.error(f"Pipeline failed: {str(e)}", exception=True)
        sys.exit(1)

if __name__ == "__main__":
    main()

// File: src\config\config.py
# src/config/config.py

from __future__ import annotations
from typing import Any, Dict, List, Optional, Tuple, Union, TypeVar, Set
from dataclasses import dataclass, field, fields, asdict
from enum import Enum
from pathlib import Path
import json
import yaml

class TargetType(Enum):
    """Types of prediction targets supported by the system."""
    DIRECTION = "direction"  # Binary classification (up/down)
    PRICE = "price"         # Direct price prediction
    VOLATILITY = "volatility"  # Volatility prediction
    CUSTOM = "custom"       # Custom target type

class NormalizationType(Enum):
    """Supported data normalization methods."""
    STANDARD = "standard"  # Zero mean, unit variance
    MINMAX = "minmax"     # Scale to [0,1] range
    ROBUST = "robust"     # Using median and IQR
    NONE = "none"         # No normalization

@dataclass
class TechnicalIndicatorParams:
    """Parameters for technical indicators."""
    rsi: Dict[str, List[int]] = field(default_factory=lambda: {'periods': [6, 14, 24]})
    ema: Dict[str, List[int]] = field(default_factory=lambda: {'periods': [5, 10, 20, 60, 120]})
    macd: Dict[str, int] = field(default_factory=lambda: {'fast': 12, 'slow': 26, 'signal': 9})
    bb: Dict[str, Union[int, float]] = field(default_factory=lambda: {'period': 20, 'std': 2.0})
    atr: Dict[str, int] = field(default_factory=lambda: {'period': 14})
    volatility: Dict[str, int] = field(default_factory=lambda: {'period': 20})
    follow_line: Dict[str, Union[int, float, bool]] = field(default_factory=lambda: {
        'atr_period': 5,
        'bb_period': 21,
        'bb_deviation': 1.00,
        'use_atr_filter': True
    })

@dataclass
class TradingConfig:
    """Configuration for the trading data pipeline.
    
    Attributes:
        sequence_length: Number of time steps in the input sequence
        prediction_horizon: List of future time steps to predict
        batch_size: Number of samples per batch
        num_workers: Number of data loading worker processes
        pin_memory: Whether to pin memory for GPU transfer
        prefetch_factor: Number of batches to prefetch
        price_features: List of price-related features to use
        volume_features: List of volume-related features to use
        technical_indicators_config: Configuration for technical indicators
        outlier_std_threshold: Standard deviation threshold for outlier removal
        volume_percentile_threshold: Percentile threshold for volume clipping
        normalization_type: Type of normalization to apply
        learning_rate: Initial learning rate
        weight_decay: L2 regularization factor
        epsilon: Small value for numerical stability
        gradient_clip_val: Maximum gradient norm
        use_augmentation: Whether to use data augmentation
        noise_level: Standard deviation of Gaussian noise
        jitter_ratio: Ratio of time steps to jitter
    """
    
    # Core Parameters
    sequence_length: int = field(default=60)
    prediction_horizon: List[int] = field(default_factory=lambda: [5])
    batch_size: int = field(default=4096)

    # Hardware Optimization
    num_workers: int = field(default=6)
    pin_memory: bool = field(default=True)
    prefetch_factor: int = field(default=2)

    # Feature Groups
    price_features: List[str] = field(default_factory=lambda: [
        'open', 'high', 'low', 'close'
    ])
    volume_features: List[str] = field(default_factory=lambda: [
        'volume', 'quote_asset_volume',
        'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume'
    ])
    technical_indicators_config: TechnicalIndicatorParams = field(
        default_factory=TechnicalIndicatorParams
    )

    # Data Preprocessing
    outlier_std_threshold: float = field(default=4.0)
    volume_percentile_threshold: float = field(default=0.9995)
    normalization_type: NormalizationType = field(default=NormalizationType.ROBUST)

    # Training Parameters
    learning_rate: float = field(default=1e-4)
    weight_decay: float = field(default=1e-6)
    epsilon: float = field(default=1e-8)
    gradient_clip_val: float = field(default=1.0)

    # Data Augmentation
    use_augmentation: bool = field(default=True)
    noise_level: float = field(default=0.01)
    jitter_ratio: float = field(default=0.1)

    # Derived Attributes
    technical_indicators: List[str] = field(init=False)
    input_dim: int = field(init=False)

    def __post_init__(self) -> None:
        """Validates configuration and initializes derived attributes."""
        self._validate_config()
        self._derive_attributes()

    def _validate_config(self) -> None:
        """Validates configuration parameters."""
        # Core parameter validation
        if self.sequence_length <= 0:
            raise ValueError("sequence_length must be positive")
        if not self.prediction_horizon:
            raise ValueError("prediction_horizon cannot be empty")
        if not all(h > 0 for h in self.prediction_horizon):
            raise ValueError("All prediction horizons must be positive")
        if self.batch_size <= 0:
            raise ValueError("batch_size must be positive")

        # Hardware parameter validation
        if self.num_workers < 0:
            raise ValueError("num_workers cannot be negative")
        if self.prefetch_factor <= 0:
            raise ValueError("prefetch_factor must be positive")

        # Feature validation
        if not self.price_features:
            raise ValueError("price_features cannot be empty")
        if not self.volume_features:
            raise ValueError("volume_features cannot be empty")
        
        # Check for feature name uniqueness
        all_features = set(self.price_features) | set(self.volume_features)
        if len(all_features) < len(self.price_features) + len(self.volume_features):
            raise ValueError("Duplicate feature names found")

        # Data preprocessing validation
        if self.outlier_std_threshold <= 0:
            raise ValueError("outlier_std_threshold must be positive")
        if not 0 < self.volume_percentile_threshold <= 1:
            raise ValueError("volume_percentile_threshold must be between 0 and 1")

        # Training parameter validation
        if self.learning_rate <= 0:
            raise ValueError("learning_rate must be positive")
        if self.weight_decay < 0:
            raise ValueError("weight_decay cannot be negative")
        if self.epsilon <= 0:
            raise ValueError("epsilon must be positive")
        if self.gradient_clip_val <= 0:
            raise ValueError("gradient_clip_val must be positive")

        # Augmentation parameter validation
        if not 0 <= self.noise_level <= 1:
            raise ValueError("noise_level must be between 0 and 1")
        if not 0 <= self.jitter_ratio <= 1:
            raise ValueError("jitter_ratio must be between 0 and 1")

    def _derive_attributes(self) -> None:
        """Derives technical indicators and input dimension."""
        self.technical_indicators = self._expand_indicator_names()
        self.input_dim = (
            len(self.price_features) +
            len(self.volume_features) +
            len(self.technical_indicators)
        )

    def _expand_indicator_names(self) -> List[str]:
        """Expands technical indicator configurations into feature names."""
        indicators = []
        
        # RSI indicators
        for period in self.technical_indicators_config.rsi['periods']:
            indicators.append(f"rsi_{period}")
        
        # EMA indicators
        for period in self.technical_indicators_config.ema['periods']:
            indicators.append(f"ema_{period}")
        
        # MACD components
        indicators.extend([
            'macd_line',
            'macd_signal',
            'macd_histogram'
        ])
        
        # Bollinger Bands components
        indicators.extend([
            'bb_upper',
            'bb_middle',
            'bb_lower',
            'bb_bandwidth'
        ])
        
        # ATR and Volatility
        indicators.extend([
            'atr',
            'volatility'
        ])
        
        # Follow Line components
        indicators.extend([
            'follow_line',
            'follow_line_trend'
        ])
        
        return indicators

    @classmethod
    def from_dict(cls, config_dict: Dict[str, Any]) -> TradingConfig:
        """Creates a TradingConfig instance from a dictionary."""
        # Convert string enum values to proper enum types
        if 'normalization_type' in config_dict:
            config_dict['normalization_type'] = NormalizationType[config_dict['normalization_type'].upper()]
        
        # Convert technical indicators config
        if 'technical_indicators_config' in config_dict:
            config_dict['technical_indicators_config'] = TechnicalIndicatorParams(
                **config_dict['technical_indicators_config']
            )
        
        return cls(**config_dict)

    @classmethod
    def from_file(cls, file_path: Union[str, Path]) -> TradingConfig:
        """Loads configuration from a JSON or YAML file."""
        file_path = Path(file_path)
        
        with open(file_path, 'r') as f:
            if file_path.suffix == '.json':
                config_dict = json.load(f)
            elif file_path.suffix in {'.yml', '.yaml'}:
                config_dict = yaml.safe_load(f)
            else:
                raise ValueError(f"Unsupported file format: {file_path.suffix}")
        
        return cls.from_dict(config_dict)

    def to_dict(self) -> Dict[str, Any]:
        """Converts configuration to a dictionary."""
        config_dict = asdict(self)
        
        # Convert enum values to strings
        config_dict['normalization_type'] = self.normalization_type.value
        
        # Remove derived attributes
        config_dict.pop('technical_indicators', None)
        config_dict.pop('input_dim', None)
        
        return config_dict

    def save(self, file_path: Union[str, Path]) -> None:
        """Saves configuration to a JSON or YAML file."""
        file_path = Path(file_path)
        config_dict = self.to_dict()
        
        with open(file_path, 'w') as f:
            if file_path.suffix == '.json':
                json.dump(config_dict, f, indent=2)
            elif file_path.suffix in {'.yml', '.yaml'}:
                yaml.dump(config_dict, f, default_flow_style=False)
            else:
                raise ValueError(f"Unsupported file format: {file_path.suffix}")

// File: src\config\__init__.py


// File: src\data\base_preprocessor.py
# src/data/base_preprocessor.py

from abc import ABC, abstractmethod
import pandas as pd
import numpy as np
from typing import Dict, Any, Optional
from dataclasses import dataclass
import logging

from config.config import TradingConfig
from shared.exceptions import ValidationError, ProcessingError
from utils.logging import custom_logger

@dataclass
class PreprocessingStats:
    """Statistics from data preprocessing operations."""
    total_rows: int = 0
    memory_usage_mb: float = 0.0
    processing_time_sec: float = 0.0
    outliers_removed: Dict[str, int] = field(default_factory=dict)
    invalid_values_fixed: int = 0
    features_generated: int = 0

class BasePreprocessor(ABC):
    """Base class for data preprocessing."""
    
    def __init__(self, config: TradingConfig):
        self.config = config
        self.logger = custom_logger.logger
        self.stats = PreprocessingStats()
    
    def clean_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """Common data cleaning operations."""
        clean_data = data.copy()
        
        # Handle outliers
        self._remove_outliers(clean_data)
        
        # Handle invalid values
        self._handle_invalid_values(clean_data)
        
        return clean_data
    
    def _remove_outliers(self, data: pd.DataFrame) -> None:
        """Remove outliers using robust statistics."""
        for col in data.select_dtypes(include=[np.number]).columns:
            rolling = data[col].rolling(window=24, center=True, min_periods=1)
            q_high = rolling.quantile(0.9995)
            q_low = rolling.quantile(0.0005)
            
            outliers = ((data[col] > q_high) | (data[col] < q_low)).sum()
            self.stats.outliers_removed[col] = outliers
            
            data[col] = np.where(
                data[col] > q_high,
                q_high,
                np.where(data[col] < q_low, q_low, data[col])
            )
    
    def _handle_invalid_values(self, data: pd.DataFrame) -> None:
        """Handle missing and invalid values."""
        numeric_cols = data.select_dtypes(include=[np.number]).columns
        invalid_mask = data[numeric_cols].isna() | np.isinf(data[numeric_cols])
        self.stats.invalid_values_fixed = invalid_mask.sum().sum()
        
        # Replace with appropriate values
        for col in numeric_cols:
            data[col] = data[col].fillna(method='ffill').fillna(0)
            data[col] = np.nan_to_num(data[col], nan=0.0, posinf=None, neginf=None)
    
    @abstractmethod
    def validate_data(self, data: pd.DataFrame) -> None:
        """Validate input data requirements."""
        pass

// File: src\data\kline_preprocessor.py
# src/data/kline_preprocessor.py

import torch
from typing import Dict, List

@dataclass
class ProcessedKlines:
    """Container for processed kline data."""
    features: torch.Tensor
    targets: Dict[int, torch.Tensor]
    feature_names: List[str]
    target_type: TargetType
    stats: PreprocessingStats

class KlinePreprocessor(BasePreprocessor):
    """Processes OHLCV data for ML model input."""
    
    def __init__(
        self,
        config: TradingConfig,
        technical_indicators: Optional[TechnicalIndicators] = None
    ):
        super().__init__(config)
        self.technical_indicators = technical_indicators or TechnicalIndicators(
            config.technical_indicators_config
        )
    
    def process_klines(
        self,
        data: pd.DataFrame,
        validate: bool = True
    ) -> ProcessedKlines:
        """Process klines into ML-ready format."""
        if validate:
            self.validate_data(data)
        
        # Clean data using base class method
        clean_data = self.clean_data(data)
        
        # Generate features
        feature_data = self._generate_features(clean_data)
        
        # Calculate targets
        targets = self._calculate_targets(clean_data['close'])
        
        # Convert to tensors
        features = self._prepare_features(feature_data)
        
        return ProcessedKlines(
            features=features,
            targets=targets,
            feature_names=self.config.technical_indicators,
            target_type=self.config.target_type,
            stats=self.stats
        )
    
    def validate_data(self, data: pd.DataFrame) -> None:
        """Validate OHLCV data requirements."""
        # Implementation for OHLCV-specific validation
        pass
    
    def _validate_ohlc(self, data: pd.DataFrame) -> None:
        """Validate and fix OHLC relationships."""
        # OHLCV-specific validation logic
        pass

// File: src\data\market_preprocessor.py
# src/data/market_preprocessor.py

from pathlib import Path
from typing import Union, Optional

class MarketDataPreprocessor(BasePreprocessor):
    """Handles loading and storing large market datasets."""
    
    def __init__(
        self,
        config: TradingConfig,
        data_path: Optional[Path] = None
    ):
        super().__init__(config)
        self.data_path = data_path
        self._setup_chunking()
    
    def load_and_process(
        self,
        output_path: Optional[Path] = None
    ) -> pd.DataFrame:
        """Load, clean, and optionally save market data."""
        # Implementation focusing on efficient data loading and storage
        pass
    
    def validate_data(self, data: pd.DataFrame) -> None:
        """Validate market data requirements."""
        pass

// File: src\data\__init__.py
# File: src/data/__init__.py

from .base_preprocessor import MarketDataPreprocessor
from .feature_transformer import KlineFeatureTransformer, KlineTransformerConfig
from .market_indicators import TechnicalIndicators, IndicatorConfig


// File: src\data\raw\btc_usdt_1m_klines.csv
// Snippet:
                open_time    open    high     low   close  volume                       close_time  quote_asset_volume  number_of_trades  taker_buy_base_asset_volume  taker_buy_quote_asset_volume  ignore
2020-01-01 00:00:00+00:00 7189.43 7190.52 7177.00 7182.44 246.092 2020-01-01 00:00:59.999000+00:00        1.767430e+06               336                       46.630                  334813.19820       0
2020-01-01 00:01:00+00:00 7182.43 7182.44 7178.75 7179.01  70.909 2020-01-01 00:01:59.999000+00:00        5.091458e+05               140                       32.597                  234063.27884       0
2020-01-01 00:02:00+00:00 7179.01 7179.01 7175.25 7177.93  99.420 2020-01-01 00:02:59.999000+00:00        7.135396e+05               148                       16.311                  117066.92118       0
2020-01-01 00:03:00+00:00 7177.77 7182.60 7177.00 7181.11  69.330 2020-01-01 00:03:59.999000+00:00        4.977934e+05               104                       43.723                  313920.02981       0
2020-01-01 00:04:00+00:00 7179.10 7179.10 7172.94 7175.25  97.368 2020-01-01 00:04:59.999000+00:00        6.986274e+05               193                       36.616                  262734.68999       0

// File: src\pipeline\augmenters.py
# File: src/pipeline/augmenters.py

import torch
import logging
from config.config import TradingConfig


class DataAugmenter:
    """Implements various data augmentation strategies."""

    def __init__(self, config: TradingConfig):
        """Initializes the data augmenter.

        Args:
            config: Trading configuration containing augmentation parameters
        """
        self.config = config
        self.logger = logging.getLogger(__name__)

    def augment(self, sequence: torch.Tensor) -> torch.Tensor:
        """Applies configured augmentations to the input sequence.

        Args:
            sequence: Input tensor to augment

        Returns:
            Augmented tensor
        """
        if not self.config.use_augmentation:
            return sequence

        augmented = sequence.clone()

        # Apply Gaussian noise
        if self.config.noise_level > 0:
            noise = torch.randn_like(augmented) * self.config.noise_level
            augmented += noise

        # Apply time jittering
        if self.config.jitter_ratio > 0:
            augmented = self._apply_time_jitter(augmented)

        return augmented

    def _apply_time_jitter(self, sequence: torch.Tensor) -> torch.Tensor:
        """Applies time jittering to the sequence.

        Args:
            sequence: Input tensor to jitter

        Returns:
            Time jittered tensor
        """
        seq_len = sequence.size(0)
        num_jitter = int(seq_len * self.config.jitter_ratio)

        if num_jitter > 0:
            jitter_idx = torch.randperm(seq_len)[:num_jitter]
            augmented_sequence = sequence.clone()
            augmented_sequence[jitter_idx] = sequence[jitter_idx].roll(1, dims=0)
            self.logger.debug(f"Applied time jittering on indices: {jitter_idx.tolist()}")
            return augmented_sequence

        return sequence


// File: src\pipeline\datasets.py
# File: src/pipeline/datasets.py

import torch
from torch.utils.data import Dataset
from typing import Dict, List, Optional, Tuple, Union
from pipeline.handlers import TargetCalculator
from config.config import TradingConfig
from pipeline.normalizers import FeatureNormalizer
from pipeline.augmenters import DataAugmenter
import logging

from data.base_preprocessor import ValidationError


def validate_tensor_shape(tensor: torch.Tensor, expected_shape: Tuple[Optional[int], ...], name: str) -> None:
    """Validates tensor shape against expected shape.

    Args:
        tensor: Tensor to validate
        expected_shape: Expected shape (None for dynamic dimensions)
        name: Name of the tensor for error messages

    Raises:
        ValidationError: If tensor shape doesn't match expected shape
    """
    if len(tensor.shape) != len(expected_shape):
        raise ValidationError(
            f"Expected {name} to have {len(expected_shape)} dimensions, "
            f"got {len(tensor.shape)}"
        )

    for i, (actual, expected) in enumerate(zip(tensor.shape, expected_shape)):
        if expected is not None and actual != expected:
            raise ValidationError(
                f"Expected {name} dimension {i} to be {expected}, got {actual}"
            )


class SequencedMarketDataset(Dataset):
    """Dataset for creating sequences from market data."""

    def __init__(
        self,
        data: pd.DataFrame,
        config: TradingConfig,
        feature_normalizer: FeatureNormalizer,
        target_calculator: TargetCalculator,
        data_augmenter: Optional[DataAugmenter] = None,
        is_training: bool = True
    ):
        """Initializes the dataset.

        Args:
            data: Input DataFrame containing market data
            config: Trading configuration
            feature_normalizer: Normalizer for features
            target_calculator: Calculator for targets
            data_augmenter: Optional augmenter for training data
            is_training: Whether this is a training dataset
        """
        self.config = config
        self.normalizer = feature_normalizer
        self.target_calculator = target_calculator
        self.augmenter = data_augmenter
        self.is_training = is_training

        self.features = self._prepare_features(data)
        self.sequences, self.targets = self._create_sequences()

        # Validate shapes
        self._validate_shapes()

    def _prepare_features(self, data: pd.DataFrame) -> torch.Tensor:
        """Prepares and normalizes features from raw data.

        Args:
            data: Input DataFrame

        Returns:
            Normalized feature tensor
        """
        # Extract relevant features
        feature_data = data[self.config.feature_keys + self.config.technical_indicators].values
        features = torch.tensor(feature_data, dtype=torch.float32)

        # Transform features using already fitted normalizer
        normalized_features = self.normalizer.transform(features)

        return normalized_features

    def _create_sequences(self) -> Tuple[torch.Tensor, torch.Tensor]:
        """Creates sequences and corresponding targets.

        Returns:
            Tuple of (sequences, targets) tensors
        """
        total_sequences = len(self.features) - self.config.sequence_length - self.config.prediction_horizon + 1
        sequences = []
        targets = []

        for i in range(total_sequences):
            # Extract sequence
            seq = self.features[i:i + self.config.sequence_length]

            # Apply augmentation if training
            if self.is_training and self.augmenter is not None:
                seq = self.augmenter.augment(seq)

            # Calculate target
            target = self.target_calculator(
                self.features,
                i + self.config.sequence_length - 1,
                self.config.prediction_horizon
            )

            sequences.append(seq)
            targets.append(target)

        return torch.stack(sequences), torch.stack(targets)

    def _validate_shapes(self) -> None:
        """Validates shapes of sequences and targets."""
        expected_seq_shape = (None, self.config.sequence_length, self.config.input_dim)
        expected_target_shape = (None, self.target_calculator.get_output_dim())

        validate_tensor_shape(self.sequences, expected_seq_shape, "sequences")
        validate_tensor_shape(self.targets, expected_target_shape, "targets")

    def __len__(self) -> int:
        """Returns the number of sequences in the dataset."""
        return len(self.sequences)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """Returns a sequence and target pair.

        Args:
            idx: Index of the sequence to retrieve

        Returns:
            Tuple of (sequence, target) tensors
        """
        return self.sequences[idx], self.targets[idx]


// File: src\pipeline\handlers.py
# File: src/pipeline/handlers.py

from abc import ABC, abstractmethod
import torch
from typing import Dict, Any, Tuple, List, Optional
import logging
from datetime import datetime, timezone
import json
import asyncio
import websockets
import queue
from dataclasses import dataclass
import itertools
from concurrent.futures import ThreadPoolExecutor

from pipeline.normalizers import FeatureNormalizer
from pipeline.augmenters import DataAugmenter
from config.config import TradingConfig, TargetType
from data.base_preprocessor import ProcessingError
from pipeline.market_indicators import TechnicalIndicators


class TargetCalculator(ABC):
    """Abstract base class for target calculation strategies."""

    @abstractmethod
    def __call__(self, data: torch.Tensor, index: int, horizon: int) -> torch.Tensor:
        """Calculates the target value for a given index and horizon.

        Args:
            data: Input tensor containing feature data
            index: Current time step index
            horizon: Number of steps ahead to predict

        Returns:
            Tensor containing the calculated target value
        """
        pass

    @abstractmethod
    def get_output_dim(self) -> int:
        """Returns the dimension of the target output."""
        pass

    @abstractmethod
    def get_target_type(self) -> TargetType:
        """Returns the type of target this calculator produces."""
        pass


class DirectionTargetCalculator(TargetCalculator):
    """Calculates directional movement as target."""

    def __init__(self):
        self.target_type = TargetType.DIRECTION

    def __call__(self, data: torch.Tensor, index: int, horizon: int) -> torch.Tensor:
        """Calculates direction: 1 if price increases, 0 otherwise."""
        current_price = data[index, 3]  # Assuming 'close' is at index 3
        future_price = data[index + horizon, 3]
        direction = 1.0 if future_price > current_price else 0.0
        return torch.tensor(direction, dtype=torch.float32)

    def get_output_dim(self) -> int:
        return 1

    def get_target_type(self) -> TargetType:
        return self.target_type


@dataclass
class RealTimeDataHandler:
    """Handles real-time market data processing and buffering."""

    config: TradingConfig
    normalizer: FeatureNormalizer
    target_calculator: TargetCalculator
    feature_extractor: Any  # Replace with actual type if available
    buffer_size: int = 300
    max_reconnect_attempts: int = 5

    def __post_init__(self):
        """Initializes buffers and queues."""
        self.feature_buffer = FeatureBuffer(self.buffer_size)
        self.sequence_queue = queue.Queue(maxsize=self.config.batch_size * 2)

        # Initialize state
        self._is_running = False
        self._websocket = None
        self._executor = ThreadPoolExecutor(max_workers=2)
        self.logger = logging.getLogger(__name__)

    async def start(self, uri: str) -> None:
        """Starts the real-time data handling process.

        Args:
            uri: WebSocket URI to connect to
        """
        self._is_running = True
        reconnect_attempts = 0

        while self._is_running and reconnect_attempts < self.max_reconnect_attempts:
            try:
                async with websockets.connect(uri) as websocket:
                    self._websocket = websocket
                    reconnect_attempts = 0  # Reset counter on successful connection
                    self.logger.info(f"Connected to {uri}")

                    while self._is_running:
                        message = await websocket.recv()
                        await self._process_message(message)

            except websockets.exceptions.ConnectionClosed:
                reconnect_attempts += 1
                wait_time = min(2 ** reconnect_attempts, 60)  # Exponential backoff
                self.logger.warning(
                    f"Connection lost. Retrying in {wait_time}s "
                    f"(attempt {reconnect_attempts}/{self.max_reconnect_attempts})"
                )
                await asyncio.sleep(wait_time)

            except Exception as e:
                self.logger.error(f"Error in WebSocket connection: {str(e)}")
                break

        self._is_running = False
        self.logger.info("Real-time data handler stopped")

    async def _process_message(self, message: str) -> None:
        """Processes incoming WebSocket messages.

        Args:
            message: Raw WebSocket message
        """
        loop = asyncio.get_event_loop()
        try:
            data = json.loads(message)

            # Extract features asynchronously to prevent blocking
            features = await loop.run_in_executor(
                self._executor,
                self.feature_extractor.extract_features,
                data
            )

            # Handle missing timestamps by extrapolating from previous data
            if 'timestamp' not in data or data['timestamp'] is None:
                if self.feature_buffer.buffer:
                    last_timestamp = self.feature_buffer.buffer[-1].get('timestamp', datetime.now(timezone.utc).timestamp() * 1000)
                    features['timestamp'] = last_timestamp + 60000  # Add 1 minute in ms
                else:
                    features['timestamp'] = datetime.now(timezone.utc).timestamp() * 1000

            # Convert to tensor
            feature_tensor = torch.tensor(features.values, dtype=torch.float32)

            # Normalize features asynchronously
            normalized_features = await loop.run_in_executor(
                self._executor,
                self.normalizer.transform,
                feature_tensor.unsqueeze(0)
            )
            normalized_features = normalized_features.squeeze(0)

            # Add to buffer
            self.feature_buffer.append(normalized_features)

            # Get sequence if available
            sequence = self.feature_buffer.get_sequence(self.config.sequence_length)
            if sequence is not None:
                # Add to queue if space available
                try:
                    self.sequence_queue.put_nowait(sequence)
                except queue.Full:
                    self.logger.warning("Sequence queue full, dropping oldest sequence")
                    self.sequence_queue.get_nowait()  # Remove oldest
                    self.sequence_queue.put_nowait(sequence)

        except json.JSONDecodeError:
            self.logger.error("Failed to decode WebSocket message")
        except Exception as e:
            self.logger.error(f"Error processing message: {str(e)}")

    def get_next_sequence(self, timeout: float = 0.1) -> Optional[torch.Tensor]:
        """Retrieves the next available sequence.

        Args:
            timeout: Time to wait for sequence in seconds

        Returns:
            Sequence tensor if available, None otherwise
        """
        try:
            return self.sequence_queue.get(timeout=timeout)
        except queue.Empty:
            return None

    def stop(self) -> None:
        """Stops the real-time data handler."""
        self._is_running = False
        if self._websocket:
            asyncio.create_task(self._websocket.close())
        self._executor.shutdown(wait=True)


class FeatureBuffer:
    """Thread-safe buffer for storing and managing feature sequences."""

    def __init__(self, max_size: int):
        """Initializes the feature buffer.

        Args:
            max_size: Maximum number of features to store
        """
        self.max_size = max_size
        self.buffer = deque(maxlen=max_size)
        self._lock = threading.Lock()

    def append(self, features: torch.Tensor) -> None:
        """Appends features to the buffer thread-safely.

        Args:
            features: Feature tensor to append
        """
        with self._lock:
            # Optionally include timestamp for extrapolation
            # Assuming features include a timestamp at a specific index
            self.buffer.append(features)

    def get_sequence(self, sequence_length: int) -> Optional[torch.Tensor]:
        """Retrieves a sequence of specified length if available.

        Args:
            sequence_length: Length of sequence to retrieve

        Returns:
            Sequence tensor if available, None otherwise
        """
        with self._lock:
            if len(self.buffer) < sequence_length:
                return None

            sequence = list(itertools.islice(
                self.buffer,
                len(self.buffer) - sequence_length,
                len(self.buffer)
            ))
            return torch.stack(sequence)

    def clear(self) -> None:
        """Clears the buffer."""
        with self._lock:
            self.buffer.clear()


// File: src\pipeline\market_indicators.py
# File: src/pipeline/market_indicators.py

import numpy as np
import pandas as pd
import ta
from typing import List, Union, Dict, Tuple
from dataclasses import dataclass, field
from config.config import TradingConfig, IndicatorConfig


@dataclass
class IndicatorConfig:
    """Configuration class for technical indicators"""
    # MACD parameters
    macd_fast: int = 12
    macd_slow: int = 26
    macd_signal: int = 9

    # Bollinger Bands parameters
    bb_period: int = 20
    bb_std: int = 2

    # Multiple periods for EMA and RSI
    ema_periods: List[int] = field(default_factory=lambda: [5, 10, 20, 60, 120])
    rsi_periods: List[int] = field(default_factory=lambda: [6, 14, 24])

    # Volatility calculation period
    volatility_period: int = 20

    # Follow Line (FL) parameters
    fl_atr_period: int = 5
    fl_bb_period: int = 21
    fl_bb_deviation: float = 1.00
    fl_use_atr_filter: bool = True


class TechnicalIndicators:
    """Class for calculating various technical indicators"""

    def __init__(self, config: IndicatorConfig = None):
        """Initialize with optional custom configuration"""
        self.config = config or IndicatorConfig()

    def calculate_all(self, df: pd.DataFrame) -> Dict:
        """Calculate all technical indicators

        Args:
            df: DataFrame with 'high', 'low', 'close' columns

        Returns:
            Dictionary containing all calculated indicators
        """
        indicators = {}

        # Multiple RSI periods
        indicators['rsi'] = {
            f'rsi_{period}': self.calculate_rsi(df, period)
            for period in self.config.rsi_periods
        }

        # Multiple EMA periods
        indicators['ema'] = {
            f'ema_{period}': self.calculate_ema(df, period)
            for period in self.config.ema_periods
        }

        # MACD
        indicators.update(self.calculate_macd(df))

        # Bollinger Bands
        indicators.update(self.calculate_bollinger_bands(df))

        # Volatility
        indicators['volatility'] = self.calculate_volatility(
            df,
            self.config.volatility_period
        )

        # Follow Line
        indicators.update(self.calculate_follow_line(df))

        return indicators

    def calculate_follow_line(self, df: pd.DataFrame) -> Dict[str, Union[pd.Series, Dict[str, pd.Series]]]:
        """Calculate the Follow Line (FL) indicator and its signals

        Args:
            df: DataFrame with 'high', 'low', 'close' columns

        Returns:
            Dictionary containing FL values and signals
        """
        # Calculate Bollinger Bands
        bb = ta.volatility.BollingerBands(
            close=df['close'],
            window=self.config.fl_bb_period,
            window_dev=self.config.fl_bb_deviation
        )
        upper_band = bb.bollinger_hband()
        lower_band = bb.bollinger_lband()

        # Calculate ATR
        atr = ta.volatility.AverageTrueRange(
            high=df['high'],
            low=df['low'],
            close=df['close'],
            window=self.config.fl_atr_period
        ).average_true_range()

        # Initialize Follow Line series
        fl = pd.Series(index=df.index, dtype=float)
        trend = pd.Series(index=df.index, dtype=float)

        # Calculate Follow Line values
        for i in range(len(df)):
            if i == 0:
                fl.iloc[i] = df['close'].iloc[i]
                trend.iloc[i] = 0
                continue

            prev_fl = fl.iloc[i - 1]
            curr_price = df['close'].iloc[i]
            curr_high = df['high'].iloc[i]
            curr_low = df['low'].iloc[i]

            # Determine new FL value based on price position relative to Bollinger Bands
            if curr_price > upper_band.iloc[i]:
                new_fl = curr_low - (atr.iloc[i] if self.config.fl_use_atr_filter else 0)
            elif curr_price < lower_band.iloc[i]:
                new_fl = curr_high + (atr.iloc[i] if self.config.fl_use_atr_filter else 0)
            else:
                new_fl = prev_fl

            # Adjust FL based on trend to prevent unnecessary reversals
            if prev_fl > new_fl:  # Previous trend was up
                fl.iloc[i] = min(prev_fl, new_fl)
                trend.iloc[i] = -1 if fl.iloc[i] < prev_fl else 1
            else:  # Previous trend was down or flat
                fl.iloc[i] = max(prev_fl, new_fl)
                trend.iloc[i] = 1 if fl.iloc[i] > prev_fl else -1

        # Generate buy/sell signals
        signals = self._generate_fl_signals(trend)

        return {
            'follow_line': {
                'fl_value': fl,
                'fl_trend': trend,
                'signals': signals
            }
        }

    def _generate_fl_signals(self, trend: pd.Series) -> Dict[str, pd.Series]:
        """Generate buy/sell signals based on trend changes

        Args:
            trend: Series of trend values (1 for up, -1 for down)

        Returns:
            Dictionary containing buy and sell signals
        """
        buy_signals = (trend == 1) & (trend.shift(1) == -1)
        sell_signals = (trend == -1) & (trend.shift(1) == 1)

        return {
            'buy_signals': buy_signals,
            'sell_signals': sell_signals
        }

    def calculate_rsi(self, df: pd.DataFrame, period: int) -> pd.Series:
        """Calculate Relative Strength Index for a given period"""
        rsi = ta.momentum.RSIIndicator(
            close=df['close'],
            window=period
        )
        return rsi.rsi()

    def calculate_ema(self, df: pd.DataFrame, period: int) -> pd.Series:
        """Calculate Exponential Moving Average for a given period"""
        ema = ta.trend.EMAIndicator(
            close=df['close'],
            window=period
        )
        return ema.ema_indicator()

    def calculate_macd(self, df: pd.DataFrame) -> Dict[str, pd.Series]:
        """Calculate MACD indicator with its components"""
        macd = ta.trend.MACD(
            close=df['close'],
            window_slow=self.config.macd_slow,
            window_fast=self.config.macd_fast,
            window_sign=self.config.macd_signal
        )
        return {
            'macd_line': macd.macd(),
            'macd_signal': macd.macd_signal(),
            'macd_histogram': macd.macd_diff()
        }

    def calculate_bollinger_bands(self, df: pd.DataFrame) -> Dict[str, pd.Series]:
        """Calculate Bollinger Bands components"""
        bb = ta.volatility.BollingerBands(
            close=df['close'],
            window=self.config.bb_period,
            window_dev=self.config.bb_std
        )
        return {
            'bb_upper': bb.bollinger_hband(),
            'bb_middle': bb.bollinger_mavg(),
            'bb_lower': bb.bollinger_lband(),
            'bb_bandwidth': bb.bollinger_hband() - bb.bollinger_lband()
        }

    def calculate_volatility(self, df: pd.DataFrame, window: int) -> pd.Series:
        """Calculate annualized price volatility"""
        returns = df['close'].pct_change().dropna()
        return returns.rolling(window=window).std() * np.sqrt(252)


// File: src\pipeline\metrics.py
# File: src/pipeline/metrics.py

import time
from typing import Dict, List
import logging


class PipelineMetrics:
    """Tracks and logs pipeline performance metrics."""

    def __init__(self):
        """Initializes the metrics tracker."""
        self.metrics: Dict[str, List[float]] = {
            'processing_time': [],
            'queue_size': [],
            'sequence_length': []
        }
        self.start_time = time.time()
        self.logger = logging.getLogger(__name__)

    def update(self, metric_name: str, value: float) -> None:
        """Updates a metric value.

        Args:
            metric_name: Name of the metric
            value: New value
        """
        if metric_name in self.metrics:
            self.metrics[metric_name].append(value)

    def log_summary(self) -> None:
        """Logs a summary of tracked metrics."""
        runtime = time.time() - self.start_time

        for name, values in self.metrics.items():
            if values:
                avg_value = sum(values) / len(values)
                min_value = min(values)
                max_value = max(values)
                self.logger.info(
                    f"{name} - Average: {avg_value:.4f}, "
                    f"Min: {min_value:.4f}, "
                    f"Max: {max_value:.4f}"
                )

        self.logger.info(f"Total runtime: {runtime:.2f}s")

    def reset(self) -> None:
        """Resets all metrics."""
        for key in self.metrics:
            self.metrics[key] = []
        self.start_time = time.time()


// File: src\pipeline\normalizers.py
# src/pipeline/normalizers.py
"""Feature normalization implementation"""

from abc import ABC, abstractmethod
import torch
from typing import Dict, Optional
from shared.types import NormalizationType
from shared.exceptions import ProcessingError

class BaseNormalizer(ABC):
    """Abstract base class for feature normalization."""
    
    @abstractmethod
    def fit(self, data: torch.Tensor) -> None:
        pass
    
    @abstractmethod
    def transform(self, data: torch.Tensor) -> torch.Tensor:
        pass
    
    @abstractmethod
    def inverse_transform(self, data: torch.Tensor) -> torch.Tensor:
        pass

class FeatureNormalizer(BaseNormalizer):
    """Implements various normalization strategies."""
    
    def __init__(self, method: NormalizationType, epsilon: float = 1e-8):
        self.method = method
        self.epsilon = epsilon
        self.params: Dict[str, torch.Tensor] = {}
        self._is_fitted = False
    
    def fit(self, data: torch.Tensor) -> None:
        if self.method == NormalizationType.STANDARD:
            self.params["mean"] = data.mean(dim=0)
            self.params["std"] = data.std(dim=0)
        elif self.method == NormalizationType.MINMAX:
            self.params["min"] = data.min(dim=0)[0]
            self.params["max"] = data.max(dim=0)[0]
        elif self.method == NormalizationType.ROBUST:
            q1 = torch.quantile(data, 0.25, dim=0)
            q3 = torch.quantile(data, 0.75, dim=0)
            self.params["median"] = torch.median(data, dim=0)[0]
            self.params["iqr"] = q3 - q1
        
        self._is_fitted = True
    
    def transform(self, data: torch.Tensor) -> torch.Tensor:
        if not self._is_fitted and self.method != NormalizationType.NONE:
            raise ProcessingError("Normalizer must be fitted before transform")
            
        if self.method == NormalizationType.STANDARD:
            return (data - self.params["mean"]) / (self.params["std"] + self.epsilon)
        elif self.method == NormalizationType.MINMAX:
            denominator = self.params["max"] - self.params["min"]
            denominator[denominator == 0] = 1
            return (data - self.params["min"]) / denominator
        elif self.method == NormalizationType.ROBUST:
            return (data - self.params["median"]) / (self.params["iqr"] + self.epsilon)
        
        return data
    
    def inverse_transform(self, data: torch.Tensor) -> torch.Tensor:
        if not self._is_fitted and self.method != NormalizationType.NONE:
            raise ProcessingError("Normalizer must be fitted before inverse_transform")
            
        if self.method == NormalizationType.STANDARD:
            return data * (self.params["std"] + self.epsilon) + self.params["mean"]
        elif self.method == NormalizationType.MINMAX:
            denominator = self.params["max"] - self.params["min"]
            denominator[denominator == 0] = 1
            return data * denominator + self.params["min"]
        elif self.method == NormalizationType.ROBUST:
            return data * (self.params["iqr"] + self.epsilon) + self.params["median"]
            
        return data

// File: src\pipeline\pipeline.py
# File: src/pipeline/pipeline.py

from typing import Any, Optional, Tuple
import torch
from torch.utils.data import DataLoader
from pathlib import Path
import pandas as pd
import logging

from config.config import TradingConfig
from pipeline.normalizers import FeatureNormalizer, NormalizationType
from pipeline.augmenters import DataAugmenter
from pipeline.datasets import SequencedMarketDataset
from pipeline.handlers import RealTimeDataHandler, TargetCalculator, DirectionTargetCalculator
from pipeline.metrics import PipelineMetrics
from utils.logging import custom_logger
from data.base_preprocessor import MarketDataPreprocessor
from pipeline.market_indicators import IndicatorConfig, TechnicalIndicators


class TradingDataPipeline:
    """Orchestrates the complete trading data pipeline."""

    def __init__(
        self,
        config: TradingConfig,
        target_calculator: TargetCalculator,
        device: torch.device = None
    ):
        """Initializes the trading pipeline.

        Args:
            config: Trading configuration
            target_calculator: Calculator for target values
            device: Torch device to use
        """
        self.config = config
        self.target_calculator = target_calculator
        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Initialize components
        self.feature_normalizer = FeatureNormalizer(
            method=config.target_normalization,
            epsilon=config.epsilon
        )
        self.data_augmenter = DataAugmenter(config) if config.use_augmentation else None

        self.logger = custom_logger.logger

        # Initialize technical indicators
        self.technical_indicators = TechnicalIndicators(
            IndicatorConfig()
        )

    def prepare_data(
        self,
        data_path: Path,
        train_split: float = 0.8
    ) -> Tuple[DataLoader, DataLoader]:
        """Prepares training and validation data loaders.

        Args:
            data_path: Path to raw data file
            train_split: Ratio of data to use for training

        Returns:
            Tuple of (train_loader, val_loader)
        """
        # Load and preprocess data
        preprocessor = MarketDataPreprocessor(self.config, data_path, self.technical_indicators)
        preprocessor.load_data()
        preprocessor.clean_data()
        preprocessor.normalize_features()
        processed_data = preprocessor.get_processed_data()

        # Split data
        train_data, val_data = self._split_data(processed_data, train_split)

        # Fit scaler on training data
        feature_columns = self.config.feature_keys + self.config.technical_indicators
        train_features = torch.tensor(train_data[feature_columns].values, dtype=torch.float32)
        self.feature_normalizer.fit(train_features)

        # Normalize both training and validation data
        preprocessor.normalizer = self.feature_normalizer  # Ensure preprocessor uses the same scaler
        preprocessor.normalize_features()
        train_data = preprocessor.get_processed_data().iloc[:len(train_data)]
        val_data = preprocessor.get_processed_data().iloc[len(train_data):]

        # Create datasets
        train_dataset = SequencedMarketDataset(
            train_data,
            self.config,
            self.feature_normalizer,
            self.target_calculator,
            self.data_augmenter,
            is_training=True
        )

        val_dataset = SequencedMarketDataset(
            val_data,
            self.config,
            self.feature_normalizer,
            self.target_calculator,
            is_training=False
        )

        # Create data loaders
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.config.batch_size,
            shuffle=True,
            num_workers=self.config.num_workers,
            pin_memory=self.config.pin_memory,
            prefetch_factor=self.config.prefetch_factor
        )

        val_loader = DataLoader(
            val_dataset,
            batch_size=self.config.batch_size,
            shuffle=False,
            num_workers=self.config.num_workers,
            pin_memory=self.config.pin_memory,
            prefetch_factor=self.config.prefetch_factor
        )

        self.logger.info(
            f"Data split - Training: {len(train_data):,} samples, "
            f"Validation: {len(val_data):,} samples"
        )

        return train_loader, val_loader

    def create_realtime_handler(self, feature_extractor: Optional[Any] = None) -> RealTimeDataHandler:
        """Creates a real-time data handler instance.

        Args:
            feature_extractor: Optional MarketFeatureExtractor instance

        Returns:
            Configured RealTimeDataHandler instance
        """
        if feature_extractor is None:
            from pipeline.handlers import MarketFeatureExtractor
            feature_extractor = MarketFeatureExtractor(self.config)

        return RealTimeDataHandler(
            config=self.config,
            normalizer=self.feature_normalizer,
            target_calculator=self.target_calculator,
            feature_extractor=feature_extractor
        )

    def _split_data(
        self,
        data: pd.DataFrame,
        train_split: float
    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Splits data into training and validation sets.

        Args:
            data: Input DataFrame
            train_split: Ratio of data to use for training

        Returns:
            Tuple of (train_data, val_data)
        """
        split_idx = int(len(data) * train_split)
        train_data = data.iloc[:split_idx].copy()
        val_data = data.iloc[split_idx:].copy()

        return train_data, val_data

    def save_pipeline_state(self, path: Path) -> None:
        """Saves the pipeline state for later use.

        Args:
            path: Path to save state
        """
        state = {
            'config': self.config.to_dict(),
            'normalizer_params': self.feature_normalizer.params,
            'normalizer_method': self.feature_normalizer.method.value
        }

        path.parent.mkdir(parents=True, exist_ok=True)
        torch.save(state, path)
        self.logger.info(f"Pipeline state saved to {path}")

    def load_pipeline_state(self, path: Path) -> None:
        """Loads a saved pipeline state.

        Args:
            path: Path to saved state
        """
        try:
            state = torch.load(path)
            self.config = TradingConfig(**state['config'])
            self.feature_normalizer = FeatureNormalizer(
                method=NormalizationType(state['normalizer_method']),
                epsilon=self.config.epsilon
            )
            self.feature_normalizer.params = state['normalizer_params']
            self.feature_normalizer._is_fitted = True

            self.logger.info(f"Pipeline state loaded from {path}")

        except Exception as e:
            self.logger.error(f"Error loading pipeline state: {str(e)}")
            raise


// File: src\pipeline\__init__.py
# File: src/pipeline/__init__.py

from .normalizers import FeatureNormalizer, NormalizationType
from .augmenters import DataAugmenter
from .datasets import SequencedMarketDataset
from .handlers import RealTimeDataHandler, TargetCalculator, DirectionTargetCalculator
from .metrics import PipelineMetrics
from .pipeline import TradingDataPipeline, PipelineMetrics
from .feature_transformer import KlineFeatureTransformer, KlineTransformerConfig
from .market_indicators import TechnicalIndicators, IndicatorConfig


// File: src\shared\exceptions.py
"""Custom exceptions used across modules"""

class ValidationError(Exception):
    """Raised when data validation fails"""
    pass

class ProcessingError(Exception):
    """Raised when data processing fails"""
    pass

// File: src\shared\types.py
# src/shared/types.py
"""Common types and enums used across modules"""

from enum import Enum
from dataclasses import dataclass
from typing import List, Dict, Any, Optional

class TargetType(Enum):
    """Enumeration of supported target types."""
    DIRECTION = "direction"
    PRICE = "price"
    VOLATILITY = "volatility"
    CUSTOM = "custom"

class NormalizationType(Enum):
    """Enumeration of supported normalization methods."""
    STANDARD = "standard"
    MINMAX = "minmax"
    ROBUST = "robust"
    NONE = "none"

@dataclass
class ProcessingMetrics:
    """Metrics for monitoring processing performance"""
    cpu_percent: float
    memory_percent: float
    gpu_utilization: List[float]
    gpu_memory_percent: List[float]

# src/shared/exceptions.py
"""Custom exceptions used across modules"""

class ValidationError(Exception):
    """Raised when data validation fails"""
    pass

class ProcessingError(Exception):
    """Raised when data processing fails"""
    pass

// File: src\utils\logging.py
# File: src/utils/logging.py

from loguru import logger
import sys
from pathlib import Path
from datetime import datetime
import json
from typing import Dict, Any

class CustomLogger:
    def __init__(self, log_dir: str = "logs"):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)

        # Remove default logger and set up our custom configuration
        logger.remove()

        # Add file handler
        log_file = self.log_dir / f"trading_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        logger.add(
            log_file,
            format="{time:YYYY-MM-DD HH:mm:ss.SSS} | {level: <8} | {name}:{function}:{line} - {message}",
            level="INFO",
            rotation="1 day",
            compression="zip"
        )

        # Add console handler
        logger.add(
            sys.stdout,
            format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
            level="INFO"
        )

    def log_metrics(self, metrics: Dict[str, Any], step: int):
        """Log metrics to both file and wandb if configured."""
        metrics_str = json.dumps(metrics)
        logger.info(f"Step {step} metrics: {metrics_str}")

custom_logger = CustomLogger()  # Singleton instance


// File: src\utils\system_monitor.py
import psutil
import GPUtil
from dataclasses import dataclass
from typing import List, Dict
import numpy as np
from loguru import logger

@dataclass
class SystemMetrics:
    cpu_percent: float
    memory_percent: float
    gpu_utilization: List[float]
    gpu_memory_utilization: List[float]

class SystemMonitor:
    def __init__(self):
        self.metrics_history: List[SystemMetrics] = []
        
    def get_metrics(self) -> SystemMetrics:
        """Get current system metrics"""
        try:
            # CPU and Memory
            cpu_percent = psutil.cpu_percent(interval=1)
            memory_percent = psutil.virtual_memory().percent
            
            # GPU metrics
            gpus = GPUtil.getGPUs()
            gpu_utilization = [gpu.load * 100 for gpu in gpus]
            gpu_memory_utilization = [gpu.memoryUtil * 100 for gpu in gpus]
            
            metrics = SystemMetrics(
                cpu_percent=cpu_percent,
                memory_percent=memory_percent,
                gpu_utilization=gpu_utilization,
                gpu_memory_utilization=gpu_memory_utilization
            )
            
            self.metrics_history.append(metrics)
            return metrics
            
        except Exception as e:
            logger.error(f"Error getting system metrics: {str(e)}")
            return SystemMetrics(0.0, 0.0, [], [])
            
    def get_metrics_summary(self) -> Dict[str, float]:
        """Get summary statistics of system metrics"""
        if not self.metrics_history:
            return {}
            
        metrics_array = np.array([
            [m.cpu_percent, m.memory_percent] +
            m.gpu_utilization +
            m.gpu_memory_utilization
            for m in self.metrics_history
        ])
        
        return {
            'cpu_mean': np.mean(metrics_array[:, 0]),
            'cpu_max': np.max(metrics_array[:, 0]),
            'memory_mean': np.mean(metrics_array[:, 1]),
            'memory_max': np.max(metrics_array[:, 1]),
            'gpu_util_mean': np.mean(metrics_array[:, 2:2+len(self.metrics_history[0].gpu_utilization)]),
            'gpu_memory_mean': np.mean(metrics_array[:, -len(self.metrics_history[0].gpu_memory_utilization):])
        }

// File: src\utils\__init__.py
# File: src/utils/__init__.py

from .logging import custom_logger
from .system_monitor import SystemMonitor


