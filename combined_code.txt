I have this project folder file C:\Users\dylan\Desktop\sheeplz-crypto-bot\data

I want modularize this whole folder , to make th code mopre maintainable, I also want to optimze

#data/__init__.py
from .config import Config
from .mexc_data_ingestion import DataIngestion

__all__ = ['Config', 'DataIngestion']

#data/config.py
from dataclasses import dataclass, field
from typing import List, Dict, Any, Union

@dataclass
class Config:
    """Configuration for DataIngestion."""
    symbol: str
    timeframes: List[str]
    private_channels: List[Union[str, Dict[str, Any]]] = field(default_factory=list)
    reconnect_delay: int = 5
    max_reconnect_delay: int = 300
    backoff_factor: float = 2.0
    rate_limit: int = 100
    processing_queue_size: int = 1000

# data/mexc_data_ingestion.py
import asyncio
import json
import logging
import websockets
import os
import hmac
import hashlib
import time
from typing import Dict, Any, Optional, List
from collections import deque
from asyncio import Queue, CancelledError
from dotenv import load_dotenv

from .config import Config
from .websocket_handler import WebSocketHandler
from .data_processor import DataProcessor

load_dotenv()


class DataIngestion:
    def __init__(self, gmn: Any, config: Config):
        self.gmn = gmn
        self.config = config
        self.ws_url = os.getenv('MEXC_WS_URL', 'wss://contract.mexc.com/ws')
        self.api_key = os.getenv('MEXC_API_KEY')
        self.api_secret = os.getenv('MEXC_API_SECRET')
        self.websocket_handler = WebSocketHandler(self.ws_url, self.api_key, self.api_secret, config.rate_limit)
        self.data_processor = DataProcessor(gmn)
        self.processing_queue: Queue[Dict[str, Any]] = asyncio.Queue(maxsize=config.processing_queue_size)

        self.reconnect_attempts = 0
        self.max_reconnect_attempts = config.reconnect_attempts_max  # From config
        self.backoff_factor = config.reconnect_backoff_factor # From config
        self._processing_task: Optional[asyncio.Task] = None
        self._batch_task: Optional[asyncio.Task] = None

    async def connect(self):
        """Connects to the WebSocket and handles reconnections."""
        while self.reconnect_attempts < self.max_reconnect_attempts:
            try:
                await self.websocket_handler.connect()
                self.reconnect_attempts = 0  # Reset attempts on successful connection
                await self._login()
                await self._subscribe()

                # Start the batch processor task here:
                self._batch_task = asyncio.create_task(self._batch_processor())

                await self._receive_data_loop()  # Start receiving data

            except (websockets.exceptions.ConnectionClosedError,
                    websockets.exceptions.ConnectionClosedOK,
                    ConnectionResetError,
                    OSError,
                    asyncio.TimeoutError) as e:
                logging.error(f"WebSocket connection error: {e}")

            except Exception as e:  # Catch all other exceptions
                logging.exception(f"An unexpected error occurred: {e}")  # Log traceback

            finally:
                if self.websocket_handler.websocket and self.websocket_handler.websocket.open:  # Correctly check if open
                    await self.websocket_handler.close() # Ensure websocket is closed before retry.

            self.reconnect_attempts += 1
            wait_time = min(self.config.reconnect_delay * (self.backoff_factor ** (self.reconnect_attempts -1)), self.config.max_reconnect_delay)
            logging.info(f"Reconnecting attempt {self.reconnect_attempts} in {wait_time:.2f} seconds...")
            await asyncio.sleep(wait_time)  # Exponential backoff



    async def _login(self):
        # ... (same as before)

    async def _subscribe(self):
        # ... (same as before)


    async def _receive_data_loop(self):
        """Receives data from the websocket."""
        while True:
            try:
                message = await self.websocket_handler.receive_message()

                if message: # Check if message is not None or empty
                    await self.processing_queue.put(message)  # Queue the message for batch processing

            except websockets.exceptions.ConnectionClosedOK as e: #Specific Exception
                logging.warning(f"WebSocket connection closed gracefully: {e.reason}")
                break #Break to allow reconnect
            except websockets.exceptions.ConnectionClosedError as e:  # Specific exception handling
                logging.error(f"WebSocket connection closed with error: {e.reason}")
                break #Break to allow reconnect
            except Exception as e:  # Catch any other exceptions
                logging.exception(f"Error in receive_data_loop: {e}")  # Include traceback
                break



    async def _batch_processor(self, batch_size: int = 10, timeout: float = 1.0):
        """Processes data in batches from the queue."""
        while True:
            batch = []
            try:
                async with asyncio.timeout(timeout):
                    for _ in range(batch_size):  # Changed to use loop for batch collection
                        message = await self.processing_queue.get()
                        batch.append(json.loads(message))  # Parse JSON here for efficiency
                        self.processing_queue.task_done()

            except asyncio.TimeoutError:
                pass  # Process the collected batch, even if it's less than batch_size

            if batch:
                try:
                    await self.data_processor.process_data(batch)  # Process the batch
                except Exception as e:
                    logging.error(f"Error processing batch: {e}")


    async def close(self):

        if self._batch_task:
            self._batch_task.cancel() # Ensure cleanup of tasks
            try:
                await self._batch_task  # Wait for batch task to cancel
            except asyncio.CancelledError:
                pass

        if self.websocket_handler:
            await self.websocket_handler.close()




# data/websocket_handler.py
import asyncio
import json
import logging
import websockets
import hmac
import hashlib
import time
from typing import Dict, Any, Optional


class WebSocketHandler:

    def __init__(self, url: str, api_key: Optional[str], api_secret: Optional[str], rate_limit: int):

        self.url = url
        self.api_key = api_key
        self.api_secret = api_secret
        self.rate_limit = rate_limit
        self.websocket: Optional[websockets.WebSocketClientProtocol] = None
        # ... (rest of the code is the same)



    async def connect(self):
        try:
            self.websocket = await websockets.connect(self.url, ping_interval=30, ping_timeout=10) # Set ping params.

        except (websockets.exceptions.WebSocketException, OSError) as e:
            logging.error(f"Could not connect to WebSocket: {e}")
            raise  # Re-raise the exception to be handled by the DataIngestion class



    # ... (login, send_message, receive_message, close, _generate_signature, _respect_rate_limit remain the same)



# data/data_processor.py
import logging
from typing import Dict, Any, List

class DataProcessor: # Modified to accept batch data
    # ... (init remains the same)

    async def process_data(self, data_batch: List[Dict[str, Any]]):  #Modified
        for data in data_batch: #Process each item individually. 
            if 'data' in data and 'channel' in data:
                channel = data['channel']
                if channel.startswith('push.kline'):
                    await self._process_kline_data(data)
                # ... (handling for other channels if needed)

    async def _process_kline_data(self, data: Dict[str, Any]):
        channel_parts = data['channel'].split('.')
        if len(channel_parts) >= 4:
            interval = channel_parts[3] #Extract interval 
            kline_data = data['data']

            # Batch update the GMN:
            if isinstance(kline_data, list):
                for kline in kline_data:
                    kline['interval'] = interval  # Add interval to each candle
                await self.gmn.update_graph(kline_data)
            elif isinstance(kline_data, dict): #Handle single candle messages:
                kline_data['interval'] = interval
                await self.gmn.update_graph([kline_data]) # Put it in a list for batch update.
            else: # More descriptive error handling
                logging.error(f"Invalid Kline data format: {type(kline_data)}, Data:{kline_data}")


        # ... (Rest of the code for other channels/methods)
content_copy
Use code with caution.
Python

Key Changes and Explanations:

DataIngestion:

Reconnection Logic: Added robust reconnection with exponential backoff (controlled by config.reconnect_delay, config.reconnect_backoff_factor, and config.max_reconnect_delay). The loop continues until max_reconnect_attempts is reached. Specific exception handling is included for common WebSocket/network errors.

Batch Processing: Starts _batch_processor as a task. The _receive_data_loop now puts messages onto the queue, and the batch processor handles updating the GMN in batches. The JSON parsing is now done in the batch processor for efficiency.

Updated Error Handling: The connect method now catches a wider range of exceptions and includes a finally block to close the websocket before reconnecting. More specific exception handling within the _receive_data_loop method. Added max_reconnect_attempts to limit reconnection tries.

WebSocketHandler:

Connection Handling: The connect method raises exceptions instead of just logging them, allowing the DataIngestion class to handle reconnection logic. Added ping/pong (heartbeat) parameters to the websockets.connect call to help keep the connection alive.

Websocket Instance Handling: Modified close method to handle None cases to prevent errors during shutdown.

DataProcessor:

Batch Processing: process_data method now handles batch updates to the GMN. Handles both lists of kline data and single kline messages. More descriptive logging for invalid kline data formats.

These changes significantly enhance the robustness and efficiency of the data ingestion process. The changes should also enhance the performance of the GMN updates by processing data in batches. Remember to add the new configuration options (reconnect_attempts_max, reconnect_backoff_factor) to your config file and update any related unit tests.


#C:\Users\dylan\Desktop\sheeplz-crypto-bot\data\data_processor.py
import logging
from typing import Dict, Any

class DataProcessor:
    def __init__(self, gmn):
        self.gmn = gmn

    async def process_data(self, data: Dict[str, Any]):
        if 'data' in data and 'channel' in data:
            channel = data['channel']
            if channel.startswith('push.kline'):
                await self._process_kline_data(data)
            elif channel.startswith("push.private."):
                await self._process_private_data(data)
        elif 'method' in data:
            self._process_method_data(data)

    async def _process_kline_data(self, data: Dict[str, Any]):
        channel_parts = data['channel'].split('.')
        if len(channel_parts) >= 4:
            interval = channel_parts[3]
            kline_data = data['data']
            if isinstance(kline_data, list):
                for kline in kline_data:
                    kline['interval'] = interval
                    await self.gmn.update_graph([kline])
            elif isinstance(kline_data, dict):
                kline_data['interval'] = interval
                await self.gmn.update_graph([kline_data])
            else:
                logging.warning("Received kline data in unexpected format.")
        else:
            logging.warning(f"Unexpected channel format: {data['channel']}")

    async def _process_private_data(self, data: Dict[str, Any]):
        channel = data.get("channel")
        if channel == 'push.account':
            logging.info(f"Account Update: {data['data']}")
        elif channel == 'push.order':
            logging.info(f"Order Update: {data['data']}")
        else:
            logging.warning(f"Received data from unhandled private channel: {channel}")

    def _process_method_data(self, data: Dict[str, Any]):
        method = data['method']
        if method == 'pong':
            logging.debug("Received pong from server.")
        else:
            logging.debug(f"Unhandled message with method: {method}")