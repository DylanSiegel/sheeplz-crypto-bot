// File: data_transformation.py
# data_transformation.py
import cudf
import cupy as cp
from cuml.decomposition import PCA as cuPCA
from sklearn.preprocessing import StandardScaler
from scipy.fft import fft


def calculate_fft(series):
    fft_values = fft(series.values)
    return cudf.DataFrame({
        'FFT_Real': fft_values.real,
        'FFT_Imag': fft_values.imag,
        'FFT_Magnitude': cp.abs(fft_values),
        'FFT_Phase': cp.angle(fft_values)
    })


def perform_pca(df, n_components=5):
    num_cols = df.select_dtypes(include=[cp.float64, cp.float32]).columns
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(df[num_cols].to_pandas())
    pca = cuPCA(n_components=n_components)
    pca_result = pca.fit_transform(scaled_data)
    return cudf.DataFrame({f'PCA_{i+1}': pca_result[:, i] for i in range(n_components)})

// File: feature_engineering.py
# feature_engineering.py
import cudf
import ta
from functools import lru_cache


def calculate_indicators(df):
    close = df['Close']
    high = df['High']
    low = df['Low']
    volume = df['Volume']
    
    # Indicators
    rsi = ta.momentum.RSIIndicator(close, window=14).rsi()
    macd_indicator = ta.trend.MACD(close)
    macd = macd_indicator.macd()
    macd_signal = macd_indicator.macd_signal()
    bb = ta.volatility.BollingerBands(close)
    bollinger_upper = bb.bollinger_hband()
    bollinger_lower = bb.bollinger_lband()
    atr = ta.volatility.AverageTrueRange(high, low, close, window=14).average_true_range()
    obv = ta.volume.OnBalanceVolumeIndicator(close, volume).on_balance_volume()

    return cudf.concat([
        df,
        rsi.rename('RSI'),
        macd.rename('MACD'),
        macd_signal.rename('MACD_Signal'),
        bollinger_upper.rename('Bollinger_Upper'),
        bollinger_lower.rename('Bollinger_Lower'),
        atr.rename('ATR'),
        obv.rename('OBV')
    ], axis=1)


def calculate_rolling_statistics(series, window=14):
    return cudf.DataFrame({
        'Rolling_Mean': series.rolling(window=window).mean(),
        'Rolling_Median': series.rolling(window=window).median(),
        'Rolling_Variance': series.rolling(window=window).var(),
        'Rolling_Skew': series.rolling(window=window).skew(),
        'Rolling_Kurtosis': series.rolling(window=window).kurt()
    })

// File: main.py
// File: main.py
# main.py
import os
import glob
from utils import optimize_dataframe, process_chunk
from tqdm import tqdm


def main():
    N_JOBS = min(24, os.cpu_count())
    file_paths = glob.glob(r"C:\Users\dylan\Desktop\DATA-LAKE\data\raw\*.csv")
    output_path = r"C:\Users\dylan\Desktop\DATA-LAKE\data\processed\btc_merged_advanced_features.csv"

    chunk_size = 10
    num_chunks = (len(file_paths) + chunk_size - 1) // chunk_size

    for i in tqdm(range(num_chunks), desc="Processing Chunks"):
        chunk = file_paths[i * chunk_size:(i + 1) * chunk_size]
        processed_chunk = process_chunk(chunk, N_JOBS)
        processed_chunk = optimize_dataframe(processed_chunk)
        processed_chunk.to_csv(
            output_path,
            mode="a",
            header=not os.path.exists(output_path),
            index=False,
        )


if __name__ == "__main__":
    main()


// File: utils.py
# utils.py
import cudf
from concurrent.futures import ThreadPoolExecutor
from feature_engineering import calculate_indicators, calculate_rolling_statistics


def optimize_dataframe(df):
    for col in df.select_dtypes(include=['float64']).columns:
        if df[col].memory_usage() > df[col].astype('float32').memory_usage():
            df[col] = df[col].astype('float32')
    for col in df.select_dtypes(include=['int64']).columns:
        if df[col].memory_usage() > df[col].astype('int32').memory_usage():
            df[col] = df[col].astype('int32')
    return df


def process_chunk(filepath_chunk, n_jobs):
    with ThreadPoolExecutor(max_workers=n_jobs) as executor:
        df_list_chunk = []
        for future in executor.map(cudf.read_csv, filepath_chunk):
            try:
                df_list_chunk.append(future)
            except Exception as e:
                print(f"Error processing file: {e}")
    
    if df_list_chunk:
        merged_df_chunk = cudf.concat(df_list_chunk, ignore_index=True).sort_values(by='Open time')
        del df_list_chunk  # Free memory immediately after concatenation
        return calculate_indicators(merged_df_chunk)
    else:
        raise ValueError("No valid data frames processed from chunk.")

