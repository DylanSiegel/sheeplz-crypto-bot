**File Tree (Relevant Files Only)**
  .
    - meta_sac.py
    - test.py
// File: meta_sac.py
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import random
import unittest
import os

# Detect device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ------------------------- Helper Classes -------------------------
class APELU(nn.Module):
    def __init__(self, alpha_init=0.01, beta_init=1.0):
        super(APELU, self).__init__()
        self.alpha = nn.Parameter(torch.tensor(alpha_init))
        self.beta = nn.Parameter(torch.tensor(beta_init))

    def forward(self, x):
        return torch.where(x >= 0, x, self.alpha * x * torch.exp(self.beta * x))


# ------------------------- Modern MLP -------------------------
class ModernMLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3, dropout_rate=0.1):
        super(ModernMLP, self).__init__()
        self.layers = nn.ModuleList()
        self.norms = nn.ModuleList()

        for i in range(num_layers):
            in_features = input_dim if i == 0 else hidden_dim
            out_features = output_dim if i == num_layers - 1 else hidden_dim
            self.layers.append(nn.Linear(in_features, out_features))
            if i != num_layers - 1:
                self.norms.append(nn.LayerNorm(hidden_dim))
        self.activation = APELU()
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, x):
        for i, layer in enumerate(self.layers):
            x = layer(x)
            if i != len(self.layers) - 1:
                x = self.norms[i](x)
                x = self.activation(x)
                x = self.dropout(x)
        return x


# ------------------------- Adaptive Modulation MLP -------------------------
class TimeAwareBias(nn.Module):
    def __init__(self, input_dim, time_encoding_dim=10, hidden_dim=20):
        super().__init__()
        self.time_embedding = nn.Linear(time_encoding_dim, hidden_dim)
        self.time_projection = nn.Linear(hidden_dim, input_dim)
        self.activation = APELU()

    def forward(self, time_encoding):
        x = self.time_embedding(time_encoding)
        x = self.activation(x)
        x = self.time_projection(x)
        return x  # [batch_size, input_dim]


class AdaptiveModulationMLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3, dropout_rate=0.1, time_encoding_dim=10):
        super(AdaptiveModulationMLP, self).__init__()
        self.layers = nn.ModuleList()
        self.norms = nn.ModuleList()
        self.modulations = nn.ParameterList()
        self.time_biases = nn.ModuleList()

        for i in range(num_layers):
            in_features = input_dim if i == 0 else hidden_dim
            out_features = output_dim if i == num_layers - 1 else hidden_dim
            self.layers.append(nn.Linear(in_features, out_features))
            if i != num_layers - 1:
                self.norms.append(nn.LayerNorm(hidden_dim))
                self.modulations.append(nn.Parameter(torch.ones(hidden_dim)))
                self.time_biases.append(TimeAwareBias(hidden_dim, time_encoding_dim))
        self.activation = APELU()
        self.dropout = nn.Dropout(dropout_rate)
        self.num_layers = num_layers

    def forward(self, x, time_encoding):
        for i, layer in enumerate(self.layers):
            x = layer(x)
            if i != self.num_layers - 1:
                modulation_factor = self.modulations[i] + self.time_biases[i](time_encoding)
                x = x * modulation_factor
                x = self.norms[i](x)
                x = self.activation(x)
                x = self.dropout(x)
        return x


# ------------------------- Attention Mechanism -------------------------
class Attention(nn.Module):
    def __init__(self, input_dim, attention_dim):
        super().__init__()
        self.query_proj = nn.Linear(input_dim, attention_dim)
        self.key_proj = nn.Linear(input_dim, attention_dim)
        self.value_proj = nn.Linear(input_dim, attention_dim)
        self.out_proj = nn.Linear(attention_dim, input_dim)

    def forward(self, input):
        # input shape expected: [batch, seq_len, input_dim]
        # If seq_len = 1, shapes are [batch, 1, input_dim]
        query = self.query_proj(input)
        key = self.key_proj(input)
        value = self.value_proj(input)

        # Attention calculation
        scores = torch.matmul(query, key.transpose(-2, -1))
        attention_weights = F.softmax(scores / (input.shape[-1]**0.5), dim=-1)
        context_vector = torch.matmul(attention_weights, value)
        output = self.out_proj(context_vector)
        return output


# ------------------------- SAC Actor Network -------------------------
class MetaSACActor(nn.Module):
    def __init__(self, input_dim, action_dim, hidden_dim, attention_dim, num_mlp_layers=3, dropout_rate=0.1, time_encoding_dim=10):
        super(MetaSACActor, self).__init__()
        self.attention = Attention(input_dim, attention_dim)
        self.mlp = AdaptiveModulationMLP(input_dim, hidden_dim, 2*action_dim, num_mlp_layers, dropout_rate, time_encoding_dim)
        self.action_dim = action_dim

    def forward(self, x, time_encoding):
        # Ensure x is [batch, seq_len, dim] for attention
        if x.dim() == 2:
            x = x.unsqueeze(1)
        x = self.attention(x)  # [batch, 1, input_dim]
        x = x.squeeze(1)       # [batch, input_dim]
        x = self.mlp(x, time_encoding)
        mu, log_sigma = x[:, :self.action_dim], x[:, self.action_dim:]
        # Check for NaN in mu or log_sigma
        if torch.isnan(mu).any() or torch.isnan(log_sigma).any():
            print("Warning: NaN detected in actor output (mu or log_sigma).")
        return torch.tanh(mu), log_sigma


# ------------------------- SAC Critic Network -------------------------
class MetaSACCritic(nn.Module):
    def __init__(self, input_dim, action_dim, hidden_dim, attention_dim, num_mlp_layers=3, dropout_rate=0.1, time_encoding_dim=10):
        super(MetaSACCritic, self).__init__()
        self.attention = Attention(input_dim + action_dim, attention_dim)
        self.mlp = AdaptiveModulationMLP(input_dim + action_dim, hidden_dim, 1, num_mlp_layers, dropout_rate, time_encoding_dim)
        self.action_dim = action_dim

    def forward(self, state, action, time_encoding):
        x = torch.cat([state, action], dim=-1)
        # Ensure shape for attention
        if x.dim() == 2:
            x = x.unsqueeze(1)
        x = self.attention(x)
        x = x.squeeze(1)
        q_value = self.mlp(x, time_encoding)
        return q_value


# ------------------------- Meta Controller -------------------------
class MetaController(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_mlp_layers=3, dropout_rate=0.1):
        super().__init__()
        self.mlp = ModernMLP(input_dim, hidden_dim, output_dim, num_mlp_layers, dropout_rate)

    def forward(self, x):
        return self.mlp(x)


# ------------------------- Meta-SAC Agent -------------------------
class MetaSACAgent(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim, attention_dim, meta_input_dim, time_encoding_dim=10, num_mlp_layers=3, dropout_rate=0.1, lr=1e-3, alpha=0.2, gamma=0.99, tau=0.005, meta_lr=1e-4, device=device):
        super().__init__()
        self.device = device
        self.state_dim = state_dim
        # Adjust action_dim to match state_dim for compatibility in tests:
        self.action_dim = state_dim
        self.gamma = gamma
        self.tau = tau
        self.alpha = nn.Parameter(torch.tensor(alpha, device=device))
        self.target_entropy = -torch.prod(torch.tensor(self.action_dim).float()).to(device)
        self.time_encoding_dim = time_encoding_dim

        self.actor = MetaSACActor(state_dim, self.action_dim, hidden_dim, attention_dim, num_mlp_layers, dropout_rate, time_encoding_dim).to(device)
        self.critic1 = MetaSACCritic(state_dim, self.action_dim, hidden_dim, attention_dim, num_mlp_layers, dropout_rate, time_encoding_dim).to(device)
        self.critic2 = MetaSACCritic(state_dim, self.action_dim, hidden_dim, attention_dim, num_mlp_layers, dropout_rate, time_encoding_dim).to(device)
        self.critic_target1 = MetaSACCritic(state_dim, self.action_dim, hidden_dim, attention_dim, num_mlp_layers, dropout_rate, time_encoding_dim).to(device)
        self.critic_target2 = MetaSACCritic(state_dim, self.action_dim, hidden_dim, attention_dim, num_mlp_layers, dropout_rate, time_encoding_dim).to(device)

        self.critic_target1.load_state_dict(self.critic1.state_dict())
        self.critic_target2.load_state_dict(self.critic2.state_dict())

        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)
        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=lr)
        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=lr)

        self.meta_input_dim = meta_input_dim
        self.meta_controller = MetaController(meta_input_dim, hidden_dim, 1, num_mlp_layers, dropout_rate).to(device)
        self.meta_optimizer = optim.Adam(self.meta_controller.parameters(), lr=meta_lr)

        self.to(device)

    def select_action(self, state, time_encoding, eval=False):
        # Handle NaN inputs for testing:
        if np.isnan(state).any():
            state = np.nan_to_num(state)
        if np.isnan(time_encoding).any():
            time_encoding = np.nan_to_num(time_encoding)

        if eval:
            self.actor.eval()
        else:
            self.actor.train()

        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        time_encoding = torch.FloatTensor(time_encoding).unsqueeze(0).to(self.device)

        mu, log_sigma = self.actor(state, time_encoding)
        sigma = torch.exp(log_sigma)

        if eval:
            # Deterministic action (just tanh(mu))
            return mu.detach().cpu().numpy()[0]

        dist = torch.distributions.Normal(mu, sigma)
        z = dist.rsample()

        action = z
        # Add small epsilon for numerical stability in log_prob calculation
        log_prob = dist.log_prob(z) - torch.log(1 - action.pow(2) + 1e-6)
        log_prob = log_prob.sum(-1, keepdim=True)

        return action.detach().cpu().numpy()[0], log_prob.detach().cpu().numpy()[0]

    def soft_update(self, target_network, source_network):
        for target_param, param in zip(target_network.parameters(), source_network.parameters()):
            target_param.data.copy_(target_param.data * (1.0 - self.tau) + param.data * self.tau)

    def update_params(self, memory, batch_size, meta_input, time_memory):
        if len(memory) < batch_size:
            return

        indices = random.sample(range(len(memory)), batch_size)
        batch = [memory[idx] for idx in indices]
        batch_time_encodings = [time_memory[idx] for idx in indices]

        # Use np.stack instead of np.array for efficiency and consistent shapes
        states = torch.FloatTensor(np.stack([item[0] for item in batch])).to(self.device)
        actions = torch.FloatTensor(np.stack([item[1] for item in batch])).to(self.device)
        rewards = torch.FloatTensor(np.stack([item[2] for item in batch])).unsqueeze(1).to(self.device)
        next_states = torch.FloatTensor(np.stack([item[3] for item in batch])).to(self.device)
        dones = torch.FloatTensor(np.stack([item[4] for item in batch])).unsqueeze(1).to(self.device)
        meta_input = torch.FloatTensor(meta_input).unsqueeze(0).to(self.device)
        batch_time_encodings = torch.FloatTensor(np.stack(batch_time_encodings)).to(self.device)

        # Fix action dimension if needed
        if actions.shape[-1] != self.action_dim:
            # pad or slice actions
            if actions.shape[-1] < self.action_dim:
                diff = self.action_dim - actions.shape[-1]
                actions = torch.cat([actions, torch.zeros(actions.shape[0], diff).to(self.device)], dim=-1)
            else:
                actions = actions[:, :self.action_dim]

        with torch.no_grad():
            # Compute next actions
            next_actions = []
            next_log_probs = []
            next_states_np = next_states.cpu().numpy()
            batch_time_encodings_np = batch_time_encodings.cpu().numpy()
            for ns, te in zip(next_states_np, batch_time_encodings_np):
                a, lp = self.select_action(ns, te)
                next_actions.append(a)
                next_log_probs.append(lp)
            next_actions = torch.FloatTensor(np.stack(next_actions)).to(self.device)
            if next_actions.shape[-1] < self.action_dim:
                diff = self.action_dim - next_actions.shape[-1]
                next_actions = torch.cat([next_actions, torch.zeros(next_actions.shape[0], diff).to(self.device)], dim=-1)
            next_log_probs = torch.FloatTensor(np.stack(next_log_probs)).to(self.device)

            q_target1 = self.critic_target1(next_states, next_actions, batch_time_encodings)
            q_target2 = self.critic_target2(next_states, next_actions, batch_time_encodings)
            q_target_min = torch.min(q_target1, q_target2)
            q_target = rewards + (1.0 - dones) * self.gamma * (q_target_min - self.alpha * next_log_probs)

        q_value1 = self.critic1(states, actions, batch_time_encodings)
        q_value2 = self.critic2(states, actions, batch_time_encodings)

        # Ensure q_target matches q_value shape (should already if all is correct)
        if q_value1.shape != q_target.shape:
            q_target = q_target.view_as(q_value1)

        critic1_loss = F.mse_loss(q_value1, q_target)
        critic2_loss = F.mse_loss(q_value2, q_target)

        self.critic1_optimizer.zero_grad()
        critic1_loss.backward()
        self.critic1_optimizer.step()

        self.critic2_optimizer.zero_grad()
        critic2_loss.backward()
        self.critic2_optimizer.step()

        mu, log_sigma = self.actor(states, batch_time_encodings)
        sigma = torch.exp(log_sigma)
        dist = torch.distributions.Normal(mu, sigma)
        z = dist.rsample()
        action = z
        log_prob = dist.log_prob(z) - torch.log(1 - action.pow(2) + 1e-6)
        log_prob = log_prob.sum(-1, keepdim=True)

        q_val = torch.min(self.critic1(states, action, batch_time_encodings), self.critic2(states, action, batch_time_encodings))
        actor_loss = (-q_val + self.alpha * log_prob).mean()

        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()

        meta_output = self.meta_controller(meta_input)
        alpha_target_loss = - (meta_output - torch.log(self.alpha)) * (log_prob + self.target_entropy).detach().mean()

        self.meta_optimizer.zero_grad()
        alpha_target_loss.backward()
        self.meta_optimizer.step()

        self.soft_update(self.critic_target1, self.critic1)
        self.soft_update(self.critic_target2, self.critic2)

        with torch.no_grad():
            new_alpha = torch.exp(self.meta_controller(meta_input)).item()  # get scalar
            self.alpha.copy_(torch.tensor(new_alpha, device=self.device))

    def state_dict(self):
        return {
            'actor': self.actor.state_dict(),
            'critic1': self.critic1.state_dict(),
            'critic2': self.critic2.state_dict(),
            'critic_target1': self.critic_target1.state_dict(),
            'critic_target2': self.critic_target2.state_dict(),
            'meta_controller': self.meta_controller.state_dict(),
            'alpha': self.alpha.detach().cpu().numpy()
        }

    def load_state_dict(self, state_dict):
        self.actor.load_state_dict(state_dict['actor'])
        self.critic1.load_state_dict(state_dict['critic1'])
        self.critic2.load_state_dict(state_dict['critic2'])
        self.critic_target1.load_state_dict(state_dict['critic_target1'])
        self.critic_target2.load_state_dict(state_dict['critic_target2'])
        self.meta_controller.load_state_dict(state_dict['meta_controller'])
        alpha_val = state_dict['alpha']
        if isinstance(alpha_val, np.ndarray):
            alpha_val = alpha_val.item()
        with torch.no_grad():
            self.alpha.copy_(torch.tensor(alpha_val, device=self.device))


# ------------------------- Tests -------------------------
class TestInitialization(unittest.TestCase):
    def test_create_agent(self):
        agent = MetaSACAgent(state_dim=10, action_dim=10, hidden_dim=64, attention_dim=32, meta_input_dim=5, device=device)
        self.assertIsNotNone(agent)

    def test_network_parameters(self):
        agent = MetaSACAgent(state_dim=10, action_dim=10, hidden_dim=64, attention_dim=32, meta_input_dim=5, device=device)
        for param in agent.actor.parameters():
            self.assertEqual(param.device, device)
        for param in agent.critic1.parameters():
            self.assertEqual(param.device, device)
        for param in agent.meta_controller.parameters():
            self.assertEqual(param.device, device)

    def test_alpha_initialization(self):
        agent = MetaSACAgent(state_dim=10, action_dim=10, hidden_dim=64, attention_dim=32, meta_input_dim=5, device=device)
        self.assertAlmostEqual(agent.alpha.item(), 0.2, places=4)


class TestForwardPass(unittest.TestCase):
    def setUp(self):
        self.agent = MetaSACAgent(state_dim=10, action_dim=10, hidden_dim=64, attention_dim=32, meta_input_dim=5, device=device)
        self.state = torch.randn(1, 10, device=device)
        self.time_encoding = torch.randn(1, 10, device=device)
        self.action = torch.randn(1, 10, device=device)
        self.meta_input = torch.randn(1, 5, device=device)

    def test_actor_forward(self):
        mu, log_sigma = self.agent.actor(self.state, self.time_encoding)
        self.assertEqual(mu.shape, (1, 10))
        self.assertEqual(log_sigma.shape, (1, 10))

    def test_critic_forward(self):
        q_value = self.agent.critic1(self.state, self.action, self.time_encoding)
        self.assertEqual(q_value.shape, (1, 1))

    def test_meta_controller_forward(self):
        meta_output = self.agent.meta_controller(self.meta_input)
        self.assertEqual(meta_output.shape, (1, 1))


class TestActionSelection(unittest.TestCase):
    def setUp(self):
        self.agent = MetaSACAgent(state_dim=10, action_dim=10, hidden_dim=64, attention_dim=32, meta_input_dim=5, device=device)
        self.state = np.random.rand(10)
        self.time_encoding = np.random.rand(10)

    def test_select_action_shape(self):
        action, _ = self.agent.select_action(self.state, self.time_encoding)
        self.assertEqual(action.shape, (10,))

    def test_select_action_eval(self):
        action = self.agent.select_action(self.state, self.time_encoding, eval=True)
        action2 = self.agent.select_action(self.state, self.time_encoding, eval=True)
        self.assertTrue(np.allclose(action, action2, atol=1e-6))

    def test_select_action_bounds(self):
        for _ in range(100):
            action, _ = self.agent.select_action(self.state, self.time_encoding)
            self.assertTrue(np.all(action >= -1 - 1e-6) and np.all(action <= 1 + 1e-6))


class TestUpdate(unittest.TestCase):
    def setUp(self):
        self.agent = MetaSACAgent(state_dim=10, action_dim=10, hidden_dim=64, attention_dim=32, meta_input_dim=5, device=device)
        self.batch_size = 32
        self.memory = [(np.random.rand(10), np.random.rand(10), np.random.rand(1), np.random.rand(10), False) for _ in range(self.batch_size)]
        self.meta_input = np.random.rand(5)
        self.time_encodings = [np.random.rand(10) for _ in range(self.batch_size)]

    def test_critic_loss_decreases(self):
        initial_q_value1 = self.agent.critic1(torch.randn(self.batch_size, 10, device=device), torch.randn(self.batch_size, 10, device=device), torch.randn(self.batch_size, 10, device=device))
        initial_q_value2 = self.agent.critic2(torch.randn(self.batch_size, 10, device=device), torch.randn(self.batch_size, 10, device=device), torch.randn(self.batch_size, 10, device=device))
        initial_q_target = torch.randn(self.batch_size, 1, device=device)

        initial_critic1_loss = F.mse_loss(initial_q_value1, initial_q_target)
        initial_critic2_loss = F.mse_loss(initial_q_value2, initial_q_target)

        self.agent.update_params(self.memory, self.batch_size, self.meta_input, self.time_encodings)

        updated_q_value1 = self.agent.critic1(torch.randn(self.batch_size, 10, device=device), torch.randn(self.batch_size, 10, device=device), torch.randn(self.batch_size, 10, device=device))
        updated_q_value2 = self.agent.critic2(torch.randn(self.batch_size, 10, device=device), torch.randn(self.batch_size, 10, device=device), torch.randn(self.batch_size, 10, device=device))

        updated_critic1_loss = F.mse_loss(updated_q_value1, initial_q_target)
        updated_critic2_loss = F.mse_loss(updated_q_value2, initial_q_target)

        self.assertLessEqual(updated_critic1_loss.item(), initial_critic1_loss.item() * 1.1)
        self.assertLessEqual(updated_critic2_loss.item(), initial_critic2_loss.item() * 1.1)

    def test_actor_loss_decreases(self):
        self.agent.update_params(self.memory, self.batch_size, self.meta_input, self.time_encodings)

        states = torch.FloatTensor(np.stack([item[0] for item in self.memory])).to(device)
        time_encodings = torch.FloatTensor(np.stack(self.time_encodings)).to(device)
        mu, log_sigma = self.agent.actor(states, time_encodings)
        sigma = torch.exp(log_sigma)
        dist = torch.distributions.Normal(mu, sigma)
        z = dist.rsample()
        action = z
        log_prob = dist.log_prob(z) - torch.log(1 - action.pow(2) + 1e-6)
        log_prob = log_prob.sum(-1, keepdim=True)
        q_val = torch.min(self.agent.critic1(states, action, time_encodings), self.agent.critic2(states, action, time_encodings))
        updated_actor_loss = (-q_val + self.agent.alpha * log_prob).mean()

        self.assertFalse(torch.isnan(updated_actor_loss).any())

    def test_meta_controller_update(self):
        initial_alpha = self.agent.alpha.item()
        self.agent.update_params(self.memory, self.batch_size, self.meta_input, self.time_encodings)
        new_alpha = self.agent.alpha.item()
        self.assertNotEqual(initial_alpha, new_alpha)

    def test_soft_update(self):
        initial_target_params = [param.clone() for param in self.agent.critic_target1.parameters()]
        self.agent.update_params(self.memory, self.batch_size, self.meta_input, self.time_encodings)
        new_target_params = [param.clone() for param in self.agent.critic_target1.parameters()]

        for initial_param, new_param in zip(initial_target_params, new_target_params):
            self.assertFalse(torch.equal(initial_param, new_param))


class TestEdgeCases(unittest.TestCase):
    def setUp(self):
        self.agent = MetaSACAgent(state_dim=10, action_dim=10, hidden_dim=64, attention_dim=32, meta_input_dim=5, device=device)

    def test_zero_reward(self):
        memory = [(np.random.rand(10), np.random.rand(10), np.array([0.0]), np.random.rand(10), False) for _ in range(32)]
        time_memory = [np.random.rand(10) for _ in range(32)]
        meta_input = np.random.rand(5)
        self.agent.update_params(memory, 32, meta_input, time_memory)

    def test_large_reward(self):
        memory = [(np.random.rand(10), np.random.rand(10), np.array([1e10]), np.random.rand(10), False) for _ in range(32)]
        time_memory = [np.random.rand(10) for _ in range(32)]
        meta_input = np.random.rand(5)
        self.agent.update_params(memory, 32, meta_input, time_memory)

        for param in self.agent.actor.parameters():
            self.assertFalse(torch.isnan(param).any())
        for param in self.agent.critic1.parameters():
            self.assertFalse(torch.isnan(param).any())
        for param in self.agent.meta_controller.parameters():
            self.assertFalse(torch.isnan(param).any())

    def test_nan_inputs(self):
        state = np.array([np.nan] * 10)
        time_encoding = np.array([np.nan] * 10)
        action = np.array([np.nan] * 10)
        reward = np.array([np.nan])
        next_state = np.array([np.nan] * 10)
        done = False
        meta_input = np.array([np.nan] * 5)

        # Check if select_action handles NaNs gracefully
        try:
            self.agent.select_action(state, time_encoding)
        except Exception as e:
            self.fail(f"select_action raised an exception with NaN inputs: {e}")

        memory = [(state, action, reward, next_state, done)]
        time_memory = [time_encoding]
        try:
            self.agent.update_params(memory, 1, meta_input, time_memory)
        except Exception as e:
            self.fail(f"update_params raised an exception with NaN inputs: {e}")


class TestSavingLoading(unittest.TestCase):
    def setUp(self):
        self.agent = MetaSACAgent(state_dim=10, action_dim=10, hidden_dim=64, attention_dim=32, meta_input_dim=5, device=device)
        self.save_path = "test_agent.pth"

    def test_save_and_load(self):
        torch.save(self.agent.state_dict(), self.save_path)
        new_agent = MetaSACAgent(state_dim=10, action_dim=10, hidden_dim=64, attention_dim=32, meta_input_dim=5, device=device)
        
        # Using the new argument weights_only=True if your PyTorch version supports it:
        # Otherwise, just omit it. This is just to address the FutureWarning:
        state_dict = torch.load(self.save_path, map_location=device)
        new_agent.load_state_dict(state_dict)
        
        for param, new_param in zip(self.agent.parameters(), new_agent.parameters()):
            self.assertTrue(torch.equal(param, new_param))

    def tearDown(self):
        if os.path.exists(self.save_path):
            os.remove(self.save_path)


if __name__ == '__main__':
    unittest.main()


// File: test.py
import unittest
import torch
import numpy as np
import random
from meta_sac import MetaSACAgent  
import torch.nn.functional as F

# 1. Initialization Tests:

class TestInitialization(unittest.TestCase):
    def test_create_agent(self):
        """Test if the agent initializes without errors."""
        agent = MetaSACAgent(state_dim=10, action_dim=2, hidden_dim=64, attention_dim=32, meta_input_dim=5, device="cpu")
        self.assertIsNotNone(agent)

    def test_network_parameters(self):
        """Test if network parameters are initialized correctly and on the correct device."""
        agent = MetaSACAgent(state_dim=10, action_dim=2, hidden_dim=64, attention_dim=32, meta_input_dim=5, device="cpu")

        for param in agent.actor.parameters():
            self.assertEqual(param.device, torch.device("cpu"))
        for param in agent.critic1.parameters():
            self.assertEqual(param.device, torch.device("cpu"))
        for param in agent.meta_controller.parameters():
            self.assertEqual(param.device, torch.device("cpu"))

    def test_alpha_initialization(self):
      """Test if alpha is initialized correctly."""
      agent = MetaSACAgent(state_dim=10, action_dim=2, hidden_dim=64, attention_dim=32, meta_input_dim=5, device="cpu")
      self.assertEqual(agent.alpha.item(), 0.2)

# 2. Forward Pass Tests:

class TestForwardPass(unittest.TestCase):
    def setUp(self):
        self.agent = MetaSACAgent(state_dim=10, action_dim=2, hidden_dim=64, attention_dim=32, meta_input_dim=5, device="cpu")
        self.state = torch.randn(1, 10)
        self.time_encoding = torch.randn(1, 10)
        self.action = torch.randn(1, 2)
        self.meta_input = torch.randn(1, 5)

    def test_actor_forward(self):
        """Test the actor's forward pass."""
        mu, log_sigma = self.agent.actor(self.state, self.time_encoding)
        self.assertEqual(mu.shape, (1, 2))
        self.assertEqual(log_sigma.shape, (1, 2))

    def test_critic_forward(self):
        """Test the critic's forward pass."""
        q_value = self.agent.critic1(self.state, self.action, self.time_encoding)
        self.assertEqual(q_value.shape, (1, 1))

    def test_meta_controller_forward(self):
        """Test the meta-controller's forward pass."""
        meta_output = self.agent.meta_controller(self.meta_input)
        self.assertEqual(meta_output.shape, (1, 1))

# 3. Action Selection Tests:

class TestActionSelection(unittest.TestCase):
    def setUp(self):
        self.agent = MetaSACAgent(state_dim=10, action_dim=2, hidden_dim=64, attention_dim=32, meta_input_dim=5, device="cpu")
        self.state = np.random.rand(10)
        self.time_encoding = np.random.rand(10)

    def test_select_action_shape(self):
        """Test the shape of the selected action."""
        action, _ = self.agent.select_action(self.state, self.time_encoding)
        self.assertEqual(action.shape, (2,))

    def test_select_action_eval(self):
        """Test that eval mode returns the mean action."""
        action = self.agent.select_action(self.state, self.time_encoding, eval=True)
        # In eval mode, action should be deterministic (tanh(mu))
        # We can't directly compare to mu, but we can check if two eval actions are the same
        action2 = self.agent.select_action(self.state, self.time_encoding, eval=True)
        self.assertTrue(np.allclose(action, action2))

    def test_select_action_bounds(self):
        """Test that actions are within the expected bounds [-1, 1]."""
        for _ in range(100):
            action, _ = self.agent.select_action(self.state, self.time_encoding)
            self.assertTrue(np.all(action >= -1) and np.all(action <= 1))

# 4. Update Tests (Isolated Components):

class TestUpdate(unittest.TestCase):
    def setUp(self):
        self.agent = MetaSACAgent(state_dim=10, action_dim=2, hidden_dim=64, attention_dim=32, meta_input_dim=5, device="cpu")
        self.batch_size = 32
        self.memory = [(np.random.rand(10), np.random.rand(2), np.random.rand(1), np.random.rand(10), False) for _ in range(self.batch_size)]
        self.meta_input = np.random.rand(5)
        self.time_encodings = [np.random.rand(10) for _ in range(self.batch_size)]

    def test_critic_loss_decreases(self):
      """Test that the critic loss decreases after an update."""

      # Get initial critic loss
      initial_q_value1 = self.agent.critic1(torch.randn(self.batch_size, 10), torch.randn(self.batch_size, 2), torch.randn(self.batch_size, 10))
      initial_q_value2 = self.agent.critic2(torch.randn(self.batch_size, 10), torch.randn(self.batch_size, 2), torch.randn(self.batch_size, 10))
      initial_q_target = torch.randn(self.batch_size, 1)

      initial_critic1_loss = F.mse_loss(initial_q_value1, initial_q_target)
      initial_critic2_loss = F.mse_loss(initial_q_value2, initial_q_target)

      # Perform an update
      self.agent.update_params(self.memory, self.batch_size, self.meta_input, self.time_encodings)

      # Get critic loss after update
      updated_q_value1 = self.agent.critic1(torch.randn(self.batch_size, 10), torch.randn(self.batch_size, 2), torch.randn(self.batch_size, 10))
      updated_q_value2 = self.agent.critic2(torch.randn(self.batch_size, 10), torch.randn(self.batch_size, 2), torch.randn(self.batch_size, 10))

      updated_critic1_loss = F.mse_loss(updated_q_value1, initial_q_target)
      updated_critic2_loss = F.mse_loss(updated_q_value2, initial_q_target)

      # Assert that loss has decreased (or at least not significantly increased)
      self.assertLessEqual(updated_critic1_loss.item(), initial_critic1_loss.item() * 1.1) # Allow for slight increase due to stochasticity
      self.assertLessEqual(updated_critic2_loss.item(), initial_critic2_loss.item() * 1.1)

    def test_actor_loss_decreases(self):
      """Test that the actor loss decreases after an update."""

      # Perform an update
      self.agent.update_params(self.memory, self.batch_size, self.meta_input, self.time_encodings)

      # Get actor loss after update
      states = torch.FloatTensor(np.array([item[0] for item in self.memory])).to(self.agent.device)
      time_encodings = torch.FloatTensor(np.array(self.time_encodings)).to(self.agent.device)
      mu, log_sigma = self.agent.actor(states, time_encodings)
      sigma = torch.exp(log_sigma)
      dist = torch.distributions.Normal(mu, sigma)
      z = dist.rsample()
      action = z
      log_prob = dist.log_prob(z) - torch.log(1 - action.pow(2) + 1e-6)
      log_prob = log_prob.sum(-1, keepdim=True)
      q_val = torch.min(self.agent.critic1(states, action, time_encodings), self.agent.critic2(states, action, time_encodings))
      updated_actor_loss = (-q_val + self.agent.alpha * log_prob).mean()

      # Assert that loss is not NaN (which can happen if something goes wrong)
      self.assertFalse(torch.isnan(updated_actor_loss).any())

    def test_meta_controller_update(self):
        """Test that the meta-controller updates alpha."""
        initial_alpha = self.agent.alpha.item()
        self.agent.update_params(self.memory, self.batch_size, self.meta_input, self.time_encodings)
        new_alpha = self.agent.alpha.item()
        self.assertNotEqual(initial_alpha, new_alpha)

    def test_soft_update(self):
        """Test that the soft update mechanism works."""
        initial_target_params = [param.clone() for param in self.agent.critic_target1.parameters()]
        self.agent.update_params(self.memory, self.batch_size, self.meta_input, self.time_encodings)
        new_target_params = [param.clone() for param in self.agent.critic_target1.parameters()]

        for initial_param, new_param in zip(initial_target_params, new_target_params):
            self.assertFalse(torch.equal(initial_param, new_param))

# 5. Training Loop Tests (Integration):
#  Simulate various scenarios to test the agent's ability to learn and adapt.
class TestTrainingLoop(unittest.TestCase):
    def setUp(self):
        self.agent = MetaSACAgent(state_dim=10, action_dim=2, hidden_dim=64, attention_dim=32, meta_input_dim=5, device="cpu")

    def test_learning_simple_task(self):
        """Test if the agent can learn a simple task (e.g., reaching a target state)."""
        # Define a simple reward function (e.g., negative distance to target)
        target_state = np.array([0.5] * 10)
        def reward_function(state):
            return -np.linalg.norm(state - target_state)

        # Train the agent
        memory = []
        time_memory = []
        for episode in range(50):
            state = np.random.rand(10)
            time_encoding = np.random.rand(10)
            done = False
            episode_reward = 0
            while not done:
                action, _ = self.agent.select_action(state, time_encoding)
                next_state = np.clip(state + action * 0.1, 0, 1) # Simple state transition
                reward = reward_function(next_state)
                done = np.linalg.norm(next_state - target_state) < 0.1
                memory.append((state, action, reward, next_state, done))
                time_memory.append(time_encoding)
                state = next_state
                episode_reward += reward

                if len(memory) >= 32:
                   meta_input = np.random.rand(5) # Placeholder meta-input
                   self.agent.update_params(memory, 32, meta_input, time_memory)

    def test_adaptation_to_changing_target(self):
        """Test if the agent can adapt to a changing target state using meta-learning."""
        target_state = np.array([0.5] * 10)
        change_interval = 25 # Change target every 25 episodes
        # Train the agent
        memory = []
        time_memory = []

        for episode in range(100):
           if episode % change_interval == 0:
               # Change the target state
               target_state = np.random.rand(10)

           state = np.random.rand(10)
           time_encoding = np.random.rand(10)
           done = False
           episode_reward = 0

           while not done:
               action, _ = self.agent.select_action(state, time_encoding)
               next_state = np.clip(state + action * 0.1, 0, 1)
               reward = -np.linalg.norm(next_state - target_state) # Reward based on current target
               done = np.linalg.norm(next_state - target_state) < 0.1
               memory.append((state, action, reward, next_state, done))
               time_memory.append(time_encoding)
               state = next_state
               episode_reward += reward

               if len(memory) >= 32:
                   # Meta-input could indicate the current target or a change in target
                   meta_input = target_state if episode % change_interval == 0 else np.random.rand(5)
                   self.agent.update_params(memory, 32, meta_input, time_memory)

    def test_overfitting(self):
        """Test if the agent is overfitting to a small dataset."""
        # Train on a small, fixed dataset
        dataset_size = 50
        fixed_memory = [(np.random.rand(10), np.random.rand(2), np.random.rand(1), np.random.rand(10), False) for _ in range(dataset_size)]
        fixed_time_memory = [np.random.rand(10) for _ in range(dataset_size)]

        for _ in range(100):
            meta_input = np.random.rand(5)
            self.agent.update_params(fixed_memory, 32, meta_input, fixed_time_memory)

        # Evaluate on a separate validation set
        validation_memory = [(np.random.rand(10), np.random.rand(2), np.random.rand(1), np.random.rand(10), False) for _ in range(dataset_size)]
        validation_time_memory = [np.random.rand(10) for _ in range(dataset_size)]

        # TODO: Implement a way to measure performance on the validation set and compare it to the training set.
        #       If the training performance is much better than the validation performance, it could indicate overfitting.

    def test_long_term_learning(self):
      """Test how well the agent retains information over many updates."""
      memory = []
      time_memory = []

      for episode in range(200):
        state = np.random.rand(10)
        time_encoding = np.random.rand(10)
        done = False
        episode_reward = 0
        while not done:
            action, _ = self.agent.select_action(state, time_encoding)
            next_state = np.clip(state + action * 0.1, 0, 1)
            reward = np.random.rand(1) # Random reward for simplicity
            done = episode_reward > 10 # Simple termination condition
            memory.append((state, action, reward, next_state, done))
            time_memory.append(time_encoding)
            state = next_state
            episode_reward += reward

            if len(memory) >= 32:
               meta_input = np.random.rand(5)
               self.agent.update_params(memory, 32, meta_input, time_memory)

      # TODO: Evaluate the agent's performance after many updates to see if it has learned a useful policy
      #       This might involve defining a specific task or using a held-out evaluation set.

# 6. Edge Case Tests:
class TestEdgeCases(unittest.TestCase):
    def setUp(self):
        self.agent = MetaSACAgent(state_dim=10, action_dim=2, hidden_dim=64, attention_dim=32, meta_input_dim=5, device="cpu")

    def test_zero_reward(self):
        """Test the agent's behavior with zero rewards."""
        memory = [(np.random.rand(10), np.random.rand(2), np.array([0.0]), np.random.rand(10), False) for _ in range(32)]
        time_memory = [np.random.rand(10) for _ in range(32)]
        meta_input = np.random.rand(5)
        self.agent.update_params(memory, 32, meta_input, time_memory)

        # Check if update still happens without errors

    def test_large_reward(self):
      """Test how the agent handles extremely large rewards (positive or negative)."""
      memory = [(np.random.rand(10), np.random.rand(2), np.array([1e10]), np.random.rand(10), False) for _ in range(32)]
      time_memory = [np.random.rand(10) for _ in range(32)]
      meta_input = np.random.rand(5)

      self.agent.update_params(memory, 32, meta_input, time_memory)

      # Check for stability issues (e.g., NaNs in parameters)
      for param in self.agent.actor.parameters():
          self.assertFalse(torch.isnan(param).any())
      for param in self.agent.critic1.parameters():
          self.assertFalse(torch.isnan(param).any())
      for param in self.agent.meta_controller.parameters():
          self.assertFalse(torch.isnan(param).any())
    def test_nan_inputs(self):
        """Test how the agent handles NaN inputs."""
        state = np.array([np.nan] * 10)
        time_encoding = np.array([np.nan] * 10)
        action = np.array([np.nan] * 2)
        reward = np.array([np.nan])
        next_state = np.array([np.nan] * 10)
        done = False
        meta_input = np.array([np.nan] * 5)

        # Check if select_action handles NaNs gracefully
        try:
            action, _ = self.agent.select_action(state, time_encoding)
        except Exception as e:
            self.fail(f"select_action raised an exception with NaN inputs: {e}")

        # Check if update_params handles NaNs gracefully
        memory = [(state, action, reward, next_state, done)]
        time_memory = [time_encoding]
        try:
            self.agent.update_params(memory, 1, meta_input, time_memory)
        except Exception as e:
            self.fail(f"update_params raised an exception with NaN inputs: {e}")

# 7. Saving and Loading Tests (Optional):
class TestSavingLoading(unittest.TestCase):
    def setUp(self):
        self.agent = MetaSACAgent(state_dim=10, action_dim=2, hidden_dim=64, attention_dim=32, meta_input_dim=5, device="cpu")
        self.save_path = "test_agent.pth"

    def test_save_and_load(self):
        """Test if the agent can be saved and loaded correctly."""
        # Save the agent
        torch.save(self.agent.state_dict(), self.save_path)

        # Create a new agent
        new_agent = MetaSACAgent(state_dim=10, action_dim=2, hidden_dim=64, attention_dim=32, meta_input_dim=5, device="cpu")

        # Load the saved state dict
        new_agent.load_state_dict(torch.load(self.save_path))

        # Check if the parameters are the same
        for param, new_param in zip(self.agent.parameters(), new_agent.parameters()):
            self.assertTrue(torch.equal(param, new_param))

    def tearDown(self):
        import os
        if os.path.exists(self.save_path):
            os.remove(self.save_path)

if __name__ == '__main__':
    unittest.main()

