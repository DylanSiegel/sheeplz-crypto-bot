**File Tree (Relevant Files Only)**
  .
    - agent.py
    - config.py
    - conftest.py
    - networks.py
    - replay_buffer.py
    - reward.py
    - train.py
  env
    - environment.py
// File: agent.py
# agent.py
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import logging
from typing import Dict, List, Tuple, Callable
from torch.optim import Adam
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.utils.tensorboard import SummaryWriter

from config import EnvironmentConfig
from env.environment import HistoricalEnvironment
from networks import (
    MetaSACActor, 
    MetaSACCritic, 
    MetaController, 
    PolicyDistiller, 
    MarketModeClassifier, 
    HighLevelPolicy
)
from replay_buffer import ReplayBuffer

logger = logging.getLogger(__name__)

class MetaSACAgent(nn.Module):
    """Meta Soft Actor-Critic (SAC) agent with training-time search."""
    def __init__(self, config: EnvironmentConfig, env: HistoricalEnvironment):
        super().__init__()
        self.config = config
        self.device = torch.device(config.device)
        self.env = env

        # Actor & Critics
        self.actor = MetaSACActor(config).to(self.device)
        self.critic1 = MetaSACCritic(config).to(self.device)
        self.critic2 = MetaSACCritic(config).to(self.device)
        self.critic_target1 = MetaSACCritic(config).to(self.device)
        self.critic_target2 = MetaSACCritic(config).to(self.device)

        self.critic_target1.load_state_dict(self.critic1.state_dict())
        self.critic_target2.load_state_dict(self.critic2.state_dict())

        # Meta-Controller
        self.meta_controller = MetaController(config).to(self.device)
        
        # Policy Distiller & Specialists
        self.specialist_policies = [MetaSACActor(config).to(self.device) for _ in range(2)]
        self.policy_distiller = PolicyDistiller(self.specialist_policies).to(self.device)

        # Market Mode Classifier & High-Level Policy
        self.market_mode_classifier = MarketModeClassifier(
            config.state_dim, config.hidden_dim, config.num_market_modes
        ).to(self.device)
        self.high_level_policy = HighLevelPolicy(config.state_dim, config.hidden_dim).to(self.device)

        # Optimizers
        self.actor_optimizer = Adam(self.actor.parameters(), lr=config.lr)
        self.critic1_optimizer = Adam(self.critic1.parameters(), lr=config.lr)
        self.critic2_optimizer = Adam(self.critic2.parameters(), lr=config.lr)
        self.meta_optimizer = Adam(self.meta_controller.parameters(), lr=config.meta_lr)
        self.distiller_optimizer = Adam(self.policy_distiller.parameters(), lr=config.lr)
        self.market_mode_optimizer = Adam(self.market_mode_classifier.parameters(), lr=config.lr)
        self.high_level_optimizer = Adam(self.high_level_policy.parameters(), lr=config.lr)

        # LR Schedulers
        self.actor_scheduler = ReduceLROnPlateau(self.actor_optimizer, mode="min", factor=0.5, patience=5)
        self.critic1_scheduler = ReduceLROnPlateau(self.critic1_optimizer, mode="min", factor=0.5, patience=5)
        self.critic2_scheduler = ReduceLROnPlateau(self.critic2_optimizer, mode="min", factor=0.5, patience=5)
        self.high_level_scheduler = ReduceLROnPlateau(self.high_level_optimizer, mode="min", factor=0.5, patience=5)
        self.meta_scheduler = ReduceLROnPlateau(self.meta_optimizer, mode="min", factor=0.5, patience=5)
        self.distiller_scheduler = ReduceLROnPlateau(self.distiller_optimizer, mode="min", factor=0.5, patience=5)

        # Entropy coefficient alpha
        self.alpha = nn.Parameter(torch.tensor(config.alpha, dtype=torch.float32, device=self.device))
        self.target_entropy = -float(config.action_dim) * config.target_entropy_factor

        # Replay Buffer
        self.replay_buffer = ReplayBuffer(config.buffer_capacity)

        # Logging
        self.writer = SummaryWriter()
        self.train_steps = 0

    def select_action(self, state: np.ndarray, time_step: int, eval=False) -> np.ndarray:
        """Selects action with policy or policy distiller (training)."""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        time_tensor = torch.tensor([time_step], dtype=torch.float32).to(self.device)
        if eval:
            with torch.no_grad():
                mu, _log_sigma = self.actor(state_tensor, time_tensor)
                action = mu
        else:
            with torch.no_grad():
                mu, log_sigma = self.policy_distiller(state_tensor, time_tensor)
                sigma = torch.exp(log_sigma)
                dist = torch.distributions.Normal(mu, sigma)
                z = dist.rsample()
                action = torch.tanh(z)
        action = action.clamp(-1, 1)
        return action.cpu().numpy().squeeze(0)

    def compute_q_targets(self, 
                          rewards: torch.Tensor, 
                          next_states: torch.Tensor, 
                          time_steps: torch.Tensor, 
                          dones: torch.Tensor,
                          r_scaling: torch.Tensor,
                          market_mode_probs: torch.Tensor) -> torch.Tensor:
        """Computes Q-targets with reward scaling."""
        with torch.no_grad():
            mu, log_sigma = self.actor(next_states, time_steps)
            sigma = torch.exp(log_sigma)
            dist = torch.distributions.Normal(mu, sigma)
            next_actions = torch.tanh(dist.rsample())
            log_probs = dist.log_prob(next_actions) - torch.log(1 - next_actions.pow(2) + self.config.epsilon)
            next_log_probs = log_probs.sum(-1, keepdim=True)

            q_target1 = self.critic_target1(next_states, next_actions, time_steps)
            q_target2 = self.critic_target2(next_states, next_actions, time_steps)
            q_target = torch.min(q_target1, q_target2)

            scaled_rewards = (
                r_scaling[:, 0:1]*rewards*market_mode_probs[:, 0:1] +
                r_scaling[:, 1:2]*rewards*market_mode_probs[:, 1:2] +
                r_scaling[:, 2:3]*rewards*market_mode_probs[:, 2:3]
            )
            y = scaled_rewards + (1.0 - dones)*self.config.gamma*(q_target - self.alpha*next_log_probs)
            return y

    def update_critics(self, states, actions, time_steps, q_targets) -> Tuple[float, float]:
        """Updates critic networks."""
        q1 = self.critic1(states, actions, time_steps)
        q2 = self.critic2(states, actions, time_steps)
        critic1_loss = F.mse_loss(q1, q_targets)
        critic2_loss = F.mse_loss(q2, q_targets)

        self.critic1_optimizer.zero_grad()
        critic1_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.critic1.parameters(), self.config.max_grad_norm)
        self.critic1_optimizer.step()

        self.critic2_optimizer.zero_grad()
        critic2_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.critic2.parameters(), self.config.max_grad_norm)
        self.critic2_optimizer.step()

        return critic1_loss.item(), critic2_loss.item()

    def update_actor(self, states, time_steps) -> float:
        """Updates actor network."""
        mu, log_sigma = self.actor(states, time_steps)
        sigma = torch.exp(log_sigma)
        dist = torch.distributions.Normal(mu, sigma)
        z = dist.rsample()
        actions = torch.tanh(z)
        log_probs = dist.log_prob(z) - torch.log(1 - actions.pow(2) + self.config.epsilon)
        log_probs = log_probs.sum(-1, keepdim=True)

        q_val = torch.min(
            self.critic1(states, actions, time_steps),
            self.critic2(states, actions, time_steps)
        )
        actor_loss = (self.alpha * log_probs - q_val).mean()

        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.config.max_grad_norm)
        self.actor_optimizer.step()

        return actor_loss.item()

    def update_distiller(self, states, time_steps) -> float:
        """Updates policy distiller."""
        mu, log_sigma = self.policy_distiller(states, time_steps)
        sigma = torch.exp(log_sigma)
        dist = torch.distributions.Normal(mu, sigma)
        z = dist.rsample()
        actions = torch.tanh(z)
        log_probs = dist.log_prob(z) - torch.log(1 - actions.pow(2) + self.config.epsilon)
        log_probs = log_probs.sum(-1, keepdim=True)

        q_val = torch.min(
            self.critic1(states, actions, time_steps),
            self.critic2(states, actions, time_steps)
        )
        distiller_loss = (self.alpha * log_probs - q_val).mean()

        self.distiller_optimizer.zero_grad()
        distiller_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.policy_distiller.parameters(), self.config.max_grad_norm)
        self.distiller_optimizer.step()

        return distiller_loss.item()

    def update_market_mode_classifier(self, states, market_modes) -> float:
        """Updates market mode classifier."""
        preds = self.market_mode_classifier(states)
        loss = F.cross_entropy(preds, market_modes)
        self.market_mode_optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.market_mode_classifier.parameters(), self.config.max_grad_norm)
        self.market_mode_optimizer.step()
        return loss.item()

    def update_meta_controller(self, meta_input_tensor, log_probs, rewards) -> Tuple[float, torch.Tensor]:
        """Updates meta-controller."""
        mean_r = torch.mean(rewards, dim=0, keepdim=True)
        var_r = torch.var(rewards, dim=0, keepdim=True)
        reward_stats = torch.cat([mean_r, var_r], dim=-1).to(self.device)
        batch_size = meta_input_tensor.size(0)
        reward_stats = reward_stats.repeat(batch_size, 1)

        out = self.meta_controller(meta_input_tensor, reward_stats)
        learning_rate_alpha = out[2]

        alpha_target_loss = -(learning_rate_alpha - torch.log(self.alpha)) * (log_probs + self.target_entropy).detach().mean()

        self.meta_optimizer.zero_grad()
        alpha_target_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.meta_controller.parameters(), self.config.max_grad_norm)
        self.meta_optimizer.step()

        r_scaling = torch.stack([out[5], out[6], out[7]], dim=-1)
        return alpha_target_loss.item(), r_scaling

    def update_high_level_policy(self, states, advantages) -> float:
        """Updates high-level policy."""
        probs = self.high_level_policy(states)
        log_probs = torch.log(probs + 1e-10)
        policy_loss = -(log_probs * advantages).mean()

        self.high_level_optimizer.zero_grad()
        policy_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.high_level_policy.parameters(), self.config.max_grad_norm)
        self.high_level_optimizer.step()
        return policy_loss.item()

    def soft_update(self, target_net, source_net) -> None:
        """Soft updates target network parameters."""
        for tp, sp in zip(target_net.parameters(), source_net.parameters()):
            tp.data.copy_(tp.data*(1-self.config.tau) + sp.data*self.config.tau)

    def update_params_with_training_time_search(
        self,
        replay_buffer: ReplayBuffer,
        meta_input: np.ndarray,
        time_memory: List[int],
        update_steps: int = 1,
        search_algorithm: str = "best-of-n",
        num_samples: int = 4,
        beam_width: int = 3,
        search_depth: int = 5,
        use_d_search: bool = False,
        exploration_noise_std_fn: Callable[[int], float] = lambda step: 0.0
    ) -> Dict[str, float]:
        """Updates parameters using search-generated data."""
        final_info = {}
        for _ in range(update_steps):
            if len(replay_buffer) < self.config.batch_size:
                return final_info

            batch, _, _ = replay_buffer.sample(self.config.batch_size)
            sampled_states = torch.FloatTensor(np.stack([b[0] for b in batch])).to(self.device)

            search_trajectories = []
            for state in sampled_states:
                exploration_noise_std = exploration_noise_std_fn(self.train_steps)
                if search_algorithm == "best-of-n":
                    all_trajectories = self.perform_best_of_n_search(
                        state, num_samples, search_depth, time_memory, exploration_noise_std
                    )
                    if use_d_search:
                        for traj in all_trajectories:
                            search_trajectories.extend(traj)
                    else:
                        best_traj = max(all_trajectories, key=lambda t: sum(step[2] for step in t))
                        search_trajectories.extend(best_traj)
                elif search_algorithm == "beam-search":
                    best_traj = self.perform_beam_search(
                        state, beam_width, search_depth, time_memory, exploration_noise_std
                    )
                    search_trajectories.extend(best_traj)
                else:
                    raise ValueError(f"Unknown search algorithm: {search_algorithm}")

            if not search_trajectories:
                continue

            states_search = torch.FloatTensor([st[0] for st in search_trajectories]).to(self.device)
            actions_search = torch.FloatTensor([st[1] for st in search_trajectories]).to(self.device)
            rewards_search = torch.FloatTensor([st[2] for st in search_trajectories]).unsqueeze(1).to(self.device)
            next_states_search = torch.FloatTensor([st[3] for st in search_trajectories]).to(self.device)
            dones_search = torch.FloatTensor([st[4] for st in search_trajectories]).unsqueeze(1).to(self.device)
            time_steps_search = torch.FloatTensor([st[5] for st in search_trajectories]).to(self.device)

            # Market mode classification (random labels for demonstration)
            random_modes = torch.randint(0, self.config.num_market_modes, (states_search.size(0),)).to(self.device)
            market_mode_loss = self.update_market_mode_classifier(states_search, random_modes)

            # Meta-Controller
            meta_input_tensor = torch.FloatTensor(meta_input).to(self.device)
            with torch.no_grad():
                mu, log_sigma = self.actor(states_search, time_steps_search)
                sigma = torch.exp(log_sigma)
                dist = torch.distributions.Normal(mu, sigma)
                z = dist.rsample()
                actions_ = torch.tanh(z)
                log_probs = dist.log_prob(z) - torch.log(1 - actions_.pow(2) + self.config.epsilon)
                log_probs = log_probs.sum(-1, keepdim=True)

            meta_loss_val, r_scaling = self.update_meta_controller(meta_input_tensor, log_probs, rewards_search)
            market_mode_probs = self.market_mode_classifier(states_search)
            q_targets = self.compute_q_targets(rewards_search, next_states_search, time_steps_search, dones_search, r_scaling, market_mode_probs)

            # Critics
            critic1_loss, critic2_loss = self.update_critics(states_search, actions_search, time_steps_search, q_targets)

            # Actor & Distiller
            actor_loss_val = self.update_actor(states_search, time_steps_search)
            distiller_loss_val = self.update_distiller(states_search, time_steps_search)

            # High-level policy
            advantages = q_targets - torch.mean(q_targets, dim=0, keepdim=True)
            high_level_loss_val = self.update_high_level_policy(states_search, advantages)

            # Soft-update critics
            self.soft_update(self.critic_target1, self.critic1)
            self.soft_update(self.critic_target2, self.critic2)

            # LR schedulers
            self.actor_scheduler.step(actor_loss_val)
            self.critic1_scheduler.step(critic1_loss)
            self.critic2_scheduler.step(critic2_loss)
            self.high_level_scheduler.step(high_level_loss_val)
            self.meta_scheduler.step(meta_loss_val)
            self.distiller_scheduler.step(distiller_loss_val)

            self.train_steps += 1
            self.writer.add_scalar("Loss/actor", actor_loss_val, self.train_steps)
            self.writer.add_scalar("Loss/critic1", critic1_loss, self.train_steps)
            self.writer.add_scalar("Loss/critic2", critic2_loss, self.train_steps)
            self.writer.add_scalar("Loss/meta", meta_loss_val, self.train_steps)
            self.writer.add_scalar("Loss/distiller", distiller_loss_val, self.train_steps)
            self.writer.add_scalar("Loss/market_mode", market_mode_loss, self.train_steps)
            self.writer.add_scalar("Loss/high_level_policy", high_level_loss_val, self.train_steps)
            self.writer.add_scalar("Params/alpha", self.alpha.item(), self.train_steps)

            final_info = {
                "actor_loss": actor_loss_val,
                "critic1_loss": critic1_loss,
                "critic2_loss": critic2_loss,
                "meta_loss": meta_loss_val,
                "distiller_loss": distiller_loss_val,
                "market_mode_loss": market_mode_loss,
                "high_level_loss": high_level_loss_val,
                "alpha": self.alpha.item()
            }

        return final_info

    def perform_best_of_n_search(
        self,
        initial_state: torch.Tensor,
        num_samples: int,
        search_depth: int,
        time_memory: List[int],
        exploration_noise_std: float = 0.0
    ) -> List[List[Tuple[np.ndarray, np.ndarray, float, np.ndarray, bool, int]]]:
        """Samples trajectories using Best-of-N search."""
        all_trajectories = []
        for _ in range(num_samples):
            state = initial_state.clone()
            trajectory = []
            time_idx = time_memory[-1]
            for _step in range(search_depth):
                with torch.no_grad():
                    mu, log_sigma = self.actor(state.unsqueeze(0), torch.tensor([time_idx], device=self.device))
                    sigma = torch.exp(log_sigma)
                    dist = torch.distributions.Normal(mu, sigma)
                    z = dist.rsample()
                    action = torch.tanh(z).squeeze(0)
                    if exploration_noise_std > 0.0:
                        noise = torch.randn_like(action) * exploration_noise_std
                        action = torch.clamp(action + noise, -1.0, 1.0)

                action_np = action.cpu().numpy()
                next_state_np, reward, done, _info = self.env.step(action_np, time_idx)
                next_state = torch.FloatTensor(next_state_np).to(self.device)
                trajectory.append((state.cpu().numpy(), action_np, reward, next_state_np, done, time_idx))

                state = next_state
                time_idx += 1
                if done:
                    break
            all_trajectories.append(trajectory)
        return all_trajectories

    def perform_beam_search(
        self,
        initial_state: torch.Tensor,
        beam_width: int,
        search_depth: int,
        time_memory: List[int],
        exploration_noise_std: float = 0.0
    ) -> List[Tuple[np.ndarray, np.ndarray, float, np.ndarray, bool, int]]:
        """Performs Beam Search."""
        beam = [(initial_state.clone(), [], 0.0, time_memory[-1])]
        final_trajs = []

        for _ in range(search_depth):
            new_beam = []
            for current_state, partial_traj, cum_reward, t_step in beam:
                if partial_traj and partial_traj[-1][4] is True:
                    final_trajs.append((current_state, partial_traj, cum_reward, t_step))
                    continue
                with torch.no_grad():
                    mu, log_sigma = self.actor(current_state.unsqueeze(0), torch.tensor([t_step], device=self.device))
                    sigma = torch.exp(log_sigma)
                    dist = torch.distributions.Normal(mu, sigma)
                    candidate_actions = []
                    for _i in range(beam_width):
                        z = dist.rsample()
                        a = torch.tanh(z).squeeze(0)
                        if exploration_noise_std > 0.0:
                            noise = torch.randn_like(a) * exploration_noise_std
                            a = torch.clamp(a + noise, -1.0, 1.0)
                        candidate_actions.append(a)

                for a_tensor in candidate_actions:
                    a_np = a_tensor.cpu().numpy()
                    ns_np, r, d, _info = self.env.step(a_np, t_step)
                    ns = torch.FloatTensor(ns_np).to(self.device)
                    new_traj = partial_traj.copy()
                    new_traj.append((current_state.cpu().numpy(), a_np, r, ns_np, d, t_step))
                    new_beam.append((ns, new_traj, cum_reward + r, t_step + 1))

            new_beam.sort(key=lambda x: x[2], reverse=True)
            beam = new_beam[:beam_width]

        final_trajs.extend(beam)
        best_traj = max(final_trajs, key=lambda x: x[2])[1]
        return best_traj

    def update_params(self, replay_buffer, meta_input, time_memory, update_steps=1):
        """Updates parameters without search."""
        final_info = {}
        for _ in range(update_steps):
            if len(replay_buffer) < self.config.batch_size:
                return final_info

            batch, _, _ = replay_buffer.sample(self.config.batch_size)
            states = torch.FloatTensor(np.stack([b[0] for b in batch])).to(self.device)
            actions = torch.FloatTensor(np.stack([b[1] for b in batch])).to(self.device)
            rewards = torch.FloatTensor(np.stack([b[2] for b in batch])).unsqueeze(1).to(self.device)
            next_states = torch.FloatTensor(np.stack([b[3] for b in batch])).to(self.device)
            dones = torch.FloatTensor(np.stack([b[4] for b in batch])).unsqueeze(1).to(self.device)
            time_steps = torch.FloatTensor(np.stack([b[5] for b in batch])).to(self.device)

            random_modes = torch.randint(0, self.config.num_market_modes, (states.shape[0],)).to(self.device)
            market_mode_loss = self.update_market_mode_classifier(states, random_modes)

            with torch.no_grad():
                mu, log_sigma = self.actor(states, time_steps)
                sigma = torch.exp(log_sigma)
                dist = torch.distributions.Normal(mu, sigma)
                z = dist.rsample()
                actions_ = torch.tanh(z)
                log_probs = dist.log_prob(z) - torch.log(1 - actions_.pow(2) + self.config.epsilon)
                log_probs = log_probs.sum(-1, keepdim=True)

            meta_input_tensor = torch.FloatTensor(meta_input).to(self.device)
            meta_loss_val, r_scaling = self.update_meta_controller(meta_input_tensor, log_probs, rewards)
            market_mode_probs = self.market_mode_classifier(states)

            q_targets = self.compute_q_targets(rewards, next_states, time_steps, dones, r_scaling, market_mode_probs)

            critic1_loss, critic2_loss = self.update_critics(states, actions, time_steps, q_targets)
            actor_loss_val = self.update_actor(states, time_steps)
            distiller_loss_val = self.update_distiller(states, time_steps)

            advantages = q_targets - torch.mean(q_targets, dim=0, keepdim=True)
            high_level_loss_val = self.update_high_level_policy(states, advantages)

            self.soft_update(self.critic_target1, self.critic1)
            self.soft_update(self.critic_target2, self.critic2)

            self.actor_scheduler.step(actor_loss_val)
            self.critic1_scheduler.step(critic1_loss)
            self.critic2_scheduler.step(critic2_loss)
            self.high_level_scheduler.step(high_level_loss_val)
            self.meta_scheduler.step(meta_loss_val)
            self.distiller_scheduler.step(distiller_loss_val)

            self.train_steps += 1
            self.writer.add_scalar("Loss/actor", actor_loss_val, self.train_steps)
            self.writer.add_scalar("Loss/critic1", critic1_loss, self.train_steps)
            self.writer.add_scalar("Loss/critic2", critic2_loss, self.train_steps)
            self.writer.add_scalar("Loss/meta", meta_loss_val, self.train_steps)
            self.writer.add_scalar("Loss/distiller", distiller_loss_val, self.train_steps)
            self.writer.add_scalar("Loss/market_mode", market_mode_loss, self.train_steps)
            self.writer.add_scalar("Loss/high_level_policy", high_level_loss_val, self.train_steps)
            self.writer.add_scalar("Params/alpha", self.alpha.item(), self.train_steps)

            final_info = {
                "actor_loss": actor_loss_val,
                "critic1_loss": critic1_loss,
                "critic2_loss": critic2_loss,
                "meta_loss": meta_loss_val,
                "distiller_loss": distiller_loss_val,
                "market_mode_loss": market_mode_loss,
                "high_level_loss": high_level_loss_val,
                "alpha": self.alpha.item()
            }
        return final_info

    def save(self, path: str) -> None:
        """Saves the agent's state dicts."""
        try:
            torch.save({
                "actor": self.actor.state_dict(),
                "critic1": self.critic1.state_dict(),
                "critic2": self.critic2.state_dict(),
                "critic_target1": self.critic_target1.state_dict(),
                "critic_target2": self.critic_target2.state_dict(),
                "meta_controller": self.meta_controller.state_dict(),
                "distiller": self.policy_distiller.state_dict(),
                "market_mode": self.market_mode_classifier.state_dict(),
                "high_level": self.high_level_policy.state_dict(),
                "alpha": self.alpha.detach().cpu().numpy(),
                "train_steps": self.train_steps
            }, path)
            logger.info(f"Model saved to {path}")
        except Exception as e:
            logger.error(f"Failed to save: {str(e)}")
            raise

    def load(self, path: str) -> None:
        """Loads the agent's state dicts."""
        try:
            ckpt = torch.load(path, map_location=self.device)
            self.actor.load_state_dict(ckpt["actor"])
            self.critic1.load_state_dict(ckpt["critic1"])
            self.critic2.load_state_dict(ckpt["critic2"])
            self.critic_target1.load_state_dict(ckpt["critic_target1"])
            self.critic_target2.load_state_dict(ckpt["critic_target2"])
            self.meta_controller.load_state_dict(ckpt["meta_controller"])
            self.policy_distiller.load_state_dict(ckpt["distiller"])
            self.market_mode_classifier.load_state_dict(ckpt["market_mode"])
            self.high_level_policy.load_state_dict(ckpt["high_level"])
            self.alpha.data.copy_(torch.tensor(ckpt["alpha"], dtype=torch.float32, device=self.device))
            self.train_steps = ckpt["train_steps"]
            logger.info(f"Model loaded from {path}")
        except Exception as e:
            logger.error(f"Failed to load: {str(e)}")
            raise


// File: config.py
# config.py
import torch
from dataclasses import dataclass

@dataclass
class EnvironmentConfig:
    """Configuration for environment, training, and hyperparameters."""
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    action_dim: int = 2
    state_dim: int = 8
    hidden_dim: int = 256
    time_encoding_dim: int = 16
    attention_dim: int = 64
    num_mlp_layers: int = 2
    dropout_rate: float = 0.1
    lr: float = 3e-4
    meta_lr: float = 1e-4
    gamma: float = 0.99
    tau: float = 0.005
    alpha: float = 0.2
    target_entropy_factor: float = 1.0
    epsilon: float = 1e-6
    max_grad_norm: float = 5.0
    buffer_capacity: int = 100000
    batch_size: int = 64
    meta_input_dim: int = 10
    num_hyperparams: int = 5
    num_market_modes: int = 3
    window_size: int = 10
    custom_layers: list = None
    add_search_to_replay: bool = False

// File: conftest.py
import sys
import os

def pytest_configure(config):
    # Add project root to sys.path
    project_root = os.path.dirname(os.path.abspath(__file__))
    sys.path.insert(0, project_root)


// File: networks.py
# networks.py
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Tuple, List, Optional
from config import EnvironmentConfig

class APELU(nn.Module):
    """Advanced Parametric Exponential Linear Unit activation."""
    def __init__(self, alpha_init: float = 0.01, beta_init: float = 1.0):
        super().__init__()
        self.alpha = nn.Parameter(torch.tensor(alpha_init, dtype=torch.float32))
        self.beta = nn.Parameter(torch.tensor(beta_init, dtype=torch.float32))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Applies APELU activation."""
        nan_mask = torch.isnan(x)
        out = torch.where(nan_mask, x, torch.zeros_like(x))
        non_nan_mask = ~nan_mask
        out = torch.where(non_nan_mask & (x >= 0), x, out)
        out = torch.where(non_nan_mask & (x < 0), self.alpha * x * torch.exp(self.beta * x), out)
        inf_mask = torch.isinf(x)
        out = torch.where(inf_mask, x, out)
        return out

class MomentumActivation(nn.Module):
    """Activation function sensitive to price momentum."""
    def __init__(self, momentum_sensitivity: float = 1.0):
        super().__init__()
        self.momentum_sensitivity = nn.Parameter(torch.tensor(momentum_sensitivity, dtype=torch.float32))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Applies momentum-based activation."""
        nan_mask = torch.isnan(x)
        out = torch.where(nan_mask, x, torch.zeros_like(x))
        non_nan_mask = ~nan_mask
        out = torch.where(non_nan_mask, x * (1 + self.momentum_sensitivity * torch.tanh(x)), out)
        inf_mask = torch.isinf(x)
        out = torch.where(inf_mask, x, out)
        return out

class VolatilityAdaptiveActivation(nn.Module):
    """Activation function that adapts based on volatility."""
    def __init__(self, initial_scale: float = 1.0):
        super().__init__()
        self.scale = nn.Parameter(torch.tensor(initial_scale, dtype=torch.float32))

    def forward(self, x: torch.Tensor, volatility: torch.Tensor) -> torch.Tensor:
        """Applies volatility-adaptive activation."""
        nan_mask_x = torch.isnan(x)
        out = torch.where(nan_mask_x, x, torch.zeros_like(x))
        volatility = torch.where(torch.isnan(volatility),
                                 torch.tensor(0.0, device=volatility.device, dtype=volatility.dtype),
                                 volatility)
        non_nan_mask_x = ~nan_mask_x
        out = torch.where(non_nan_mask_x, x * (1 + self.scale * torch.tanh(volatility)), out)
        return out

class KLinePatternLayer(nn.Module):
    """Detects candlestick patterns such as bullish/bearish engulfing."""
    def __init__(self, hidden_dim: int):
        super().__init__()
        self.linear = nn.Linear(3, hidden_dim)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Detects patterns and applies linear layer."""
        x = torch.where(torch.isnan(x), torch.tensor(0.0, device=x.device, dtype=x.dtype), x)
        patterns = self.detect_patterns(x)
        patterns = torch.where(torch.isnan(patterns), torch.tensor(0.0, device=x.device), patterns)
        return F.relu(self.linear(patterns))

    def detect_patterns(self, x: torch.Tensor) -> torch.Tensor:
        """Detects bullish/bearish engulfing patterns."""
        batch_size, _ = x.shape
        open_prices = x[:, 0]
        close_prices = x[:, 3]
        patterns = torch.zeros((batch_size, 3), device=x.device)
        if batch_size > 1:
            prev_open = open_prices[:-1]
            prev_close = close_prices[:-1]
            curr_open = open_prices[1:]
            curr_close = close_prices[1:]
            bullish = (prev_close < prev_open) & (curr_close > curr_open) & (curr_open < prev_close) & (curr_close > prev_open)
            bearish = (prev_close > prev_open) & (curr_close < curr_open) & (curr_open > prev_close) & (curr_close < prev_open)
            no_pattern = ~(bullish | bearish)
            patterns[1:, 0] = bullish.float()
            patterns[1:, 1] = bearish.float()
            patterns[1:, 2] = no_pattern.float()
        patterns[0, 2] = 1
        return patterns

class VolatilityTrackingLayer(nn.Module):
    """Tracks volatility using a window of close prices."""
    def __init__(self, hidden_dim: int, window_size: int):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.window_size = window_size
        self.linear = nn.Linear(1, hidden_dim)
        self.close_prices_buffer = None

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Calculates and applies volatility vector."""
        x = torch.where(torch.isnan(x), torch.tensor(0.0, device=x.device, dtype=x.dtype), x)
        close_prices = x[:, 3].unsqueeze(-1)
        self.update_buffer(close_prices)
        if self.close_prices_buffer.shape[1] >= self.window_size:
            vol_vec = self.calculate_volatility_vectors()
        else:
            vol_vec = torch.zeros((x.shape[0], 1), device=x.device, dtype=x.dtype)
        vol_vec = torch.where(torch.isnan(vol_vec), torch.tensor(0.0, device=x.device), vol_vec)
        return F.relu(self.linear(vol_vec))

    def update_buffer(self, close_prices: torch.Tensor):
        """Updates close prices buffer."""
        if self.close_prices_buffer is None:
            self.close_prices_buffer = close_prices
        else:
            buf_len = self.close_prices_buffer.shape[1]
            if buf_len < self.window_size:
                self.close_prices_buffer = torch.cat([self.close_prices_buffer, close_prices], dim=1)
            else:
                self.close_prices_buffer = torch.cat([self.close_prices_buffer[:, 1:], close_prices], dim=1)

    def calculate_volatility_vectors(self) -> torch.Tensor:
        """Calculates volatility as std of log returns."""
        log_returns = torch.log(self.close_prices_buffer[:, 1:] / self.close_prices_buffer[:, :-1])
        log_returns = torch.where(torch.isnan(log_returns), torch.tensor(0.0, device=log_returns.device), log_returns)
        log_returns = torch.where(torch.isinf(log_returns), torch.tensor(0.0, device=log_returns.device), log_returns)
        volatility = torch.std(log_returns, dim=1, unbiased=True).unsqueeze(-1)
        return volatility

class TimeWarpLayer(nn.Module):
    """Applies a time warping effect by averaging with last input."""
    def __init__(self, hidden_dim: int, window_size: int = 10):
        super().__init__()
        self.linear = nn.Linear(4, hidden_dim)
        self.last_x = None

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Applies time warping and linear layer."""
        x = torch.where(torch.isnan(x), torch.tensor(0.0, device=x.device, dtype=x.dtype), x)
        if self.last_x is None:
            self.last_x = x
        time_warped_x = (self.last_x + x) / 2
        self.last_x = x.clone().detach()
        return self.linear(time_warped_x)

class ExponentialMovingAverageLayer(nn.Module):
    """Calculates and applies EMA of close prices."""
    def __init__(self, window_size: int, hidden_dim: int):
        super().__init__()
        self.window_size = window_size
        self.alpha = 2/(window_size+1)
        self.ema = None
        self.linear = nn.Linear(1, hidden_dim)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Calculates EMA and applies linear layer."""
        x = torch.where(torch.isnan(x), torch.tensor(0.0, device=x.device), x)
        close_prices = x[:, 3]
        if self.ema is None:
            self.ema = close_prices.clone()
        else:
            self.ema = (close_prices * self.alpha) + (self.ema * (1 - self.alpha))
        ema_values = self.ema.unsqueeze(-1)
        ema_values = torch.where(torch.isnan(ema_values), torch.tensor(0.0, device=x.device), ema_values)
        return F.relu(self.linear(ema_values))

class FractalDimensionLayer(nn.Module):
    """Calculates Higuchi Fractal Dimension of a time series."""
    def __init__(self, hidden_dim: int, max_k: int = 10, buffer_size: int = 50):
        super().__init__()
        self.linear = nn.Linear(1, hidden_dim)
        self.max_k = max_k
        self.buffer_size = buffer_size
        self.values_buffer = []

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Calculates HFD and applies linear layer."""
        x = torch.where(torch.isnan(x), torch.tensor(0.0, device=x.device), x)
        close_prices = x[:, 3]
        hfd_values = []
        for price in close_prices:
            self.update_buffer(price.item())
            if len(self.values_buffer) > self.max_k:
                hfd_values.append(self.calculate_hfd())
            else:
                hfd_values.append(0.0)
        hfd_tensor = torch.tensor(hfd_values, device=x.device, dtype=torch.float32).unsqueeze(-1)
        return F.relu(self.linear(hfd_tensor))

    def update_buffer(self, value: float):
        """Updates the buffer with new value."""
        self.values_buffer.append(value)
        if len(self.values_buffer) > self.buffer_size:
            self.values_buffer.pop(0)

    def calculate_hfd(self):
        """Calculates Higuchi Fractal Dimension."""
        import numpy as np
        if len(self.values_buffer) < self.max_k + 1:
            return 0.0
        arr = np.array(self.values_buffer)
        lk_values = []
        for k in range(1, self.max_k+1):
            lk_total = 0
            for m in range(k):
                idxs = np.arange(m, len(arr), k)
                if len(idxs) >= 2:
                    lengths = np.abs(np.diff(arr[idxs]))
                    lk_total += np.sum(lengths)*(len(arr)-1)/(len(idxs)*k)
            if k > 0 and lk_total > 0:
                lk_values.append(lk_total/k)
        if len(lk_values) > 1:
            k_arr = np.arange(1, len(lk_values)+1)
            log_k = np.log(k_arr)
            log_lk = np.log(lk_values)
            log_k = np.nan_to_num(log_k, nan=0.0, posinf=1.0, neginf=-1.0)
            log_lk = np.nan_to_num(log_lk, nan=0.0, posinf=1.0, neginf=-1.0)
            slope, _ = np.polyfit(log_k, log_lk, 1)
            return -slope
        else:
            return 0.0

class SinusoidalTimeEncoding(nn.Module):
    """Encodes time using sinusoidal functions."""
    def __init__(self, time_encoding_dim: int):
        super().__init__()
        self.time_encoding_dim = time_encoding_dim
        self.frequencies = 10**(torch.arange(0, time_encoding_dim//2)*(-2/(time_encoding_dim//2)))

    def forward(self, time_step: torch.Tensor) -> torch.Tensor:
        """Applies sinusoidal time encoding."""
        time_step = time_step.float().unsqueeze(-1)
        scaled_time = time_step * self.frequencies.to(time_step.device)
        sin_enc = torch.sin(scaled_time)
        cos_enc = torch.cos(scaled_time)
        if self.time_encoding_dim % 2 == 0:
            encoding = torch.cat([sin_enc, cos_enc], dim=-1)
        else:
            zero_pad = torch.zeros_like(cos_enc[:, :1])
            encoding = torch.cat([sin_enc, cos_enc, zero_pad], dim=-1)
        return encoding

class TimeAwareBias(nn.Module):
    """Learns a bias that is a function of time encoding."""
    def __init__(self, input_dim: int, time_encoding_dim: int, hidden_dim: int):
        super().__init__()
        self.time_embedding = nn.Linear(time_encoding_dim, hidden_dim)
        self.time_projection = nn.Linear(hidden_dim, input_dim)
        self.activation = APELU()

    def forward(self, time_encoding: torch.Tensor) -> torch.Tensor:
        """Applies time-aware bias."""
        x = self.time_embedding(time_encoding)
        x = self.activation(x)
        return self.time_projection(x)

class ModernMLP(nn.Module):
    """MLP with custom layers, layer norm, dropout, and APELU."""
    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int,
                 num_layers: int, dropout_rate: float, use_custom_layers: bool,
                 window_size: int, custom_layers: Optional[List[str]] = None):
        super().__init__()
        self.layers = nn.ModuleList()
        self.norms = nn.ModuleList()
        self.use_custom_layers = use_custom_layers
        self.hidden_dim = hidden_dim
        self.activation = APELU()
        self.dropout = nn.Dropout(dropout_rate)

        self.custom_layers_list: List[nn.Module] = []
        if use_custom_layers:
            layer_mapping = {
                "KLinePatternLayer": lambda: KLinePatternLayer(hidden_dim),
                "VolatilityTrackingLayer": lambda: VolatilityTrackingLayer(hidden_dim, window_size),
                "TimeWarpLayer": lambda: TimeWarpLayer(hidden_dim, window_size),
                "ExponentialMovingAverageLayer": lambda: ExponentialMovingAverageLayer(window_size, hidden_dim),
                "FractalDimensionLayer": lambda: FractalDimensionLayer(hidden_dim)
            }
            if custom_layers is None:
                self.custom_layers_list = [
                    KLinePatternLayer(hidden_dim),
                    VolatilityTrackingLayer(hidden_dim, window_size),
                    TimeWarpLayer(hidden_dim, window_size),
                    ExponentialMovingAverageLayer(window_size, hidden_dim),
                    FractalDimensionLayer(hidden_dim)
                ]
            else:
                for layer_name in custom_layers:
                    if layer_name in layer_mapping:
                        self.custom_layers_list.append(layer_mapping[layer_name]())
            in_features = hidden_dim * len(self.custom_layers_list)
        else:
            in_features = input_dim

        prev_features = in_features
        for i in range(num_layers):
            out_features = output_dim if i == num_layers - 1 else hidden_dim
            self.layers.append(nn.Linear(prev_features, out_features))
            if i != num_layers - 1:
                self.norms.append(nn.LayerNorm(out_features))
            prev_features = out_features

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Applies MLP with optional custom layers."""
        if self.use_custom_layers:
            outputs = []
            for cl in self.custom_layers_list:
                out = cl(x)
                outputs.append(out)
            x = torch.cat(outputs, dim=-1)

        for i, layer in enumerate(self.layers):
            x = layer(x)
            if i != len(self.layers) - 1:
                x = self.norms[i](x)
                x = self.activation(x)
                x = self.dropout(x)
        return x

class AdaptiveModulationMLP(nn.Module):
    """MLP with time-aware modulation and custom layers."""
    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int,
                 num_layers: int, dropout_rate: float, time_encoding_dim: int,
                 use_custom_layers: bool, window_size: int, custom_layers: Optional[List[str]]=None):
        super().__init__()
        self.layers = nn.ModuleList()
        self.norms = nn.ModuleList()
        self.modulations = nn.ParameterList()
        self.time_biases = nn.ModuleList()
        self.sinusoidal_encoding = SinusoidalTimeEncoding(time_encoding_dim)
        self.use_custom_layers = use_custom_layers
        self.activation = APELU()
        self.dropout = nn.Dropout(dropout_rate)
        self.num_layers = num_layers

        self.custom_layers_list: List[nn.Module] = []
        if use_custom_layers:
            layer_mapping = {
                "KLinePatternLayer": lambda: KLinePatternLayer(hidden_dim),
                "VolatilityTrackingLayer": lambda: VolatilityTrackingLayer(hidden_dim, window_size),
                "TimeWarpLayer": lambda: TimeWarpLayer(hidden_dim, window_size),
                "ExponentialMovingAverageLayer": lambda: ExponentialMovingAverageLayer(window_size, hidden_dim),
                "FractalDimensionLayer": lambda: FractalDimensionLayer(hidden_dim)
            }
            if custom_layers is None:
                self.custom_layers_list = [
                    KLinePatternLayer(hidden_dim),
                    VolatilityTrackingLayer(hidden_dim, window_size),
                    TimeWarpLayer(hidden_dim, window_size),
                    ExponentialMovingAverageLayer(window_size, hidden_dim),
                    FractalDimensionLayer(hidden_dim)
                ]
            else:
                for layer_name in custom_layers:
                    if layer_name in layer_mapping:
                        self.custom_layers_list.append(layer_mapping[layer_name]())

            in_features = hidden_dim * len(self.custom_layers_list)
        else:
            in_features = input_dim

        prev_features = in_features
        for i in range(num_layers):
            out_features = output_dim if i == num_layers - 1 else hidden_dim
            self.layers.append(nn.Linear(prev_features, out_features))
            if i != num_layers - 1:
                self.norms.append(nn.LayerNorm(out_features))
                self.modulations.append(nn.Parameter(torch.ones(out_features)))
                self.time_biases.append(TimeAwareBias(out_features, time_encoding_dim, out_features))
            prev_features = out_features

    def forward(self, x: torch.Tensor, time_step: torch.Tensor) -> torch.Tensor:
        """Applies MLP with time-aware modulation."""
        time_encoding = self.sinusoidal_encoding(time_step)

        if self.use_custom_layers:
            outputs = []
            for cl in self.custom_layers_list:
                out = cl(x)
                outputs.append(out)
            x = torch.cat(outputs, dim=-1)

        for i, layer in enumerate(self.layers):
            x = layer(x)
            if i != self.num_layers - 1:
                mod_factor = self.modulations[i] + self.time_biases[i](time_encoding)
                x = x * mod_factor
                x = self.norms[i](x)
                x = self.activation(x)
                x = self.dropout(x)
        return x

class Attention(nn.Module):
    """Attention mechanism."""
    def __init__(self, input_dim: int, attention_dim: int):
        super().__init__()
        self.query_proj = nn.Linear(input_dim, attention_dim)
        self.key_proj = nn.Linear(input_dim, attention_dim)
        self.value_proj = nn.Linear(input_dim, attention_dim)
        self.out_proj = nn.Linear(attention_dim, input_dim)
        nn.init.xavier_uniform_(self.query_proj.weight)
        nn.init.xavier_uniform_(self.key_proj.weight)
        nn.init.xavier_uniform_(self.value_proj.weight)
        nn.init.xavier_uniform_(self.out_proj.weight)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Applies attention mechanism."""
        x = torch.where(torch.isnan(x), torch.tensor(0.0, device=x.device, dtype=x.dtype), x)
        query = self.query_proj(x)
        key = self.key_proj(x)
        value = self.value_proj(x)
        attn_output = torch.nn.functional.scaled_dot_product_attention(query, key, value)
        return self.out_proj(attn_output)

class MetaSACActor(nn.Module):
    """Actor network for MetaSAC."""
    def __init__(self, config: EnvironmentConfig):
        super().__init__()
        self.attention = Attention(config.state_dim, config.attention_dim)
        self.mlp = AdaptiveModulationMLP(
            input_dim=config.state_dim,
            hidden_dim=config.hidden_dim,
            output_dim=2*config.action_dim,
            num_layers=config.num_mlp_layers,
            dropout_rate=config.dropout_rate,
            time_encoding_dim=config.time_encoding_dim,
            use_custom_layers=bool(config.custom_layers),
            window_size=config.window_size,
            custom_layers=config.custom_layers
        )
        self.action_dim = config.action_dim

    def forward(self, x: torch.Tensor, time_step: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """Outputs mean and log sigma for action sampling."""
        if x.dim() == 2:
            x = x.unsqueeze(1)
        x = self.attention(x)
        x = x.squeeze(1)
        x = self.mlp(x, time_step)
        mu = x[:, :self.action_dim]
        log_sigma = x[:, self.action_dim:]
        mu = torch.tanh(mu)
        return mu, log_sigma

class MetaSACCritic(nn.Module):
    """Critic network for MetaSAC."""
    def __init__(self, config: EnvironmentConfig):
        super().__init__()
        combined_dim = config.state_dim + config.action_dim
        self.attention = Attention(combined_dim, config.attention_dim)
        self.mlp = AdaptiveModulationMLP(
            input_dim=combined_dim,
            hidden_dim=config.hidden_dim,
            output_dim=1,
            num_layers=config.num_mlp_layers,
            dropout_rate=config.dropout_rate,
            time_encoding_dim=config.time_encoding_dim,
            use_custom_layers=bool(config.custom_layers),
            window_size=config.window_size,
            custom_layers=config.custom_layers
        )

    def forward(self, state: torch.Tensor, action: torch.Tensor, time_step: torch.Tensor) -> torch.Tensor:
        """Outputs Q-value estimate."""
        x = torch.cat([state, action], dim=-1)
        if x.dim() == 2:
            x = x.unsqueeze(1)
        x = self.attention(x)
        x = x.squeeze(1)
        x = self.mlp(x, time_step)
        return x

class MetaController(nn.Module):
    """Meta-controller for adjusting hyperparameters."""
    def __init__(self, config: EnvironmentConfig):
        super().__init__()
        from typing import Tuple
        self.mlp = ModernMLP(
            input_dim=config.meta_input_dim + 2,
            hidden_dim=config.hidden_dim,
            output_dim=config.num_hyperparams + 3, # +3 for reward scaling
            num_layers=config.num_mlp_layers,
            dropout_rate=config.dropout_rate,
            use_custom_layers=False,
            window_size=config.window_size
        )
        self.num_hyperparams = config.num_hyperparams

    def forward(self, x: torch.Tensor, reward_stats: torch.Tensor) -> Tuple[torch.Tensor, ...]:
        """Outputs learned hyperparameters."""
        cat_input = torch.cat([x, reward_stats], dim=-1)
        out = self.mlp(cat_input)
        learning_rate_actor = torch.sigmoid(out[:, 0])
        learning_rate_critic = torch.sigmoid(out[:, 1])
        learning_rate_alpha = torch.sigmoid(out[:, 2])
        tau = torch.sigmoid(out[:, 3])
        gamma = 0.9 + 0.09*torch.sigmoid(out[:, 4])
        r_scaling_momentum = torch.sigmoid(out[:, 5])
        r_scaling_reversal = torch.sigmoid(out[:, 6])
        r_scaling_volatility = torch.sigmoid(out[:, 7])
        return (learning_rate_actor, learning_rate_critic, learning_rate_alpha, 
                tau, gamma, r_scaling_momentum, r_scaling_reversal, r_scaling_volatility)

class PolicyDistiller(nn.Module):
    """Combines outputs from multiple specialist policies."""
    def __init__(self, specialist_policies: List[nn.Module]):
        super().__init__()
        self.specialists = nn.ModuleList(specialist_policies)

    def forward(self, state: torch.Tensor, time_step: torch.Tensor):
        """Averages outputs from specialist policies."""
        outputs = [spec(state, time_step) for spec in self.specialists]
        mus = torch.stack([o[0] for o in outputs], dim=0)
        log_sigmas = torch.stack([o[1] for o in outputs], dim=0)
        mu_avg = torch.mean(mus, dim=0)
        log_sigma_avg = torch.mean(log_sigmas, dim=0)
        return mu_avg, log_sigma_avg

class MarketModeClassifier(nn.Module):
    """Classifies market modes (e.g., bull, bear, neutral)."""
    def __init__(self, input_dim, hidden_dim, output_dim=3):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim),
            nn.Softmax(dim=-1)
        )
    
    def forward(self, x):
        """Outputs probabilities for each market mode."""
        return self.net(x)

class HighLevelPolicy(nn.Module):
    """High-level policy network."""
    def __init__(self, input_dim: int, hidden_dim: int):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Outputs a high-level action or weighting."""
        return self.net(x)

// File: replay_buffer.py
# replay_buffer.py
import random
import numpy as np
from typing import Tuple, List

class ReplayBuffer:
    """Stores transitions for training."""
    def __init__(self, capacity: int):
        self.capacity = capacity
        self.buffer = []
        self.position = 0

    def add(self, state, action, reward, next_state, done, time_step):
        """Adds transition to buffer."""
        transition = (state, action, reward, next_state, done, time_step)
        if len(self.buffer) < self.capacity:
            self.buffer.append(transition)
        else:
            self.buffer[self.position] = transition
        self.position = (self.position + 1) % self.capacity

    def sample(self, batch_size: int) -> Tuple[List, None, None]:
        """Samples a batch of transitions."""
        batch = random.sample(self.buffer, batch_size)
        return batch, None, None

    def __len__(self):
        return len(self.buffer)

// File: reward.py
#reward.py

// File: train.py
# train.py
import numpy as np
import torch
from config import EnvironmentConfig
from env.environment import HistoricalEnvironment
from agent import MetaSACAgent

def get_noise_schedule(initial_noise: float, final_noise: float, decay_steps: int) -> Callable[[int], float]:
    """
    Creates a linear noise schedule.

    Args:
        initial_noise: Initial noise value.
        final_noise: Final noise value.
        decay_steps: Number of steps to decay noise over.

    Returns:
        A function that takes the current step and returns the noise value.
    """
    def noise_fn(step: int) -> float:
        if step > decay_steps:
            return final_noise
        return initial_noise - (initial_noise - final_noise) * (step / decay_steps)
    return noise_fn

if __name__ == "__main__":
    # 1) Create config and mock data
    config = EnvironmentConfig()
    np.random.seed(42)
    mock_data = np.random.randn(2000, config.state_dim).astype(np.float32)

    # 2) Create environment
    env = HistoricalEnvironment(mock_data)

    # 3) Create agent
    agent = MetaSACAgent(config, env)

    # 4) Warm-up replay buffer
    initial_steps = 200
    state = env.reset()
    for step in range(initial_steps):
        action = np.random.uniform(-1, 1, config.action_dim)
        next_state, reward, done, _info = env.step(action, step)
        agent.replay_buffer.add(state, action, reward, next_state, done, step)
        if done:
            state = env.reset()
        else:
            state = next_state

    # 5) Noise schedule
    noise_schedule = get_noise_schedule(initial_noise=0.2, final_noise=0.01, decay_steps=10000)

    # 6) Training loop
    num_epochs = 5
    updates_per_epoch = 10
    for epoch in range(num_epochs):


        for _update in range(updates_per_epoch):
            # Example meta input: random
            meta_input = np.random.randn(config.batch_size, config.meta_input_dim).astype(np.float32)
            time_memory = [0]

            agent.update_params_with_training_time_search(
                agent.replay_buffer,
                meta_input=meta_input,
                time_memory=time_memory,
                update_steps=1,
                search_algorithm="best-of-n",
                num_samples=4,
                beam_width=3,
                search_depth=5,
                use_d_search=False,
                exploration_noise_std_fn=noise_schedule
            )
        print(f"Epoch {epoch+1}/{num_epochs} completed, total steps={agent.train_steps}.")

    # 7) Save model
    agent.save("metasac_final.pth")

// File: env\environment.py
# env/environment.py
import numpy as np

class HistoricalEnvironment:
    """Simulates stepping through historical data."""
    def __init__(self, historical_data: np.ndarray):
        self.historical_data = historical_data
        self.current_time = 0
        self.max_time = len(historical_data) - 1

    def reset(self) -> np.ndarray:
        """Resets environment to start."""
        self.current_time = 0
        return self.historical_data[self.current_time]

    def step(self, action: np.ndarray, current_time: int):
        """Steps to next time, returns next_state, reward, done, info."""
        next_time = current_time + 1
        done = next_time >= self.max_time
        if not done:
            next_state = self.historical_data[next_time]
        else:
            next_state = np.zeros_like(self.historical_data[0])
        reward = float(np.random.randn() * 0.01 + 0.01)
        return next_state, reward, done, {}

