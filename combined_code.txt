// File: clean.py
import subprocess
import os

def clean_project(project_dir):
  """
  Cleans a Python project by removing unused imports and variables using Autoflake.

  Args:
    project_dir: The path to the root directory of the Python project.
  """

  exclude_paths = [
      "venv",
      "migrations",
      ".mypy_cache",
      "__pycache__",
      # Add any other directories or file names to exclude
  ]

  for root, _, files in os.walk(project_dir):
      for file in files:
          if file.endswith(".py") and not any(exclude in root for exclude in exclude_paths):
              file_path = os.path.join(root, file)
              try:
                  subprocess.run(
                      [
                          "autoflake",
                          "--in-place",
                          "--remove-unused-variables",
                          "--remove-all-unused-imports",
                          file_path,
                      ],
                      check=True,
                  )
                  print(f"Cleaned: {file_path}")
              except subprocess.CalledProcessError as e:
                  print(f"Error cleaning {file_path}: {e}")

if __name__ == "__main__":
  project_directory = r"C:\Users\dylan\Desktop\sheeplz-crypto-bot"  # Your project directory
  clean_project(project_directory)

// File: main.py
import asyncio
import logging
import tracemalloc
import os
from typing import Optional, List

import torch
import numpy as np
import joblib
from sklearn.preprocessing import MinMaxScaler

from data.ingestion.mexc_data_ingestion import DataIngestion
from data.config import Config as DataIngestionConfig
from models.agents.agent import TradingAgent
from models.gmn.gmn import CryptoGMN
from models.lnn.lnn_model import LiquidNeuralNetwork
from models.utils.risk_management import RiskManager
from models.utils.config import Config

config = Config("configs/config.yaml")

log_file_path = os.path.join(os.getcwd(), "logs", "trading_bot.log")
os.makedirs(os.path.dirname(log_file_path), exist_ok=True)

logging.basicConfig(
    level=getattr(logging, config.log_level.upper(), logging.INFO),
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file_path),
        logging.StreamHandler()
    ]
)

scaler_path = "models/lnn/scaler.joblib"

bot_tasks: List[asyncio.Task] = []

async def main():
    global bot_tasks

    try:
        timeframes = config.timeframes
        max_history_length = config.max_history_length

        gmn = CryptoGMN(timeframes, config.indicators, max_history_length=max_history_length, config=config)

        data_ingestion_config = DataIngestionConfig(
            symbol=config.symbol,
            timeframes=config.timeframes,
            private_channels=config.private_channels,
            reconnect_delay=config.reconnect_delay,
            max_retry_attempts=config.max_retry_attempts
        )
        data_ingestion = DataIngestion(gmn, data_ingestion_config)

        risk_manager = RiskManager(config.risk_parameters)
        scaler = await load_scaler(scaler_path)  # No need to pass gmn and history_length
        model = await load_or_train_lnn(gmn, config.lnn_model_path, config, scaler)

        if model is None:
            logging.error("Failed to load or train LNN model. Exiting.")
            await shutdown()  # Call simplified shutdown
            return

        agent = TradingAgent(timeframes, config.indicators, model, config, risk_manager, scaler)
        tracemalloc.start()
        bot_tasks = [
            asyncio.create_task(data_ingestion.connect()),
            asyncio.create_task(agent_loop(agent, gmn))
        ]

        try:
            await asyncio.gather(*bot_tasks)
        except asyncio.CancelledError:
            logging.info("Main tasks cancelled.")
        except Exception as e:
            logging.exception(f"Unhandled exception in main loop: {e}")
        finally:
            snapshot = tracemalloc.take_snapshot()
            top_stats = snapshot.statistics('lineno')
            logging.info("Top 10 memory allocations:")
            for stat in top_stats[:10]:
                logging.info(stat)

            tracemalloc.stop()
            await shutdown()

    except KeyboardInterrupt:
        logging.info("Trading bot stopped by user.")


async def agent_loop(agent: TradingAgent, gmn: CryptoGMN):
    while True:
        try:
            market_data = gmn.get_all_data()
            if not all(market_data.values()):
                await asyncio.sleep(config.agent_loop_delay)
                continue
            await agent.make_decision(market_data)
        except Exception as e:
            logging.error(f"Error in agent loop: {e}", exc_info=True)
        await asyncio.sleep(config.agent_loop_delay)


async def load_or_train_lnn(gmn, model_path, config, scaler):
    try:
        input_size = len(config.timeframes) * len(config.indicators)
        model = LiquidNeuralNetwork(input_size, config.lnn_hidden_size, 1).to(config.device)
        model.load_state_dict(torch.load(model_path, map_location=config.device))
        model.eval()
        logging.info("Loaded pre-trained LNN model.")
        return model
    except FileNotFoundError:
        logging.info("Pre-trained LNN model not found. Training a new model...")
        return await train_and_save_lnn(gmn, model_path, config, scaler)
    except Exception as e:
        logging.exception(f"Error loading LNN model: {e}")
        return None


async def train_and_save_lnn(gmn, model_path, config, scaler):
    try:
        X_train, y_train = await prepare_lnn_training_data(gmn, config.training_history_length, scaler)
        if X_train is None or y_train is None:
            return None
        X_train = torch.tensor(X_train, dtype=torch.float32, device=config.device)
        y_train = torch.tensor(y_train, dtype=torch.float32, device=config.device)
        input_size = X_train.shape[1]
        model = LiquidNeuralNetwork(input_size, config.lnn_hidden_size, 1).to(config.device)
        criterion = torch.nn.BCEWithLogitsLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=config.lnn_learning_rate)
        epochs = config.lnn_training_epochs
        batch_size = 32
        dataset = torch.utils.data.TensorDataset(X_train, y_train)
        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

        for epoch in range(epochs):
            model.train()
            epoch_loss = 0.0
            for batch_X, batch_y in dataloader:
                optimizer.zero_grad()
                outputs = model(batch_X.unsqueeze(1))
                loss = criterion(outputs.squeeze(), batch_y)
                loss.backward()
                optimizer.step()
                epoch_loss += loss.item()
            avg_loss = epoch_loss / len(dataloader)
            logging.info(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")

        torch.save(model.state_dict(), model_path)
        model.eval()
        logging.info(f"LNN model trained and saved to {model_path}")
        joblib.dump(scaler, scaler_path)
        logging.info(f"Scaler saved to {scaler_path}")
        return model
    except Exception as e:
        logging.error(f"Error during LNN training: {e}", exc_info=True)
        return None


async def prepare_lnn_training_data(gmn: CryptoGMN, history_length: int, scaler: MinMaxScaler):
    try:
        market_data = gmn.get_all_data()
        if not market_data or len(market_data['1m']['price']) < history_length + 1:
            logging.error("Not enough data to prepare training dataset.")
            return None, None

        X = []
        y = []
        for i in range(history_length, len(market_data['1m']['price']) - 1):
            features = []
            for timeframe in gmn.timeframes:
                for indicator in gmn.indicators:
                    data_series = market_data[timeframe].get(indicator)
                    if data_series and len(data_series) > i:
                        value = data_series[i]
                        if isinstance(value, dict):
                            features.extend(list(value.values()))
                        else:
                            features.append(value)
                    else:
                        features.append(0.0)

            future_price = market_data['1m']['price'][i + 1]
            current_price = market_data['1m']['price'][i]
            price_change = (future_price - current_price) / current_price
            y.append(1 if price_change > 0 else 0)
            X.append(features)

        X = np.array(X, dtype=np.float32)
        y = np.array(y, dtype=np.float32)
        X_scaled = scaler.fit_transform(X)  # Fit and transform inside the function
        return X_scaled, y

    except Exception as e:
        logging.error(f"Error preparing LNN training data: {e}", exc_info=True)
        return None, None


async def load_scaler(scaler_path: str) -> MinMaxScaler:
    """Loads a saved scaler or creates a new one."""
    try:
        scaler = joblib.load(scaler_path)
        logging.info(f"Loaded scaler from {scaler_path}")
        return scaler
    except FileNotFoundError:
        logging.warning(f"Scaler file not found at {scaler_path}. Initializing a new scaler.")
        return MinMaxScaler()


async def shutdown():
    """Shuts down the application gracefully."""
    global bot_tasks
    for task in bot_tasks:
        if task:
            task.cancel()
            try:
                await task
            except asyncio.CancelledError:
                pass
    logging.info("Shutdown complete.")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logging.info("Trading bot stopped by user. Shutting down...")
        asyncio.run(shutdown())

// File: data\data_processor.py
import asyncio

class DataProcessor:
    def __init__(self, storage, indicator_calculator, error_handler, config):
        self.storage = storage
        self.indicator_calculator = indicator_calculator
        self.error_handler = error_handler
        self.config = config

    async def process_data(self, data_batch):
        """Processes kline data and applies indicators asynchronously."""
        processed_data = {}
        for data in data_batch:
            try:
                kline_data = self._extract_kline_data(data)
                timeframe = self._get_timeframe(data)
                if timeframe not in processed_data:
                    processed_data[timeframe] = []
                processed_data[timeframe].append(kline_data)
            except Exception as e:
                self.error_handler.handle_error(f"Error extracting kline data: {e}", exc_info=True)

        try:
            indicators = await self._calculate_indicators_async(processed_data)
            unified_feed = self._create_unified_feed(processed_data, indicators)
            await self.storage.store_data(unified_feed)
        except Exception as e:
            self.error_handler.handle_error(f"Error calculating indicators or storing data: {e}", exc_info=True)

    async def _calculate_indicators_async(self, processed_data):
        """Calculates indicators asynchronously."""
        loop = asyncio.get_running_loop()
        return await loop.run_in_executor(None, self.indicator_calculator.calculate_indicators, processed_data)

    def _extract_kline_data(self, data):
        # Extract kline data (fill with your logic)
        return {
            'open': data['d']['k']['o'],
            'high': data['d']['k']['h'],
            'low': data['d']['k']['l'],
            'close': data['d']['k']['c'],
            'volume': data['d']['k']['v'],
            'close_time': data['d']['k']['T']
        }

    def _get_timeframe(self, data):
        # Extract timeframe (assuming format is 'spot@public.kline.v3.api@BTCUSDT@kline_1m')
        return data.get('c', '').split('@')[-1].split('_')[-1]

    def _create_unified_feed(self, klines, indicators):
        """Combines kline data and indicators into a unified feed."""
        unified_feed = {}
        for timeframe, data in klines.items():
            unified_feed[timeframe] = {
                'price': [entry['close'] for entry in data],
                'volume': [entry['volume'] for entry in data],
                'open': [entry['open'] for entry in data],
                'high': [entry['high'] for entry in data],
                'low': [entry['low'] for entry in data],
                'close_time': [entry['close_time'] for entry in data],
                'indicators': indicators.get(timeframe, {})
            }
        return unified_feed


// File: data\indicator_calculations.py
import pandas as pd
from finta import TA
from .error_handler import ErrorHandler
from typing import Dict, Any, List
import concurrent.futures

class IndicatorCalculator:
    def __init__(self, error_handler: ErrorHandler):
        """
        Initializes the IndicatorCalculator with an error handler.
        Args:
            error_handler (ErrorHandler): Instance to handle errors during calculations.
        """
        self.error_handler = error_handler

    def calculate_indicators(self, symbol: str, data: Dict[str, pd.DataFrame]) -> Dict[str, Dict[str, Any]]:
        """
        Calculates indicators for all timeframes.
        Args:
            symbol (str): The trading symbol (e.g., 'BTC_USDT').
            data (Dict[str, pd.DataFrame]): Dictionary where keys are timeframes and values are DataFrames.
        Returns:
            Dict[str, Dict[str, Any]]: Dictionary of indicators keyed by timeframe.
        """
        indicators = {}
        try:
            with concurrent.futures.ThreadPoolExecutor() as executor:
                futures = {executor.submit(self._calculate_for_timeframe, symbol, timeframe, df): timeframe for timeframe, df in data.items()}
                for future in concurrent.futures.as_completed(futures):
                    timeframe = futures[future]
                    try:
                        indicators[timeframe] = future.result()
                    except Exception as e:
                        self.error_handler.handle_error(f"Error calculating indicators for {symbol} {timeframe}: {e}", exc_info=True)
        except Exception as e:
            self.error_handler.handle_error(f"Error in calculate_indicators: {e}", exc_info=True)
        return indicators

    def _calculate_for_timeframe(self, symbol: str, timeframe: str, df: pd.DataFrame) -> Dict[str, Any]:
        """
        Helper function to calculate indicators for a specific timeframe.
        Args:
            symbol (str): The trading symbol (e.g., 'BTC_USDT').
            timeframe (str): The timeframe being processed.
            df (pd.DataFrame): DataFrame containing OHLC data for the timeframe.
        Returns:
            Dict[str, Any]: Calculated indicators for the timeframe.
        """
        indicators = {}
        try:
            # Ensure dataframe has the required columns
            required_columns = ['open', 'high', 'low', 'close', 'volume']
            if not all(col in df.columns for col in required_columns):
                raise ValueError(f"Missing required columns in {timeframe} data for {symbol}: {df.columns}")

            indicators['rsi'] = self.calculate_rsi(df)
            indicators['macd'] = self.calculate_macd(df)
            indicators['fibonacci'] = self.calculate_fibonacci(df)
        except Exception as e:
            self.error_handler.handle_error(f"Error calculating indicators for {symbol} {timeframe}: {e}", exc_info=True)
        return indicators

    def calculate_rsi(self, df: pd.DataFrame) -> List[float]:
        """
        Calculates the RSI (Relative Strength Index).
        Args:
            df (pd.DataFrame): DataFrame containing OHLC data.
        Returns:
            List[float]: RSI values.
        """
        try:
            rsi_values = TA.RSI(df).fillna(0).tolist()
            return rsi_values
        except Exception as e:
            self.error_handler.handle_error(f"Error calculating RSI: {e}", exc_info=True)
            return []

    def calculate_macd(self, df: pd.DataFrame) -> Dict[str, List[float]]:
        """
        Calculates the MACD (Moving Average Convergence Divergence).
        Args:
            df (pd.DataFrame): DataFrame containing OHLC data.
        Returns:
            Dict[str, List[float]]: MACD values, signal line, and histogram.
        """
        try:
            macd_values = TA.MACD(df).fillna(0)
            return {
                'macd': macd_values['MACD'].tolist(),
                'macd_signal': macd_values['SIGNAL'].tolist(),
                'macd_hist': macd_values['HISTOGRAM'].tolist()
            }
        except Exception as e:
            self.error_handler.handle_error(f"Error calculating MACD: {e}", exc_info=True)
            return {'macd': [], 'macd_signal': [], 'macd_hist': []}

    def calculate_fibonacci(self, df: pd.DataFrame) -> Dict[str, float]:
        """
        Calculates Fibonacci Retracement levels manually.
        Args:
            df (pd.DataFrame): DataFrame containing OHLC data.
        Returns:
            Dict[str, float]: Fibonacci retracement levels.
        """
        try:
            high = df['high'].max()
            low = df['low'].min()
            diff = high - low
            fib_levels = {
                "23.6%": high - 0.236 * diff,
                "38.2%": high - 0.382 * diff,
                "50.0%": high - 0.5 * diff,
                "61.8%": high - 0.618 * diff,
                "78.6%": high - 0.786 * diff
            }
            return fib_levels
        except Exception as e:
            self.error_handler.handle_error(f"Error calculating Fibonacci Retracement: {e}", exc_info=True)
            return {}


// File: data\mexc_websocket_connector.py
import os
import json
import websockets
import asyncio
from dotenv import load_dotenv

load_dotenv()

class MexcWebsocketConnector:
    def __init__(self, data_queue):
        self.ws_url = os.getenv("MEXC_WS_URL", "wss://wbs.mexc.com/ws")
        self.api_key = os.getenv("MEXC_API_KEY")
        self.api_secret = os.getenv("MEXC_API_SECRET")
        self.reconnect_delay = 5.0
        self.data_queue = data_queue

    async def connect(self):
        while True:
            try:
                async with websockets.connect(self.ws_url) as ws:
                    await self._subscribe(ws)
                    await self._receive_batched_klines(ws)
            except Exception as e:
                print(f"WebSocket connection error: {e}")
                await asyncio.sleep(self.reconnect_delay)

    async def _subscribe(self, ws):
        subscribe_message = {
            "method": "SUBSCRIBE",
            "params": ["spot@public.kline.v3.api@BTCUSDT@kline_1m", "spot@public.kline.v3.api@BTCUSDT@kline_5m"],
            "id": 1
        }
        await ws.send(json.dumps(subscribe_message))

    async def _receive_batched_klines(self, ws):
        """Receives and processes kline data in batches."""
        kline_batch = []
        while True:
            try:
                message = await ws.recv()
                data = json.loads(message)
                if "spot@public.kline.v3.api" in data.get("c", ""):
                    kline_batch.append(data)
                else:
                    if kline_batch:
                        await self.data_queue.put(kline_batch)
                        kline_batch = []  # Reset batch
            except Exception as e:
                print(f"Error receiving kline data: {e}")
                break


// File: data\__init__.py
from .mexc_websocket_connector import MexcWebsocketConnector
from .data_processor import DataProcessor
from .storage.data_storage import DataStorage

__all__ = ['MexcWebsocketConnector', 'DataProcessor', 'WebSocketManager', 'DataStorage']


// File: data\storage\data_storage.py
import asyncio
import pandas as pd
import os

class DataStorage:
    def __init__(self, storage_path="data_storage"):
        self.storage_path = storage_path
        os.makedirs(self.storage_path, exist_ok=True)

    async def store_data(self, unified_feed):
        """Stores unified feed data asynchronously."""
        loop = asyncio.get_running_loop()
        await loop.run_in_executor(None, self._store_data_sync, unified_feed)

    def _store_data_sync(self, unified_feed):
        """Synchronous helper function for data storage."""
        symbol = "BTC_USDT"  # Assuming single symbol; adjust if needed.
        for timeframe, content in unified_feed.items():
            df = pd.DataFrame(content)  # Create DataFrame from content
            filename = f"{symbol}_{timeframe}.csv"
            filepath = os.path.join(self.storage_path, filename)
            df.to_csv(filepath, index=False)
            print(f"Data stored in {filepath}")


// File: data\storage\__init__.py


// File: models\__init__.py


// File: models\agents\agent.py
# File: models/agents/agent.py

import logging
import numpy as np
import torch
from sklearn.preprocessing import MinMaxScaler
from models.utils.config import Config
from models.utils.risk_management import RiskManager
import os
import aiohttp  # Ensure aiohttp is imported
import hashlib
import hmac
import time

class TradingAgent:
    def __init__(
        self, 
        timeframes, 
        indicators, 
        model, 
        config: Config, 
        risk_manager: RiskManager, 
        scaler: MinMaxScaler
    ):
        self.timeframes = timeframes
        self.indicators = indicators
        self.model = model
        self.config = config
        self.threshold_buy = config.threshold_buy
        self.threshold_sell = config.threshold_sell
        self.risk_manager = risk_manager
        self.trade_parameters = config.trade_parameters
        self.leverage = self.trade_parameters.get("leverage", 20)
        self.position = None
        self.scaler = scaler
        self.api_key = os.getenv("MEXC_API_KEY")
        self.api_secret = os.getenv("MEXC_API_SECRET")
        self.base_url = 'https://contract.mexc.com/api/v1/'  # Update if necessary

        self.peak_portfolio_value = 1.0  # Initialize for drawdown calculations
        self.portfolio_value = 1.0  # Initialize portfolio value

        # Initialize aiohttp session
        self.session = aiohttp.ClientSession()

    async def make_decision(self, market_data):
        """Processes market data, makes predictions, and executes trades based on the model's output."""
        try:
            input_vector = self._prepare_input(market_data)
            if input_vector is None:
                logging.warning("Input vector is None. Skipping decision.")
                return

            # Model expects input in float32
            input_tensor = torch.tensor([input_vector], dtype=torch.float32)

            with torch.no_grad():
                self.model.eval()
                prediction = self.model(input_tensor)
                prediction = torch.sigmoid(prediction)  # Apply sigmoid to get probability
                prediction_value = prediction.item()

            logging.info(f"Model Prediction Probability: {prediction_value:.4f}")

            current_drawdown = self.calculate_current_drawdown()

            if self.risk_manager.check_risk(current_drawdown, self.position, market_data):
                if prediction_value >= self.threshold_buy and self.position != 'long':
                    await self._execute_trade('buy')
                    self.position = 'long'
                elif prediction_value <= self.threshold_sell and self.position != 'short':
                    await self._execute_trade('sell')
                    self.position = 'short'
                elif self.threshold_sell < prediction_value < self.threshold_buy:
                    if self.position is not None:
                        await self._execute_trade('close')
                        self.position = None
            else:
                logging.warning("Risk management check failed. Not executing trade.")

        except Exception as e:
            logging.error(f"Error in make_decision: {e}")

    def _prepare_input(self, market_data):
        """Prepares and scales the input vector for the LNN model."""
        input_vector = []
        for timeframe in self.timeframes:
            for indicator in self.indicators:
                data = market_data.get(timeframe, {}).get(indicator)
                if data is None or len(data) == 0:
                    logging.warning(f"Missing data for {timeframe} {indicator}. Skipping.")
                    return None

                if isinstance(data[-1], dict):
                    values = [v for v in data[-1].values() if isinstance(v, (int, float))]
                    input_vector.extend(values)
                else:
                    input_vector.append(data[-1])

        if not input_vector:
            logging.warning("Input vector is empty. No data available for making a decision.")
            return None

        input_vector = np.array([input_vector], dtype=np.float32)
        input_vector = self.scaler.transform(input_vector).astype(np.float32)
        return input_vector.flatten()

    async def _execute_trade(self, action, symbol="BTC_USDT"):
        """Executes a trade action (buy, sell, close) via the MEXC API using aiohttp."""
        try:
            timestamp = int(time.time() * 1000)
            params = {
                "symbol": symbol,
                "timestamp": timestamp
            }

            # Determine trade parameters based on action
            if action == 'buy':
                side = 'OPEN_LONG'
                quantity = self.trade_parameters.get("volume", 1)
            elif action == 'sell':
                side = 'OPEN_SHORT'
                quantity = self.trade_parameters.get("volume", 1)
            elif action == 'close':
                if self.position == 'long':
                    side = 'CLOSE_LONG'
                elif self.position == 'short':
                    side = 'CLOSE_SHORT'
                else:
                    logging.warning("No position to close.")
                    return
                quantity = self.trade_parameters.get("volume", 1)
            else:
                logging.warning(f"Invalid trade action: {action}")
                return

            # Set additional parameters
            params.update({
                "price": '',  # Empty for market orders
                "vol": quantity,
                "side": side,
                "type": self.trade_parameters.get("order_type", 1),  # 1: Market order
                "leverage": self.leverage,
                "openType": self.trade_parameters.get("open_type", 1),  # 1: Isolated margin
            })

            # Generate signature
            query_string = '&'.join([f"{key}={params[key]}" for key in sorted(params)])
            signature = hmac.new(
                self.api_secret.encode('utf-8'),
                query_string.encode('utf-8'),
                hashlib.sha256
            ).hexdigest()
            params['sign'] = signature

            # Send request using aiohttp
            url = self.base_url + 'order/submit'
            async with self.session.post(url, params=params) as response:
                response_data = await response.json()
                if response.status == 200 and response_data.get("success", False):
                    logging.info(f"Successfully executed {action} order: {response_data}")
                    # Update portfolio value based on trade execution
                    self.update_portfolio(action, response_data)
                else:
                    logging.error(f"Failed to execute {action} order: {response_data}")

        except Exception as e:
            logging.error(f"Error executing trade: {e}")

    def calculate_current_drawdown(self):
        """Calculates the current drawdown based on portfolio value."""
        self.peak_portfolio_value = max(self.peak_portfolio_value, self.portfolio_value)
        drawdown = (self.peak_portfolio_value - self.portfolio_value) / self.peak_portfolio_value
        logging.info(f"Current Drawdown: {drawdown:.4f}")
        return drawdown

    def update_portfolio(self, action: str, response_data: Dict):
        """
        Updates the portfolio value based on the executed trade.
        This is a placeholder function. You need to implement actual portfolio management logic.
        """
        # Example: Update portfolio based on the price and quantity
        try:
            price = float(response_data.get('data', {}).get('price', self.portfolio_value))
            quantity = float(response_data.get('data', {}).get('vol', 0))
            if action == 'buy':
                # Example logic: Increase portfolio value
                self.portfolio_value += price * quantity
            elif action == 'sell':
                # Example logic: Decrease portfolio value
                self.portfolio_value -= price * quantity
            elif action == 'close':
                # Example logic: Neutralize position
                pass
            logging.info(f"Portfolio updated after {action}: {self.portfolio_value}")
        except Exception as e:
            logging.error(f"Error updating portfolio: {e}")

    async def close(self):
        """Closes the aiohttp session."""
        await self.session.close()

// File: models\agents\rl_agent.py
# models/agents/rl_agent.py

import gym
import numpy as np
from stable_baselines3 import PPO

class TradingEnvironment(gym.Env):
    def __init__(self, market_data):
        super(TradingEnvironment, self).__init__()
        self.market_data = market_data
        self.current_step = 0
        self.action_space = gym.spaces.Discrete(3)  # Buy, Sell, Hold
        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(OBSERVATION_SPACE_SIZE,), dtype=np.float32)

    def reset(self):
        self.current_step = 0
        return self._next_observation()

    def _next_observation(self):
        # Return the next market observation
        obs = self.market_data.iloc[self.current_step]
        return obs.values

    def step(self, action):
        # Execute one time step within the environment
        self.current_step += 1

        reward = self._calculate_reward(action)
        done = self.current_step >= len(self.market_data) - 1
        obs = self._next_observation() if not done else np.zeros(self.observation_space.shape)

        return obs, reward, done, {}

    def _calculate_reward(self, action):
        # Implement reward calculation
        return reward

def train_rl_agent():
    # Load market data
    market_data = pd.read_csv("data/distilled/distilled_data.csv")
    env = TradingEnvironment(market_data)
    model = PPO("MlpPolicy", env, verbose=1)
    model.learn(total_timesteps=10000)
    model.save("models/agents/ppo_agent")

if __name__ == "__main__":
    train_rl_agent()


// File: models\agents\__init__.py


// File: models\gmn\config.py
import os
from pydantic import BaseModel, Field, validator
from typing import List

class CryptoGMNConfig(BaseModel):
    timeframes: List[str] = ["1m", "5m"]
    indicators: List[str] = ["price", "volume", "rsi", "macd", "fibonacci"]
    max_history: int = 1000
    executor_workers: int = 5
    cache_size: int = 100
    db_path: str = "market_data.db"
    performance_threshold: float = 0.1
    
    @validator('timeframes', 'indicators', pre=True)
    def split_comma_separated(cls, v):
        if isinstance(v, str):
            return [item.strip() for item in v.split(",")]
        return v

def load_config():
    env = os.getenv("ENVIRONMENT", "DEFAULT").upper()
    config_file = f"config_{env.lower()}.ini" if env != "DEFAULT" else "config.ini"
    
    # Here you would typically load from the config file
    # For simplicity, we're using default values
    return CryptoGMNConfig()

// File: models\gmn\crypto_gmn.py
import asyncio
from collections import deque
from concurrent.futures import ThreadPoolExecutor
import logging
from typing import Any, Dict, List, Optional, Union
import numpy as np
from pydantic import BaseModel, ValidationError

logger = logging.getLogger(__name__)

class CryptoGMNError(Exception):
    """Base exception for CryptoGMN."""
    pass

class DataValidationError(CryptoGMNError):
    """Exception raised when data validation fails."""
    pass

class DataModel(BaseModel):
    c: float
    v: Optional[float] = 0.0

    class Config:
        arbitrary_types_allowed = True

class CryptoGMN:
    def __init__(self, timeframes: List[str], max_history_length: int, executor_workers: int):
        self.timeframes = timeframes
        self.max_history_length = max_history_length
        self.market_data: Dict[str, Dict[str, deque]] = {
            timeframe: {'price': deque(maxlen=max_history_length), 'volume': deque(maxlen=max_history_length)}
            for timeframe in timeframes
        }
        self.locks: Dict[str, asyncio.Lock] = {timeframe: asyncio.Lock() for timeframe in timeframes}
        self._executor_lock = asyncio.Lock()
        self._executor: Optional[ThreadPoolExecutor] = None
        self.executor_workers = executor_workers

    async def __aenter__(self):
        async with self._executor_lock:
            if self._executor is None:
                self._executor = ThreadPoolExecutor(max_workers=self.executor_workers)
                logger.info(f"ThreadPoolExecutor initialized with {self.executor_workers} workers.")
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.shutdown()

    async def update_graph(self, new_data_items: List[Dict[str, Any]]) -> None:
        try:
            await asyncio.gather(*(self._update_single_data(data) for data in new_data_items))
        except Exception as e:
            logger.error(f"Error updating graph: {e}", exc_info=True)
            raise CryptoGMNError("Failed to update graph") from e

    async def _update_single_data(self, new_data: Dict[str, Any]) -> None:
        try:
            validated_data = DataModel(**new_data)
            price = validated_data.c
            volume = validated_data.v
        except ValidationError as e:
            logger.error(f"Data validation error: {e}. Data: {new_data}")
            raise DataValidationError(f"Invalid data format: {e}") from e

        try:
            await asyncio.gather(*(self._update_timeframe(tf, price, volume) for tf in self.timeframes))
        except Exception as e:
            logger.error(f"Error updating timeframes: {e}", exc_info=True)
            raise CryptoGMNError("Failed to update timeframes") from e

    async def _update_timeframe(self, timeframe: str, price: float, volume: float) -> None:
        async with self.locks[timeframe]:
            try:
                data = self.market_data[timeframe]
                data['price'].append(price)
                data['volume'].append(volume)
            except Exception as e:
                logger.error(f"Error updating timeframe {timeframe}: {e}", exc_info=True)
                raise CryptoGMNError(f"Failed to update timeframe {timeframe}") from e

    def get_data(self, timeframe: str, data_type: str) -> Optional[List[Union[float]]]:
        try:
            return list(self.market_data[timeframe][data_type])
        except KeyError:
            logger.warning(f"No data found for timeframe '{timeframe}' and data type '{data_type}'.")
            return None

    def get_all_data(self) -> Dict[str, Dict[str, List[Union[float]]]]:
        return {
            timeframe: {
                data_type: list(data)
                for data_type, data in indicators.items()
            }
            for timeframe, indicators in self.market_data.items()
        }

    async def shutdown(self) -> None:
        if self._executor:
            self._executor.shutdown(wait=True)
            self._executor = None
        logger.info("CryptoGMN has been shut down gracefully.")

// File: models\gmn\data_model.py
from pydantic import BaseModel, validator
from typing import Optional

class DataModel(BaseModel):
    c: float  # Close price
    v: Optional[float] = 0.0  # Volume

    @validator('c', 'v', pre=True)
    def validate_numeric(cls, v):
        if isinstance(v, (int, float)):
            return float(v)
        try:
            return float(v)
        except (ValueError, TypeError):
            raise ValueError(f"Value '{v}' is not a valid float.")

// File: models\gmn\gmn.py
import asyncio
from collections import deque
from concurrent.futures import ThreadPoolExecutor
from typing import Any, Dict, List, Optional, Union, Callable, cast
import numpy as np
import talib  # type: ignore  # Suppress type errors for TA-Lib
import logging
from configparser import ConfigParser
from pydantic import BaseModel, ValidationError, validator

logger = logging.getLogger(__name__)

# Load configuration from external file (config.ini)
config = ConfigParser()
config.read("config.ini")  # Ensure a config.ini file exists in the same directory

# Default configuration values
DEFAULT_TIMEFRAMES = config.get("DEFAULT", "timeframes", fallback="1m,5m").split(",")  # Example: ['1m', '5m']
DEFAULT_INDICATORS = config.get("DEFAULT", "indicators", fallback="price,volume,rsi,macd,fibonacci").split(",")  # Example: ['price', 'volume', 'rsi', 'macd', 'fibonacci']
MAX_HISTORY = config.getint("DEFAULT", "max_history", fallback=1000)
EXECUTOR_WORKERS = config.getint("DEFAULT", "executor_workers", fallback=5)


class CryptoGMNError(Exception):
    """Base exception for CryptoGMN."""
    pass


class IndicatorCalculationError(CryptoGMNError):
    """Exception raised when an indicator calculation fails."""
    pass


class DataModel(BaseModel):
    """Pydantic model for validating incoming data."""
    c: float  # Close price
    v: Optional[float] = 0.0  # Volume

    @validator('c', 'v', pre=True)
    def validate_numeric(cls, v):
        if isinstance(v, (int, float)):
            return float(v)
        try:
            return float(v)
        except (ValueError, TypeError):
            raise ValueError(f"Value '{v}' is not a valid float.")


class CryptoGMN:
    """Manages cryptocurrency market data and technical indicators."""

    def __init__(self, timeframes: List[str] = DEFAULT_TIMEFRAMES,  # Default values
                 indicators: List[str] = DEFAULT_INDICATORS,
                 max_history_length: int = MAX_HISTORY,
                 executor_workers: int = EXECUTOR_WORKERS) -> None:
        """
        Initializes the CryptoGMN instance.

        :param timeframes: List of timeframes (e.g., ['1m', '5m', '1h'])
        :param indicators: List of indicators (e.g., ['price', 'volume', 'rsi', 'macd', 'fibonacci'])
        :param max_history_length: Maximum number of data points to store per indicator
        :param executor_workers: Number of worker threads for indicator calculations
        """
        self.timeframes = timeframes
        self.indicators = indicators
        self.max_history_length = max_history_length
        self.market_data: Dict[str, Dict[str, deque]] = {
            timeframe: {indicator: deque(maxlen=max_history_length) for indicator in indicators}
            for timeframe in timeframes
        }
        self.locks: Dict[str, asyncio.Lock] = {timeframe: asyncio.Lock() for timeframe in timeframes}
        self.executor: ThreadPoolExecutor  # Explicitly type hint
        # Asynchronous context manager for Executor
        self._executor_lock = asyncio.Lock()
        self._executor: Optional[ThreadPoolExecutor] = None

    async def __aenter__(self):
        async with self._executor_lock:
            if self._executor is None:
                self._executor = ThreadPoolExecutor(max_workers=self.executor_workers)  # Use the config value here
            self.executor = cast(ThreadPoolExecutor, self._executor)  # Cast to make mypy happy.
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.shutdown()

    async def update_graph(self, new_data_items: List[Dict[str, Any]]) -> None:
        """Updates market data with new items concurrently."""
        await asyncio.gather(*(self._update_single_data(data) for data in new_data_items))  # More concise

    async def _update_single_data(self, new_data: Dict[str, Any]) -> None:
        """Updates data for a single new item across all timeframes."""
        try:
            validated_data = DataModel(**new_data)
            price = validated_data.c
            volume = validated_data.v
        except ValidationError as e:
            logger.error(f"Data validation error: {e}. Data: {new_data}")
            return

        await asyncio.gather(*(self._update_timeframe(tf, price, volume) for tf in self.timeframes))

    async def _update_timeframe(self, timeframe: str, price: float, volume: float) -> None:
        """Updates data for a specific timeframe, including batching for high-frequency data."""
        async with self.locks[timeframe]:
            data = self.market_data[timeframe]
            data['price'].append(price)
            data['volume'].append(volume)

            prices_array = np.array(data['price'], dtype=np.float64)

            if len(prices_array) >= 14:
                await self._calculate_indicators(timeframe, prices_array)

    async def _calculate_indicators(self, timeframe: str, prices_array: np.ndarray):
        """Calculates all indicators for a timeframe concurrently."""
        tasks = []
        if 'rsi' in self.indicators:
            tasks.append(self._calculate_and_append(timeframe, 'rsi', talib.RSI, prices_array, {'timeperiod': 14}))
        if 'macd' in self.indicators:
            tasks.append(self._calculate_and_append(
                timeframe, 'macd',
                lambda prices: self._macd_wrapper(prices),  # Lambda for MACD extraction
                prices_array
            ))
        if 'fibonacci' in self.indicators:
            tasks.append(self._calculate_and_append_fibonacci(timeframe, prices_array))
        # Add other indicators similarly if needed
        if tasks:
            await asyncio.gather(*tasks)

    async def _calculate_and_append(self, timeframe: str, indicator: str,
                                    indicator_func: Callable, prices: np.ndarray,
                                    kwargs: Dict[str, Any] = {}) -> None:
        """
        Calculates an indicator and appends the result to the market data.

        :param timeframe: The timeframe for which to calculate the indicator
        :param indicator: The name of the indicator
        :param indicator_func: The TA-Lib function to calculate the indicator
        :param prices: NumPy array of prices
        :param kwargs: Additional keyword arguments for the indicator function
        """
        loop = asyncio.get_running_loop()
        try:
            result = await loop.run_in_executor(
                self.executor, indicator_func, prices, **kwargs  # Simplify executor call
            )
        except Exception as e:
            logger.error(f"Error running indicator '{indicator}' for timeframe '{timeframe}': {e}")
            return

        if result is not None:
            try:
                latest = result[-1]
                if isinstance(latest, np.ndarray):
                    latest = latest.item()  # Convert numpy scalar to Python float
                if np.isnan(latest):
                    logger.warning(f"{indicator.upper()} returned NaN or empty result for timeframe {timeframe}")
                    return
                self.market_data[timeframe][indicator].append(float(latest))
            except IndexError:
                logger.warning(f"{indicator.upper()} returned NaN or empty result for timeframe {timeframe}")

    async def _calculate_and_append_fibonacci(self, timeframe: str, prices: np.ndarray, lookback: int = 14) -> None:
        """Calculates and appends Fibonacci levels."""
        loop = asyncio.get_running_loop()
        try:
            fibonacci_levels = await loop.run_in_executor(
                self.executor, self._calculate_fibonacci, prices, lookback
            )
        except Exception as e:
            logger.error(f"Error calculating Fibonacci for timeframe '{timeframe}': {e}")
            return

        if fibonacci_levels:
            self.market_data[timeframe]['fibonacci'].append(fibonacci_levels)

    def _macd_wrapper(self, prices: np.ndarray, fastperiod: int = 12, slowperiod: int = 26, signalperiod: int = 9) -> Optional[float]:
        """Wrapper for talib.MACD to return only the MACD line."""
        try:
            macd, _, _ = talib.MACD(prices, fastperiod=fastperiod, slowperiod=slowperiod, signalperiod=signalperiod)
            if macd is None or len(macd) == 0:
                logger.warning(f"MACD calculation returned no data for timeframe.")
                return None
            latest_macd = macd[-1]
            if np.isnan(latest_macd):
                logger.warning(f"MACD returned NaN for timeframe.")
                return None
            return float(latest_macd)
        except Exception as e:
            logger.error(f"Error in _macd_wrapper: {e}")
            return None

    def _calculate_fibonacci(self, prices: np.ndarray, lookback: int) -> Optional[Dict[str, Union[float, bool]]]:
        """Calculates Fibonacci retracement levels and indicates the closest level."""
        try:
            if len(prices) < lookback:
                logger.warning("Not enough data to calculate Fibonacci retracement.")
                return None
            recent_prices = prices[-lookback:]
            high = np.max(recent_prices)
            low = np.min(recent_prices)
            close = recent_prices[-1]
            diff = high - low
            if diff == 0:
                logger.warning("High and low prices are the same; cannot calculate Fibonacci levels.")
                return None  # Avoid division by zero

            levels = {
                "23.6%": high - 0.236 * diff,
                "38.2%": high - 0.382 * diff,
                "50%": high - 0.5 * diff,
                "61.8%": high - 0.618 * diff,
                "78.6%": high - 0.786 * diff,
                "100%": low,
            }
            closest_level_key = min(
                levels.keys(), key=lambda k: abs(levels[k] - close)
            )
            for k in levels:
                levels[k] = {"value": levels[k], "is_closest": k == closest_level_key}  # Indicate closest

            return levels
        except Exception as e:
            logger.error(f"Error calculating Fibonacci retracement: {e}")
            return None

    def get_data(self, timeframe: str, indicator: str) -> Optional[List[Union[float, Dict[str, Union[float, bool]]]]]:
        """Retrieves the latest data for a specific timeframe and indicator."""
        try:
            return list(self.market_data[timeframe][indicator])
        except KeyError:
            logger.warning(
                f"No data found for timeframe '{timeframe}' and indicator '{indicator}'."
            )
            return None

    def get_all_data(self) -> Dict[str, Dict[str, List[Union[float, Dict[str, Union[float, bool]]]]]]:
        """Retrieves all market data across all timeframes and indicators."""
        return {
            timeframe: {
                indicator: list(data)
                for indicator, data in indicators.items()
            }
            for timeframe, indicators in self.market_data.items()
        }

    async def shutdown(self) -> None:
        """Shuts down the executor gracefully."""
        if self._executor:
            self._executor.shutdown(wait=True)
            self._executor = None  # Important for proper cleanup
        logger.info("CryptoGMN has been shut down gracefully.")


// File: models\gmn\logger.py
import logging

class CryptoGMNLogger:
    def __init__(self, name: str, level: int = logging.INFO):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(level)
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        
        file_handler = logging.FileHandler('cryptogmn.log')
        file_handler.setFormatter(formatter)
        self.logger.addHandler(file_handler)
        
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        self.logger.addHandler(console_handler)

logger = CryptoGMNLogger("CryptoGMN").logger

// File: models\gmn\main.py
import asyncio
import logging
import os
from crypto_gmn import CryptoGMN
from config import load_config

async def main():
    # Optionally set the environment variable before running
    # os.environ["ENVIRONMENT"] = "PRODUCTION"

    config = load_config()
    
    async with CryptoGMN(config) as gmn:
        # Example batch of new data points
        new_data = [
            {'c': 150.0, 'v': 1000},
            {'c': 155.0, 'v': 1200},
            {'c': 160.0, 'v': 1500},
            {'c': 158.0, 'v': 1100},
            {'c': 162.0, 'v': 1300},
        ]

        # Update the graph with new data
        await gmn.update_graph(new_data)

        # Retrieve specific indicator data
        rsi_data = gmn.get_data('1m', 'rsi')
        macd_data = gmn.get_data('1m', 'macd')
        fibonacci_data = gmn.get_data('1m', 'fibonacci')

        print(f"RSI Data for 1m: {rsi_data}")
        print(f"MACD Data for 1m: {macd_data}")
        print(f"Fibonacci Data for 1m: {fibonacci_data}")

        # Retrieve all market data
        all_data = gmn.get_all_data()
        print(f"All Market Data: {all_data}")

        # Dynamically add a new indicator (e.g., EMA)
        ema_func = gmn.indicator_factory.create_ema(timeperiod=50)
        gmn.add_indicator('1m', 'ema', ema_func)

        # Update with new data to trigger EMA calculation
        new_data = [
            {'c': 165.0, 'v': 1400},
            {'c': 170.0, 'v': 1600},
        ]
        await gmn.update_graph(new_data)

        ema_data = gmn.get_data('1m', 'ema')
        print(f"EMA Data for 1m: {ema_data}")

        # Fetch real-time data from Binance
        await gmn.fetch_real_time_data(exchange='binance', symbol='BTCUSDT')

        # Access cached indicator data
        cached_rsi = gmn.get_cached_indicator('1m', 'rsi', window=10)
        print(f"Cached RSI Data for 1m (last 10): {cached_rsi}")

if __name__ == "__main__":
    asyncio.run(main())

// File: models\gmn\performance_monitor.py
from collections import deque

class PerformanceMonitor:
    def __init__(self, window_size: int = 100):
        self.processing_times = deque(maxlen=window_size)

    def record(self, start_time: float, end_time: float):
        self.processing_times.append(end_time - start_time)

    @property
    def average_processing_time(self):
        return sum(self.processing_times) / len(self.processing_times) if self.processing_times else 0

// File: models\gmn\utils.py
import functools
from logger import logger

def retry(exceptions, tries=3, delay=1.0):
    def decorator(func):
        @functools.wraps(func)
        async def wrapper(*args, **kwargs):
            for attempt in range(tries):
                try:
                    return await func(*args, **kwargs)
                except exceptions as e:
                    logger.warning(f"Attempt {attempt + 1} failed with error: {e}")
                    if attempt < tries - 1:
                        await asyncio.sleep(delay)
                    else:
                        logger.error(f"All {tries} attempts failed for function {func.__name__}")
                        raise
        return wrapper
    return decorator

// File: models\gmn\__init__.py


// File: models\lnn\lnn_model.py
# File: models/lnn/lnn_model.py

import torch
import torch.nn as nn

class LiquidNeuralNetwork(nn.Module):
    def __init__(self, input_size: int, hidden_size: int, output_size: int, num_layers: int = 2, dropout: float = 0.2):
        super(LiquidNeuralNetwork, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, output_size)
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.ReLU()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass of the LNN.
        Args:
            x: Input tensor of shape (batch_size, seq_len, input_size)
        Returns:
            Output tensor of shape (batch_size, output_size)
        """
        out, _ = self.lstm(x)  # out: (batch_size, seq_len, hidden_size)
        out = self.dropout(out[:, -1, :])  # Take the output from the last time step
        out = self.relu(out)
        out = self.fc(out)
        return out


// File: models\lnn\train_lnn.py
# File: models/lnn/train_lnn.py

import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from models.lnn.lnn_model import LiquidNeuralNetwork
from models.utils.config import Config
from sklearn.preprocessing import MinMaxScaler
import logging


def train_lnn():
    """Trains the LNN model and saves it."""
    config = Config("configs/config.yaml")
    
    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )

    try:
        # Load and prepare your data
        # Replace 'your_training_data.csv' with your actual data source
        data = pd.read_csv("data/your_training_data.csv")  # Ensure this file exists and is correctly formatted
        X = data.iloc[:, :-1].values
        y = data.iloc[:, -1].values

        # Scale the input features
        scaler = MinMaxScaler()
        X_scaled = scaler.fit_transform(X)

        # Convert to tensors
        X_tensor = torch.tensor(X_scaled, dtype=torch.float32).unsqueeze(1)  # Shape: (batch_size, seq_len, input_size)
        y_tensor = torch.tensor(y, dtype=torch.float32)  # Shape: (batch_size,)

        # Create DataLoader
        dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)
        dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)

        # Initialize the model, loss function, and optimizer
        input_size = X_scaled.shape[1]
        model = LiquidNeuralNetwork(input_size, config.lnn_hidden_size, 1)
        criterion = nn.BCEWithLogitsLoss()  # Suitable for binary classification
        optimizer = optim.Adam(model.parameters(), lr=config.lnn_learning_rate)

        # Training loop
        epochs = config.lnn_training_epochs
        for epoch in range(epochs):
            model.train()
            epoch_loss = 0.0
            for batch_X, batch_y in dataloader:
                optimizer.zero_grad()
                outputs = model(batch_X.half()).squeeze()  # Forward pass
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                epoch_loss += loss.item()
            avg_loss = epoch_loss / len(dataloader)
            logging.info(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")

        # Save the trained model
        torch.save(model.state_dict(), config.lnn_model_path)
        logging.info(f"LNN model trained and saved to {config.lnn_model_path}")

    except FileNotFoundError:
        logging.error("Training data file not found. Please provide a valid CSV file.")
    except Exception as e:
        logging.error(f"Error during LNN training: {e}")


if __name__ == "__main__":
    train_lnn()


// File: models\lnn\__init__.py


// File: models\utils\config.py
# File: models/utils/config.py

import yaml
from typing import List, Dict, Any


class Config:
    def __init__(self, config_path: str):
        try:
            with open(config_path, "r") as f:
                self.config: Dict[str, Any] = yaml.safe_load(f)

                # Trading Parameters
                self.symbol: str = self.config.get("symbol", "BTC_USDT")
                self.interval: str = self.config.get("interval", "Min1")
                self.timeframes: List[str] = self.config.get("timeframes", ["1m", "5m", "15m", "1h", "4h"])
                self.indicators: List[str] = self.config.get("indicators", ["price", "volume", "rsi", "macd", "fibonacci"])

                # GMN Parameters
                self.max_history_length: int = self.config.get("max_history_length", 1000)

                # LNN Parameters
                self.lnn_model_path: str = self.config.get("lnn_model_path", "models/lnn/lnn_model.pth")
                self.lnn_hidden_size: int = self.config.get("lnn_hidden_size", 64)
                self.lnn_training_epochs: int = self.config.get("lnn_training_epochs", 10)
                self.training_history_length: int = self.config.get("training_history_length", 500)
                self.lnn_learning_rate: float = self.config.get("lnn_learning_rate", 0.001)

                # Agent Parameters
                self.threshold_buy: float = self.config.get("threshold_buy", 0.7)
                self.threshold_sell: float = self.config.get("threshold_sell", 0.3)

                # Risk Management
                self.risk_parameters: Dict[str, Any] = self.config.get("risk_parameters", {})

                # Trade Execution
                self.trade_parameters: Dict[str, Any] = self.config.get("trade_parameters", {})

                # System
                self.agent_loop_delay: int = self.config.get("agent_loop_delay", 1)
                self.reconnect_delay: int = self.config.get("reconnect_delay", 5)
                self.log_level = self.config.get("log_level", "INFO")

        except FileNotFoundError:
            raise FileNotFoundError(f"Config file not found: {config_path}")
        except yaml.YAMLError as e:
            raise ValueError(f"Error parsing YAML config file: {e}")


// File: models\utils\risk_management.py
# File: models/utils/risk_management.py

import logging
from typing import Dict, Any


class RiskManager:
    def __init__(self, risk_parameters: Dict[str, Any]):
        self.max_drawdown = risk_parameters.get("max_drawdown", 0.1)
        self.max_position_size = risk_parameters.get("max_position_size", 0.05)  # Example: 5% of portfolio
        # ... (initialize other risk parameters as needed)

    def check_risk(self, current_drawdown: float, current_position: str, market_data: Dict) -> bool:
        """
        Checks if the current trade action is within risk parameters.
        Args:
            current_drawdown (float): Current drawdown ratio.
            current_position (str): Current position ('long', 'short', or None).
            market_data (Dict): Latest market data for additional risk checks.
        Returns:
            bool: True if within risk parameters, False otherwise.
        """
        if current_drawdown > self.max_drawdown:
            logging.warning(
                f"Risk check failed: Drawdown ({current_drawdown:.2f}) exceeds maximum allowed ({self.max_drawdown:.2f})."
            )
            return False

        # Example: Prevent increasing position size beyond maximum allowed
        # Implement additional risk checks based on current_position and market_data
        # For instance, limit the number of concurrent positions, check volatility, etc.

        return True


// File: models\utils\__init__.py


// File: tests\test_processed_data.py
import pytest
import asyncio
from data.data_processor import DataProcessor
from data.config import Config
from data.indicator_calculations import IndicatorCalculator
from data.error_handler import ErrorHandler
from data.storage.data_storage import DataStorage
from data.mexc_websocket_connector import MexcWebsocketConnector


@pytest.mark.asyncio
async def test_realtime_mexc_data_pipeline():
    """
    Test the data pipeline with real-time data from the MEXC WebSocket API.
    It connects to the WebSocket, processes incoming kline data in real-time,
    and validates that the unified feed is being generated correctly.
    """
    # Step 1: Set up Config, ErrorHandler, and IndicatorCalculator
    config = Config()
    error_handler = ErrorHandler()
    indicator_calculator = IndicatorCalculator(error_handler)

    # Step 2: Initialize DataStorage to store processed data
    data_storage = DataStorage()

    # Step 3: Initialize DataProcessor with real components
    processor = DataProcessor(data_storage, indicator_calculator, error_handler, config)

    # Step 4: Set up WebSocket connection to MEXC
    # Make sure you have set your ws_url in config.yaml to "wss://wbs.mexc.com/ws"
    ws_data_queue = asyncio.Queue()  # Queue for processing incoming data
    mexc_connector = MexcWebsocketConnector(config, ws_data_queue)

    # Step 5: Function to process real-time data
    async def process_real_time_data():
        while True:
            try:
                data_batch = await ws_data_queue.get()
                if data_batch:  # Ensure there's data to process
                    await processor.process_data(data_batch)
            except Exception as e:
                print(f"Error while processing real-time data: {e}")
                break

    # Step 6: Start WebSocket connection and data processing concurrently
    websocket_task = asyncio.create_task(mexc_connector.connect())
    data_processing_task = asyncio.create_task(process_real_time_data())

    # Step 7: Allow the tasks to run for a certain amount of time (e.g., 2 minutes) for test purposes
    await asyncio.sleep(120)  # Adjust sleep time for a longer test duration if needed

    # Step 8: Cancel the tasks after the test period
    websocket_task.cancel()
    data_processing_task.cancel()

    # Step 9: Validate stored data after 2 minutes of real-time data collection
    symbol = "BTC_USDT"
    unified_feed_1m = data_storage.get_data(symbol, "1m")
    unified_feed_5m = data_storage.get_data(symbol, "5m")

    # Ensure the unified feed contains data after processing real-time WebSocket data
    assert not unified_feed_1m.empty, "Unified feed for 1m timeframe is empty"
    assert not unified_feed_5m.empty, "Unified feed for 5m timeframe is empty"

    # Print some of the collected data for manual inspection
    print(f"Unified feed for 1m: {unified_feed_1m.head()}")
    print(f"Unified feed for 5m: {unified_feed_5m.head()}")

    # Validate specific indicators in the unified feed
    validate_indicators(unified_feed_1m)
    validate_indicators(unified_feed_5m)


def validate_indicators(unified_feed):
    """Helper function to validate the indicator contents."""
    assert 'rsi' in unified_feed.columns, "Missing RSI indicator"
    assert 'macd' in unified_feed.columns, "Missing MACD indicator"
    assert 'fibonacci' in unified_feed.columns, "Missing Fibonacci indicator"


// File: tests\__init__.py


