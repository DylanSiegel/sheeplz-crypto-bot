// File: crypto_trading_bot.py
# File: crypto_trading_bot.py

import asyncio
import logging
import tracemalloc
import os
from typing import Tuple, Optional, Dict
import torch
import numpy as np
import aiohttp  # Asynchronous HTTP client
import joblib    # For saving and loading scalers

from data.mexc_data_ingestion import DataIngestion
from models.agents.agent import TradingAgent
from models.gmn.gmn import CryptoGMN
from models.lnn.lnn_model import LiquidNeuralNetwork
from models.utils.risk_management import RiskManager
from models.utils.config import Config
from sklearn.preprocessing import MinMaxScaler

# Load configuration
config = Config("configs/config.yaml")

# Configure logging
log_file_path = os.path.join(os.getcwd(), "logs", "trading_bot.log")
os.makedirs(os.path.dirname(log_file_path), exist_ok=True)

logging.basicConfig(
    level=getattr(logging, config.log_level.upper(), logging.INFO),
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file_path),
        logging.StreamHandler()
    ]
)

# Initialize scaler and save path
scaler_path = "models/lnn/scaler.joblib"

async def main():
    """Main function to run the crypto trading bot."""
    timeframes = config.timeframes
    indicators = config.indicators
    max_history_length = config.max_history_length

    # Initialize GMN
    gmn = CryptoGMN(timeframes, indicators, max_history_length=max_history_length)

    # Initialize Data Ingestion
    data_ingestion = DataIngestion(gmn, config)

    # Initialize Risk Manager
    risk_manager = RiskManager(config.risk_parameters)

    # Load or initialize scaler
    scaler = await load_scaler(scaler_path)

    # Initialize or train LNN Model
    lnn_model_path = config.lnn_model_path
    try:
        model_state_dict = torch.load(lnn_model_path, map_location=torch.device('cpu'))
        input_size = len(timeframes) * len(indicators)  # Input size based on features
        model = LiquidNeuralNetwork(input_size, config.lnn_hidden_size, 1)
        model.load_state_dict(model_state_dict)
        model.eval()
        logging.info("Loaded pre-trained LNN model.")
    except FileNotFoundError:
        logging.info("Pre-trained LNN model not found. Training a new model...")
        try:
            model = await train_and_save_lnn(gmn, lnn_model_path, config=config, scaler=scaler)
            if model is None:
                logging.error("Failed to train LNN model. Exiting.")
                await shutdown(gmn, data_ingestion, risk_manager, scaler)
                return
        except Exception as e:
            logging.error(f"Error preparing training data or training the model: {e}")
            await shutdown(gmn, data_ingestion, risk_manager, scaler)
            return
    except Exception as e:
        logging.error(f"Error loading LNN model: {e}")
        await shutdown(gmn, data_ingestion, risk_manager, scaler)
        return

    # Initialize TradingAgent with the loaded scaler
    agent = TradingAgent(timeframes, indicators, model, config, risk_manager, scaler)

    tracemalloc.start()

    # Create asyncio tasks
    tasks = [
        asyncio.create_task(data_ingestion.connect()),
        asyncio.create_task(agent_loop(agent, gmn))
    ]

    try:
        await asyncio.gather(*tasks)
    except asyncio.CancelledError:
        logging.info("Main tasks have been cancelled.")
    except Exception as e:
        logging.error(f"Error in main execution: {e}")
    finally:
        # Memory profiling
        snapshot = tracemalloc.take_snapshot()
        top_stats = snapshot.statistics('lineno')
        logging.info("Top 10 memory allocations:")
        for stat in top_stats[:10]:
            logging.info(stat)

        tracemalloc.stop()
        await shutdown(gmn, data_ingestion, risk_manager, scaler, agent)

async def shutdown(gmn: CryptoGMN, data_ingestion: DataIngestion, risk_manager: RiskManager, scaler: MinMaxScaler, agent: Optional[TradingAgent] = None):
    """Gracefully shuts down all components."""
    try:
        gmn.shutdown()
    except Exception as e:
        logging.error(f"Error shutting down GMN: {e}")

    try:
        await data_ingestion.close()
    except AttributeError:
        # If DataIngestion does not have a close method
        pass
    except Exception as e:
        logging.error(f"Error shutting down Data Ingestion: {e}")

    try:
        await agent.close()
    except AttributeError:
        # If TradingAgent does not have a close method
        pass
    except Exception as e:
        logging.error(f"Error shutting down Trading Agent: {e}")

    logging.info("Shutdown complete.")

async def agent_loop(agent: TradingAgent, gmn: CryptoGMN):
    """The main agent loop that retrieves data, makes decisions, and executes trades."""
    while True:
        market_data = {}
        for timeframe in agent.timeframes:
            market_data[timeframe] = {}
            for indicator in agent.indicators:
                try:
                    data = gmn.get_data(timeframe, indicator)
                    if data is not None:
                        market_data[timeframe][indicator] = data
                except ValueError as e:
                    logging.error(f"Error getting data for {timeframe} {indicator} from GMN: {e}")

        try:
            await agent.make_decision(market_data)
        except Exception as e:
            logging.error(f"Error in agent loop: {e}")

        await asyncio.sleep(config.agent_loop_delay)

async def train_and_save_lnn(
    gmn: CryptoGMN, 
    model_path: str, 
    config: Config, 
    scaler: MinMaxScaler
) -> Optional[LiquidNeuralNetwork]:
    """Trains a new LNN model and saves it to the specified path."""
    try:
        X_train, y_train = await prepare_lnn_training_data(gmn, config.training_history_length)
        if X_train is None or y_train is None:
            logging.error("Failed to prepare LNN training data.")
            return None

        input_size = X_train.shape[1]
        model = LiquidNeuralNetwork(input_size, config.lnn_hidden_size, 1)
        criterion = torch.nn.BCEWithLogitsLoss()  # Suitable for binary classification
        optimizer = torch.optim.Adam(model.parameters(), lr=config.lnn_learning_rate)

        epochs = config.lnn_training_epochs
        batch_size = config.config.get("lnn_batch_size", 32)  # Optional: Add batch_size to config.yaml
        dataset = torch.utils.data.TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))
        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

        for epoch in range(epochs):
            model.train()
            epoch_loss = 0.0
            for batch_X, batch_y in dataloader:
                optimizer.zero_grad()
                # Move data to the same device as model if using GPU
                outputs = model(batch_X.unsqueeze(1))  # Shape: (batch_size, 1)
                loss = criterion(outputs.squeeze(), batch_y)
                loss.backward()
                optimizer.step()
                epoch_loss += loss.item()
            avg_loss = epoch_loss / len(dataloader)
            logging.info(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")

        # Save the trained model
        torch.save(model.state_dict(), model_path)
        model.eval()
        logging.info(f"LNN model trained and saved to {model_path}")

        # Save the scaler for future use
        joblib.dump(scaler, scaler_path)
        logging.info(f"Scaler saved to {scaler_path}")

        return model

    except Exception as e:
        logging.error(f"Error during LNN training: {e}")
        return None

async def prepare_lnn_training_data(gmn: CryptoGMN, history_length: int = 500) -> Tuple[np.ndarray, np.ndarray]:
    """
    Prepares training data for the LNN model.
    Generates feature vectors and corresponding labels based on historical market data.
    """
    try:
        market_data = gmn.get_all_data()

        # Check if sufficient data is available
        if not market_data or len(market_data['1m']['price']) < history_length + 1:
            logging.error("Not enough data to prepare training dataset.")
            return None, None

        X = []
        y = []

        # Prepare feature vectors and labels
        for i in range(history_length, len(market_data['1m']['price']) - 1):
            features = []

            for timeframe in gmn.timeframes:
                for indicator in gmn.indicators:
                    data_series = market_data[timeframe].get(indicator)
                    if data_series and len(data_series) > i:
                        value = data_series[i]
                        if isinstance(value, dict):
                            # Flatten dictionary values
                            features.extend(list(value.values()))
                        else:
                            features.append(value)
                    else:
                        features.append(0.0)  # Placeholder for missing data

            # Target: Future price change (binary classification)
            future_price = market_data['1m']['price'][i + 1]  # Next minute's price
            current_price = market_data['1m']['price'][i]
            price_change = (future_price - current_price) / current_price

            y.append(1 if price_change > 0 else 0)  # Binary label

            X.append(features)

        # Convert to numpy arrays
        X = np.array(X, dtype=np.float32)
        y = np.array(y, dtype=np.float32)

        # Scale features using the same scaler as during training
        # This scaler should be fitted during training and loaded here
        scaler = joblib.load(scaler_path)
        X_scaled = scaler.transform(X)

        logging.info("LNN training data prepared successfully.")
        return X_scaled, y

    except Exception as e:
        logging.error(f"Error preparing LNN training data: {e}")
        return None, None

async def load_scaler(scaler_path: str) -> MinMaxScaler:
    """Loads an existing scaler or initializes a new one."""
    if os.path.exists(scaler_path):
        scaler = joblib.load(scaler_path)
        logging.info(f"Loaded scaler from {scaler_path}.")
    else:
        scaler = MinMaxScaler()
        logging.info("Initialized new MinMaxScaler.")
    return scaler

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logging.info("Trading bot stopped by user.")


// File: main.py
# File: main.py

import asyncio
from crypto_trading_bot import main

if __name__ == "__main__":
    asyncio.run(main())


// File: configs\config.yaml
# File: configs/config.yaml

symbol: BTC_USDT
interval: Min1
timeframes: ["1m", "5m", "15m", "1h", "4h"]
indicators: ["price", "volume", "rsi", "macd", "fibonacci"]
max_history_length: 1000
lnn_model_path: models/lnn/lnn_model.pth
lnn_hidden_size: 64
lnn_training_epochs: 10
training_history_length: 500
lnn_learning_rate: 0.001
threshold_buy: 0.7
threshold_sell: 0.3
risk_parameters:
  max_drawdown: 0.1
  max_position_size: 0.05
trade_parameters:
  leverage: 20
  order_type: 1  # 1: Market order
  volume: 1
  open_type: 1  # 1: Isolated margin
agent_loop_delay: 1
reconnect_delay: 5
log_level: INFO


// File: data\mexc_data_ingestion.py
# File: data/mexc_data_ingestion.py

import asyncio
import json
import logging
import websockets
import os
import hmac
import hashlib
import time
from dotenv import load_dotenv
from models.utils.config import Config
import websockets.exceptions as ws_exceptions

load_dotenv('.env')  # Load environment variables from .env file

class DataIngestion:
    def __init__(self, gmn: CryptoGMN, config: Config):
        self.config = config
        self.gmn = gmn
        self.symbol = config.symbol
        self.interval = config.interval
        self.ws_url = os.getenv('MEXC_WS_URL', 'wss://contract.mexc.com/ws')
        self.api_key = os.getenv('MEXC_API_KEY')
        self.api_secret = os.getenv('MEXC_API_SECRET')

    async def connect(self):
        while True:
            try:
                async with websockets.connect(self.ws_url) as websocket:
                    logging.info(f"Connected to {self.ws_url}")

                    if self.api_key and self.api_secret:
                        await self._login(websocket)

                    await self._subscribe_public_channels(websocket)
                    await self._receive_data_loop(websocket)

            except ws_exceptions.ConnectionClosedOK as e:
                logging.warning(f"WebSocket connection closed gracefully: {e.reason}")
                break
            except ws_exceptions.ConnectionClosedError as e:
                logging.error(f"WebSocket connection closed with error: {e.reason}")
                await asyncio.sleep(self.config.reconnect_delay)
                continue
            except Exception as e:
                logging.error(f"Connection error: {e}")
                await asyncio.sleep(self.config.reconnect_delay)
                continue

    async def _login(self, websocket):
        timestamp = int(time.time() * 1000)
        signature = self._generate_signature(timestamp)
        login_message = {
            "method": "login",
            "param": {
                "apiKey": self.api_key,
                "signature": signature,
                "timestamp": timestamp
            }
        }
        await websocket.send(json.dumps(login_message))
        response = await websocket.recv()
        logging.info(f"Login response: {response}")

    def _generate_signature(self, timestamp: int) -> str:
        message = f"{self.api_key}{timestamp}"
        signature = hmac.new(
            self.api_secret.encode('utf-8'),
            message.encode('utf-8'),
            hashlib.sha256
        ).hexdigest()
        return signature

    async def _subscribe_public_channels(self, websocket):
        subscription_message = {
            "method": "sub.kline",
            "param": {"symbol": self.symbol, "interval": self.interval},
            "id": 1
        }
        await websocket.send(json.dumps(subscription_message))
        await asyncio.sleep(1)  # Brief pause to allow subscription to process
        logging.info(f"Subscribed to {self.symbol} {self.interval} kline data.")

    async def _receive_data_loop(self, websocket):
        while True:
            try:
                message = await websocket.recv()
                data = json.loads(message)
                await self._process_data(data)
            except ws_exceptions.ConnectionClosedOK as e:
                logging.warning(f"WebSocket connection closed gracefully: {e.reason}")
                break
            except ws_exceptions.ConnectionClosedError as e:
                logging.error(f"WebSocket connection closed with error: {e.reason}")
                await asyncio.sleep(self.config.reconnect_delay)
                break
            except json.JSONDecodeError:
                logging.error("Received invalid JSON data.")
            except Exception as e:
                logging.error(f"Error in receive_data_loop: {e}", exc_info=True)
                break

    async def _process_data(self, data: Dict):
        if 'data' in data and 'channel' in data and data['channel'] == 'push.kline':
            await self._process_kline_data(data['data'])

    async def _process_kline_data(self, kline_data):
        try:
            if isinstance(kline_data, list):
                # Assuming kline_data is a list of candle dictionaries
                await self.gmn.update_graph(kline_data)
                logging.debug(f"Updated GMN with kline data: {kline_data}")
            elif isinstance(kline_data, dict):
                # Single candle data
                await self.gmn.update_graph([kline_data])
                logging.debug(f"Updated GMN with kline data: {kline_data}")
            else:
                logging.warning("Received kline data in unexpected format.")
        except Exception as e:
            logging.error(f"Error updating GMN with kline data: {e}")


// File: data\__init__.py


// File: models\__init__.py


// File: models\agents\agent.py
# File: models/agents/agent.py

import asyncio
import logging
import numpy as np
import torch
from sklearn.preprocessing import MinMaxScaler
from models.utils.config import Config
from models.utils.risk_management import RiskManager
import os
import aiohttp  # Ensure aiohttp is imported
import hashlib
import hmac
import time
import joblib

class TradingAgent:
    def __init__(
        self, 
        timeframes, 
        indicators, 
        model, 
        config: Config, 
        risk_manager: RiskManager, 
        scaler: MinMaxScaler
    ):
        self.timeframes = timeframes
        self.indicators = indicators
        self.model = model
        self.config = config
        self.threshold_buy = config.threshold_buy
        self.threshold_sell = config.threshold_sell
        self.risk_manager = risk_manager
        self.trade_parameters = config.trade_parameters
        self.leverage = self.trade_parameters.get("leverage", 20)
        self.position = None
        self.scaler = scaler
        self.api_key = os.getenv("MEXC_API_KEY")
        self.api_secret = os.getenv("MEXC_API_SECRET")
        self.base_url = 'https://contract.mexc.com/api/v1/'  # Update if necessary

        self.peak_portfolio_value = 1.0  # Initialize for drawdown calculations
        self.portfolio_value = 1.0  # Initialize portfolio value

        # Initialize aiohttp session
        self.session = aiohttp.ClientSession()

    async def make_decision(self, market_data):
        """Processes market data, makes predictions, and executes trades based on the model's output."""
        try:
            input_vector = self._prepare_input(market_data)
            if input_vector is None:
                logging.warning("Input vector is None. Skipping decision.")
                return

            # Model expects input in float32
            input_tensor = torch.tensor([input_vector], dtype=torch.float32)

            with torch.no_grad():
                self.model.eval()
                prediction = self.model(input_tensor)
                prediction = torch.sigmoid(prediction)  # Apply sigmoid to get probability
                prediction_value = prediction.item()

            logging.info(f"Model Prediction Probability: {prediction_value:.4f}")

            current_drawdown = self.calculate_current_drawdown()

            if self.risk_manager.check_risk(current_drawdown, self.position, market_data):
                if prediction_value >= self.threshold_buy and self.position != 'long':
                    await self._execute_trade('buy')
                    self.position = 'long'
                elif prediction_value <= self.threshold_sell and self.position != 'short':
                    await self._execute_trade('sell')
                    self.position = 'short'
                elif self.threshold_sell < prediction_value < self.threshold_buy:
                    if self.position is not None:
                        await self._execute_trade('close')
                        self.position = None
            else:
                logging.warning("Risk management check failed. Not executing trade.")

        except Exception as e:
            logging.error(f"Error in make_decision: {e}")

    def _prepare_input(self, market_data):
        """Prepares and scales the input vector for the LNN model."""
        input_vector = []
        for timeframe in self.timeframes:
            for indicator in self.indicators:
                data = market_data.get(timeframe, {}).get(indicator)
                if data is None or len(data) == 0:
                    logging.warning(f"Missing data for {timeframe} {indicator}. Skipping.")
                    return None

                if isinstance(data[-1], dict):
                    values = [v for v in data[-1].values() if isinstance(v, (int, float))]
                    input_vector.extend(values)
                else:
                    input_vector.append(data[-1])

        if not input_vector:
            logging.warning("Input vector is empty. No data available for making a decision.")
            return None

        input_vector = np.array([input_vector], dtype=np.float32)
        input_vector = self.scaler.transform(input_vector).astype(np.float32)
        return input_vector.flatten()

    async def _execute_trade(self, action, symbol="BTC_USDT"):
        """Executes a trade action (buy, sell, close) via the MEXC API using aiohttp."""
        try:
            timestamp = int(time.time() * 1000)
            params = {
                "symbol": symbol,
                "timestamp": timestamp
            }

            # Determine trade parameters based on action
            if action == 'buy':
                side = 'OPEN_LONG'
                quantity = self.trade_parameters.get("volume", 1)
            elif action == 'sell':
                side = 'OPEN_SHORT'
                quantity = self.trade_parameters.get("volume", 1)
            elif action == 'close':
                if self.position == 'long':
                    side = 'CLOSE_LONG'
                elif self.position == 'short':
                    side = 'CLOSE_SHORT'
                else:
                    logging.warning("No position to close.")
                    return
                quantity = self.trade_parameters.get("volume", 1)
            else:
                logging.warning(f"Invalid trade action: {action}")
                return

            # Set additional parameters
            params.update({
                "price": '',  # Empty for market orders
                "vol": quantity,
                "side": side,
                "type": self.trade_parameters.get("order_type", 1),  # 1: Market order
                "leverage": self.leverage,
                "openType": self.trade_parameters.get("open_type", 1),  # 1: Isolated margin
            })

            # Generate signature
            query_string = '&'.join([f"{key}={params[key]}" for key in sorted(params)])
            signature = hmac.new(
                self.api_secret.encode('utf-8'),
                query_string.encode('utf-8'),
                hashlib.sha256
            ).hexdigest()
            params['sign'] = signature

            # Send request using aiohttp
            url = self.base_url + 'order/submit'
            async with self.session.post(url, params=params) as response:
                response_data = await response.json()
                if response.status == 200 and response_data.get("success", False):
                    logging.info(f"Successfully executed {action} order: {response_data}")
                    # Update portfolio value based on trade execution
                    self.update_portfolio(action, response_data)
                else:
                    logging.error(f"Failed to execute {action} order: {response_data}")

        except Exception as e:
            logging.error(f"Error executing trade: {e}")

    def calculate_current_drawdown(self):
        """Calculates the current drawdown based on portfolio value."""
        self.peak_portfolio_value = max(self.peak_portfolio_value, self.portfolio_value)
        drawdown = (self.peak_portfolio_value - self.portfolio_value) / self.peak_portfolio_value
        logging.info(f"Current Drawdown: {drawdown:.4f}")
        return drawdown

    def update_portfolio(self, action: str, response_data: Dict):
        """
        Updates the portfolio value based on the executed trade.
        This is a placeholder function. You need to implement actual portfolio management logic.
        """
        # Example: Update portfolio based on the price and quantity
        try:
            price = float(response_data.get('data', {}).get('price', self.portfolio_value))
            quantity = float(response_data.get('data', {}).get('vol', 0))
            if action == 'buy':
                # Example logic: Increase portfolio value
                self.portfolio_value += price * quantity
            elif action == 'sell':
                # Example logic: Decrease portfolio value
                self.portfolio_value -= price * quantity
            elif action == 'close':
                # Example logic: Neutralize position
                pass
            logging.info(f"Portfolio updated after {action}: {self.portfolio_value}")
        except Exception as e:
            logging.error(f"Error updating portfolio: {e}")

    async def close(self):
        """Closes the aiohttp session."""
        await self.session.close()


// File: models\agents\rl_agent.py
# models/agents/rl_agent.py

import gym
import numpy as np
import torch
from stable_baselines3 import PPO

class TradingEnvironment(gym.Env):
    def __init__(self, market_data):
        super(TradingEnvironment, self).__init__()
        self.market_data = market_data
        self.current_step = 0
        self.action_space = gym.spaces.Discrete(3)  # Buy, Sell, Hold
        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(OBSERVATION_SPACE_SIZE,), dtype=np.float32)

    def reset(self):
        self.current_step = 0
        return self._next_observation()

    def _next_observation(self):
        # Return the next market observation
        obs = self.market_data.iloc[self.current_step]
        return obs.values

    def step(self, action):
        # Execute one time step within the environment
        self.current_step += 1

        reward = self._calculate_reward(action)
        done = self.current_step >= len(self.market_data) - 1
        obs = self._next_observation() if not done else np.zeros(self.observation_space.shape)

        return obs, reward, done, {}

    def _calculate_reward(self, action):
        # Implement reward calculation
        return reward

def train_rl_agent():
    # Load market data
    market_data = pd.read_csv("data/distilled/distilled_data.csv")
    env = TradingEnvironment(market_data)
    model = PPO("MlpPolicy", env, verbose=1)
    model.learn(total_timesteps=10000)
    model.save("models/agents/ppo_agent")

if __name__ == "__main__":
    train_rl_agent()


// File: models\agents\__init__.py


// File: models\gmn\gmn.py
# File: models/gmn/gmn.py

import networkx as nx
from collections import deque
import numpy as np
import talib
import logging
import asyncio
import concurrent.futures
import threading

class CryptoGMN:
    def __init__(self, timeframes, indicators, max_history_length=1000):
        self.timeframes = timeframes
        self.indicators = indicators
        self.max_history_length = max_history_length
        self.graph = nx.Graph()
        self._initialize_nodes()
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=5)
        self.lock = threading.Lock()

    def _initialize_nodes(self):
        for timeframe in self.timeframes:
            for indicator in self.indicators:
                self.graph.add_node(
                    (timeframe, indicator),
                    data=deque(maxlen=self.max_history_length)
                )

    async def update_graph(self, new_data_items):
        """Asynchronously updates the graph with new data."""
        loop = asyncio.get_running_loop()
        await loop.run_in_executor(self.executor, self._update_node_data_batch, new_data_items)

    def _update_node_data_batch(self, new_data_items):
        """Synchronously updates the graph nodes with new data."""
        with self.lock:
            for new_data_item in new_data_items:
                for timeframe in self.timeframes:
                    try:
                        # Update price and volume
                        price = float(new_data_item.get('c', 0.0))  # 'c' for close price
                        volume = float(new_data_item.get('v', 0.0))  # 'v' for volume
                        self.graph.nodes[(timeframe, "price")]['data'].append(price)
                        self.graph.nodes[(timeframe, "volume")]['data'].append(volume)

                        # Recalculate indicators
                        prices = list(self.graph.nodes[(timeframe, "price")]['data'])
                        if len(prices) >= 14:  # Minimum data length for indicators
                            if 'rsi' in self.indicators:
                                rsi = self.calculate_rsi(prices)
                                if rsi is not None:
                                    self.graph.nodes[(timeframe, "rsi")]['data'].append(rsi)
                            if 'macd' in self.indicators:
                                macd = self.calculate_macd(prices)
                                if macd is not None:
                                    self.graph.nodes[(timeframe, "macd")]['data'].append(macd)
                            if 'fibonacci' in self.indicators:
                                fibonacci = self.calculate_fibonacci(prices)
                                if fibonacci is not None:
                                    self.graph.nodes[(timeframe, "fibonacci")]['data'].append(fibonacci)
                    except Exception as e:
                        logging.error(f"Error updating data for timeframe {timeframe}: {e}, Data: {new_data_item}")

    def get_data(self, timeframe, indicator):
        """Retrieves the latest data for a specific timeframe and indicator."""
        with self.lock:
            try:
                data = self.graph.nodes[(timeframe, indicator)]['data']
                return list(data) if data else None
            except KeyError:
                logging.error(f"No data for {timeframe} and {indicator}")
                return None

    def get_all_data(self):
        """Retrieves all market data across all timeframes and indicators."""
        with self.lock:
            market_data = {}
            for timeframe in self.timeframes:
                market_data[timeframe] = {}
                for indicator in self.indicators:
                    data = self.graph.nodes.get((timeframe, indicator), {}).get('data')
                    market_data[timeframe][indicator] = list(data) if data else []
            return market_data

    def shutdown(self):
        """Shuts down the executor gracefully."""
        self.executor.shutdown(wait=True)

    def calculate_rsi(self, prices, period=14):
        """Calculates the Relative Strength Index (RSI)."""
        try:
            rsi = talib.RSI(np.array(prices, dtype=np.float64), timeperiod=period)
            return rsi[-1] if len(rsi) > 0 else None
        except Exception as e:
            logging.error(f"Error calculating RSI: {e}")
            return None

    def calculate_macd(self, prices, fastperiod=12, slowperiod=26, signalperiod=9):
        """Calculates the Moving Average Convergence Divergence (MACD)."""
        try:
            macd, macdsignal, macdhist = talib.MACD(
                np.array(prices, dtype=np.float64),
                fastperiod=fastperiod,
                slowperiod=slowperiod,
                signalperiod=signalperiod
            )
            return macd[-1] if len(macd) > 0 else None
        except Exception as e:
            logging.error(f"Error calculating MACD: {e}")
            return None

    def calculate_fibonacci(self, prices, lookback=14):
        """Calculates the closest Fibonacci retracement level based on the latest price."""
        try:
            if len(prices) < lookback:
                return None
            recent_prices = prices[-lookback:]
            high = max(recent_prices)
            low = min(recent_prices)
            close = prices[-1]
            diff = high - low
            levels = {
                "23.6%": high - 0.236 * diff,
                "38.2%": high - 0.382 * diff,
                "50%": high - 0.5 * diff,
                "61.8%": high - 0.618 * diff,
                "78.6%": high - 0.786 * diff,
                "100%": low
            }
            # Determine which level the close price is closest to
            closest_level = min(levels, key=lambda x: abs(levels[x] - close))
            return levels[closest_level]
        except Exception as e:
            logging.error(f"Error calculating Fibonacci retracement: {e}")
            return None


// File: models\gmn\__init__.py


// File: models\lnn\lnn_model.py
# File: models/lnn/lnn_model.py

import torch
import torch.nn as nn

class LiquidNeuralNetwork(nn.Module):
    def __init__(self, input_size: int, hidden_size: int, output_size: int, num_layers: int = 2, dropout: float = 0.2):
        super(LiquidNeuralNetwork, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, output_size)
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.ReLU()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass of the LNN.
        Args:
            x: Input tensor of shape (batch_size, seq_len, input_size)
        Returns:
            Output tensor of shape (batch_size, output_size)
        """
        out, _ = self.lstm(x)  # out: (batch_size, seq_len, hidden_size)
        out = self.dropout(out[:, -1, :])  # Take the output from the last time step
        out = self.relu(out)
        out = self.fc(out)
        return out


// File: models\lnn\train_lnn.py
# File: models/lnn/train_lnn.py

import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from models.lnn.lnn_model import LiquidNeuralNetwork
from models.utils.config import Config
import numpy as np
from sklearn.preprocessing import MinMaxScaler
import logging


def train_lnn():
    """Trains the LNN model and saves it."""
    config = Config("configs/config.yaml")
    
    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )

    try:
        # Load and prepare your data
        # Replace 'your_training_data.csv' with your actual data source
        data = pd.read_csv("data/your_training_data.csv")  # Ensure this file exists and is correctly formatted
        X = data.iloc[:, :-1].values
        y = data.iloc[:, -1].values

        # Scale the input features
        scaler = MinMaxScaler()
        X_scaled = scaler.fit_transform(X)

        # Convert to tensors
        X_tensor = torch.tensor(X_scaled, dtype=torch.float32).unsqueeze(1)  # Shape: (batch_size, seq_len, input_size)
        y_tensor = torch.tensor(y, dtype=torch.float32)  # Shape: (batch_size,)

        # Create DataLoader
        dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)
        dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)

        # Initialize the model, loss function, and optimizer
        input_size = X_scaled.shape[1]
        model = LiquidNeuralNetwork(input_size, config.lnn_hidden_size, 1)
        criterion = nn.BCEWithLogitsLoss()  # Suitable for binary classification
        optimizer = optim.Adam(model.parameters(), lr=config.lnn_learning_rate)

        # Training loop
        epochs = config.lnn_training_epochs
        for epoch in range(epochs):
            model.train()
            epoch_loss = 0.0
            for batch_X, batch_y in dataloader:
                optimizer.zero_grad()
                outputs = model(batch_X.half()).squeeze()  # Forward pass
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                epoch_loss += loss.item()
            avg_loss = epoch_loss / len(dataloader)
            logging.info(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")

        # Save the trained model
        torch.save(model.state_dict(), config.lnn_model_path)
        logging.info(f"LNN model trained and saved to {config.lnn_model_path}")

    except FileNotFoundError:
        logging.error("Training data file not found. Please provide a valid CSV file.")
    except Exception as e:
        logging.error(f"Error during LNN training: {e}")


if __name__ == "__main__":
    train_lnn()


// File: models\lnn\__init__.py


// File: models\utils\config.py
# File: models/utils/config.py

import yaml
import logging
from typing import List, Dict, Any


class Config:
    def __init__(self, config_path: str):
        try:
            with open(config_path, "r") as f:
                self.config: Dict[str, Any] = yaml.safe_load(f)

                # Trading Parameters
                self.symbol: str = self.config.get("symbol", "BTC_USDT")
                self.interval: str = self.config.get("interval", "Min1")
                self.timeframes: List[str] = self.config.get("timeframes", ["1m", "5m", "15m", "1h", "4h"])
                self.indicators: List[str] = self.config.get("indicators", ["price", "volume", "rsi", "macd", "fibonacci"])

                # GMN Parameters
                self.max_history_length: int = self.config.get("max_history_length", 1000)

                # LNN Parameters
                self.lnn_model_path: str = self.config.get("lnn_model_path", "models/lnn/lnn_model.pth")
                self.lnn_hidden_size: int = self.config.get("lnn_hidden_size", 64)
                self.lnn_training_epochs: int = self.config.get("lnn_training_epochs", 10)
                self.training_history_length: int = self.config.get("training_history_length", 500)
                self.lnn_learning_rate: float = self.config.get("lnn_learning_rate", 0.001)

                # Agent Parameters
                self.threshold_buy: float = self.config.get("threshold_buy", 0.7)
                self.threshold_sell: float = self.config.get("threshold_sell", 0.3)

                # Risk Management
                self.risk_parameters: Dict[str, Any] = self.config.get("risk_parameters", {})

                # Trade Execution
                self.trade_parameters: Dict[str, Any] = self.config.get("trade_parameters", {})

                # System
                self.agent_loop_delay: int = self.config.get("agent_loop_delay", 1)
                self.reconnect_delay: int = self.config.get("reconnect_delay", 5)
                self.log_level = self.config.get("log_level", "INFO")

        except FileNotFoundError:
            raise FileNotFoundError(f"Config file not found: {config_path}")
        except yaml.YAMLError as e:
            raise ValueError(f"Error parsing YAML config file: {e}")


// File: models\utils\risk_management.py
# File: models/utils/risk_management.py

import logging
from typing import Dict, Any


class RiskManager:
    def __init__(self, risk_parameters: Dict[str, Any]):
        self.max_drawdown = risk_parameters.get("max_drawdown", 0.1)
        self.max_position_size = risk_parameters.get("max_position_size", 0.05)  # Example: 5% of portfolio
        # ... (initialize other risk parameters as needed)

    def check_risk(self, current_drawdown: float, current_position: str, market_data: Dict) -> bool:
        """
        Checks if the current trade action is within risk parameters.
        Args:
            current_drawdown (float): Current drawdown ratio.
            current_position (str): Current position ('long', 'short', or None).
            market_data (Dict): Latest market data for additional risk checks.
        Returns:
            bool: True if within risk parameters, False otherwise.
        """
        if current_drawdown > self.max_drawdown:
            logging.warning(
                f"Risk check failed: Drawdown ({current_drawdown:.2f}) exceeds maximum allowed ({self.max_drawdown:.2f})."
            )
            return False

        # Example: Prevent increasing position size beyond maximum allowed
        # Implement additional risk checks based on current_position and market_data
        # For instance, limit the number of concurrent positions, check volatility, etc.

        return True


// File: models\utils\__init__.py


// File: tests\test_config.py
# File: tests/test_config.py

import pytest
import os
from models.utils.config import Config
from unittest.mock import mock_open, patch
import yaml

def test_config_loading():
    mock_yaml = """
    symbol: BTC_USDT
    interval: Min1
    timeframes: ["1m", "5m", "15m", "1h", "4h"]
    indicators: ["price", "volume", "rsi", "macd", "fibonacci"]
    max_history_length: 1000
    lnn_model_path: models/lnn/lnn_model.pth
    lnn_hidden_size: 64
    lnn_training_epochs: 10
    training_history_length: 500
    lnn_learning_rate: 0.001
    threshold_buy: 0.7
    threshold_sell: 0.3
    risk_parameters:
      max_drawdown: 0.1
      max_position_size: 0.05
    trade_parameters:
      leverage: 20
      order_type: 1
      volume: 1
      open_type: 1
    agent_loop_delay: 1
    reconnect_delay: 5
    log_level: INFO
    """
    with patch("builtins.open", mock_open(read_data=mock_yaml)):
        config = Config("configs/config.yaml")
        assert config.symbol == "BTC_USDT"
        assert config.interval == "Min1"
        assert config.timeframes == ["1m", "5m", "15m", "1h", "4h"]
        assert config.indicators == ["price", "volume", "rsi", "macd", "fibonacci"]
        assert config.max_history_length == 1000
        assert config.lnn_model_path == "models/lnn/lnn_model.pth"
        assert config.lnn_hidden_size == 64
        assert config.lnn_training_epochs == 10
        assert config.training_history_length == 500
        assert config.lnn_learning_rate == 0.001
        assert config.threshold_buy == 0.7
        assert config.threshold_sell == 0.3
        assert config.risk_parameters == {"max_drawdown": 0.1, "max_position_size": 0.05}
        assert config.trade_parameters == {
            "leverage": 20,
            "order_type": 1,
            "volume": 1,
            "open_type": 1
        }
        assert config.agent_loop_delay == 1
        assert config.reconnect_delay == 5
        assert config.log_level == "INFO"

def test_config_file_not_found():
    with pytest.raises(FileNotFoundError):
        Config("configs/non_existent_config.yaml")

def test_config_invalid_yaml():
    invalid_yaml = "symbol: BTC_USDT\ninvalid_yaml: [unclosed"
    with patch("builtins.open", mock_open(read_data=invalid_yaml)):
        with pytest.raises(ValueError):
            Config("configs/config.yaml")


// File: tests\test_crypto_gmn.py
# File: tests/test_crypto_gmn.py

import pytest
from models.gmn.gmn import CryptoGMN

@pytest.fixture
def gmn():
    timeframes = ["1m"]
    indicators = ["price", "volume", "rsi", "macd", "fibonacci"]
    return CryptoGMN(timeframes, indicators, max_history_length=100)

@pytest.mark.asyncio
async def test_gmn_data_update(gmn):
    new_data = [{
        "t": 1625097600000,
        "o": "35000.00",
        "h": "35100.00",
        "l": "34900.00",
        "c": "35050.00",
        "v": "100.5",
        "q": "3520250.00"
    }]
    await gmn.update_graph(new_data)

    price_data = gmn.get_data("1m", "price")
    volume_data = gmn.get_data("1m", "volume")

    assert price_data == [35050.00]
    assert volume_data == [100.5]

@pytest.mark.asyncio
async def test_gmn_indicator_calculations(gmn):
    # Insert enough data to calculate indicators
    for i in range(15):
        await gmn.update_graph([{
            "t": 1625097600000 + i * 60000,
            "o": "35000.00",
            "h": "35100.00",
            "l": "34900.00",
            "c": str(35050 + i),
            "v": "100.5",
            "q": "3520250.00"
        }])

    rsi_data = gmn.get_data("1m", "rsi")
    macd_data = gmn.get_data("1m", "macd")
    fibonacci_data = gmn.get_data("1m", "fibonacci")

    assert len(rsi_data) == 1
    assert len(macd_data) == 1
    assert len(fibonacci_data) == 1

@pytest.mark.asyncio
async def test_gmn_concurrent_updates(gmn):
    # Simulate concurrent updates
    new_data_1 = [{
        "t": 1625097600000,
        "o": "35000.00",
        "h": "35100.00",
        "l": "34900.00",
        "c": "35050.00",
        "v": "100.5",
        "q": "3520250.00"
    }]
    new_data_2 = [{
        "t": 1625097660000,
        "o": "35050.00",
        "h": "35150.00",
        "l": "34950.00",
        "c": "35100.00",
        "v": "101.0",
        "q": "3530250.00"
    }]

    await asyncio.gather(
        gmn.update_graph(new_data_1),
        gmn.update_graph(new_data_2)
    )

    price_data = gmn.get_data("1m", "price")
    volume_data = gmn.get_data("1m", "volume")

    assert price_data == [35050.00, 35100.00]
    assert volume_data == [100.5, 101.0]


// File: tests\test_crypto_trading_bot.py
# File: tests/test_crypto_trading_bot.py

import pytest
import asyncio
from unittest.mock import AsyncMock, patch
from crypto_trading_bot import main, shutdown
from models.gmn.gmn import CryptoGMN
from models.agents.agent import TradingAgent
from models.utils.risk_management import RiskManager
from models.utils.config import Config
from sklearn.preprocessing import MinMaxScaler

@pytest.mark.asyncio
async def test_main_success():
    # Mock Config
    with patch("models.utils.config.Config.__init__", return_value=None) as mock_config_init:
        config = Config("configs/config.yaml")
        config.timeframes = ["1m"]
        config.indicators = ["price", "volume", "rsi", "macd", "fibonacci"]
        config.max_history_length = 1000
        config.lnn_model_path = "models/lnn/lnn_model.pth"
        config.lnn_hidden_size = 64
        config.lnn_training_epochs = 1
        config.training_history_length = 500
        config.lnn_learning_rate = 0.001
        config.threshold_buy = 0.7
        config.threshold_sell = 0.3
        config.risk_parameters = {"max_drawdown": 0.1, "max_position_size": 0.05}
        config.trade_parameters = {"leverage": 20, "order_type": 1, "volume": 1, "open_type": 1}
        config.agent_loop_delay = 1
        config.reconnect_delay = 5
        config.log_level = "INFO"

        # Mock CryptoGMN
        with patch("crypto_trading_bot.CryptoGMN", return_value=AsyncMock(spec=CryptoGMN)) as mock_gmn_cls:
            mock_gmn = mock_gmn_cls.return_value
            mock_gmn.get_all_data.return_value = {
                "1m": {
                    "price": [35050.00] * 501,
                    "volume": [100.5] * 501,
                    "rsi": [50] * 501,
                    "macd": [0.1] * 501,
                    "fibonacci": [35000.00] * 501
                }
            }

            # Mock DataIngestion
            with patch("crypto_trading_bot.DataIngestion", return_value=AsyncMock()) as mock_data_ingestion_cls:
                mock_data_ingestion = mock_data_ingestion_cls.return_value
                mock_data_ingestion.connect = AsyncMock()

                # Mock TradingAgent
                with patch("crypto_trading_bot.TradingAgent", return_value=AsyncMock()) as mock_agent_cls:
                    mock_agent = mock_agent_cls.return_value
                    mock_agent.make_decision = AsyncMock()

                    # Mock model loading
                    with patch("torch.load", return_value={}):
                        with patch.object(TradingAgent, 'close', new_callable=AsyncMock):
                            # Run main and ensure it starts correctly
                            task = asyncio.create_task(main())
                            await asyncio.sleep(0.1)  # Allow some time for tasks
                            task.cancel()
                            with pytest.raises(asyncio.CancelledError):
                                await task

                            mock_data_ingestion.connect.assert_called_once()
                            mock_agent.make_decision.assert_not_called()  # As no data was fed

@pytest.mark.asyncio
async def test_main_with_model_training():
    # Similar to above, but simulate missing model and trigger training
    pass  # Placeholder for further integration tests


// File: tests\test_data_ingestion.py


// File: tests\test_gmn.py
# File: tests/test_gmn.py

import pytest
from models.gmn.gmn import CryptoGMN
from collections import deque
import asyncio

@pytest.fixture
def gmn():
    timeframes = ["1m"]
    indicators = ["price", "volume", "rsi", "macd", "fibonacci"]
    return CryptoGMN(timeframes, indicators, max_history_length=100)

@pytest.mark.asyncio
async def test_gmn_update_graph(gmn):
    new_data = [{
        "t": 1625097600000,
        "o": "35000.00",
        "h": "35100.00",
        "l": "34900.00",
        "c": "35050.00",
        "v": "100.5",
        "q": "3520250.00"
    }]
    await gmn.update_graph(new_data)

    price_data = gmn.get_data("1m", "price")
    volume_data = gmn.get_data("1m", "volume")
    rsi_data = gmn.get_data("1m", "rsi")
    macd_data = gmn.get_data("1m", "macd")
    fibonacci_data = gmn.get_data("1m", "fibonacci")

    assert price_data == [35050.00]
    assert volume_data == [100.5]
    assert len(rsi_data) == 0  # Not enough data for RSI
    assert len(macd_data) == 0  # Not enough data for MACD
    assert len(fibonacci_data) == 0  # Not enough data for Fibonacci

    # Add more data to trigger indicator calculations
    for i in range(1, 15):
        await gmn.update_graph([{
            "t": 1625097600000 + i * 60000,
            "o": "35000.00",
            "h": "35100.00",
            "l": "34900.00",
            "c": str(35000 + i),
            "v": "100.5",
            "q": "3520250.00"
        }])

    price_data = gmn.get_data("1m", "price")
    assert len(price_data) == 15
    assert rsi_data is not None
    assert len(rsi_data) == 1
    assert macd_data is not None
    assert len(macd_data) == 1
    assert fibonacci_data is not None
    assert len(fibonacci_data) == 1

@pytest.mark.asyncio
async def test_gmn_concurrent_updates(gmn):
    # Simulate concurrent updates
    new_data_1 = [{
        "t": 1625097600000,
        "o": "35000.00",
        "h": "35100.00",
        "l": "34900.00",
        "c": "35050.00",
        "v": "100.5",
        "q": "3520250.00"
    }]
    new_data_2 = [{
        "t": 1625097660000,
        "o": "35050.00",
        "h": "35150.00",
        "l": "34950.00",
        "c": "35100.00",
        "v": "101.0",
        "q": "3530250.00"
    }]

    await asyncio.gather(
        gmn.update_graph(new_data_1),
        gmn.update_graph(new_data_2)
    )

    price_data = gmn.get_data("1m", "price")
    volume_data = gmn.get_data("1m", "volume")
    assert price_data == [35050.00, 35100.00]
    assert volume_data == [100.5, 101.0]


// File: tests\test_lnn_model.py
# File: tests/test_lnn_model.py

import pytest
import torch
from models.lnn.lnn_model import LiquidNeuralNetwork

def test_lnn_forward():
    input_size = 5
    hidden_size = 10
    output_size = 1
    batch_size = 2
    seq_len = 1

    model = LiquidNeuralNetwork(input_size, hidden_size, output_size)
    model.eval()

    # Create a dummy input tensor
    dummy_input = torch.randn(batch_size, seq_len, input_size)
    output = model(dummy_input)

    # Check output shape
    assert output.shape == (batch_size, output_size)

def test_lnn_training_step():
    input_size = 5
    hidden_size = 10
    output_size = 1

    model = LiquidNeuralNetwork(input_size, hidden_size, output_size)
    criterion = torch.nn.BCEWithLogitsLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    # Dummy data
    X = torch.randn(4, 1, input_size)
    y = torch.tensor([1, 0, 1, 0], dtype=torch.float32)

    # Forward pass
    outputs = model(X).squeeze()
    loss = criterion(outputs, y)

    # Backward pass
    loss.backward()
    optimizer.step()

    # Ensure loss is a scalar
    assert loss.item() >= 0


// File: tests\test_risk_management.py
# File: tests/test_risk_management.py

import pytest
from models.utils.risk_management import RiskManager

@pytest.fixture
def risk_manager():
    params = {
        "max_drawdown": 0.1,
        "max_position_size": 0.05
    }
    return RiskManager(params)

def test_risk_within_limits(risk_manager):
    current_drawdown = 0.05
    current_position = 'long'
    market_data = {}
    assert risk_manager.check_risk(current_drawdown, current_position, market_data) == True

def test_risk_exceeds_drawdown(risk_manager):
    current_drawdown = 0.15
    current_position = 'long'
    market_data = {}
    assert risk_manager.check_risk(current_drawdown, current_position, market_data) == False

def test_risk_without_position(risk_manager):
    current_drawdown = 0.05
    current_position = None
    market_data = {}
    assert risk_manager.check_risk(current_drawdown, current_position, market_data) == True

def test_risk_edge_case(risk_manager):
    current_drawdown = 0.1
    current_position = 'short'
    market_data = {}
    assert risk_manager.check_risk(current_drawdown, current_position, market_data) == False  # Assuming >= is considered exceeding


// File: tests\test_rl_agent.py
# File: tests/test_rl_agent.py

import pytest
from models.agents.rl_agent import TradingEnvironment, train_rl_agent
import pandas as pd
from unittest.mock import patch

@pytest.fixture
def mock_market_data():
    data = {
        "price": [100, 102, 98, 101, 99, 105, 103],
        "volume": [10, 15, 10, 20, 15, 25, 20],
        "rsi": [50, 55, 45, 60, 50, 65, 55],
        "macd": [0.1, 0.2, -0.1, 0.3, 0.0, 0.4, 0.2],
        "fibonacci": [99, 100, 98, 101, 99, 105, 103]
    }
    return pd.DataFrame(data)

def test_trading_environment_reset(mock_market_data):
    env = TradingEnvironment(mock_market_data)
    obs = env.reset()
    assert env.current_step == 0
    assert len(obs) == len(mock_market_data.columns)

def test_trading_environment_step(mock_market_data):
    env = TradingEnvironment(mock_market_data)
    env.reset()
    obs, reward, done, info = env.step(0)  # Action: Buy
    assert env.current_step == 1
    assert not done
    assert len(obs) == len(mock_market_data.columns)


// File: tests\test_trading_agent.py
# File: tests/test_trading_agent.py

import pytest
import asyncio
from unittest.mock import AsyncMock, patch
import torch
import numpy as np
from models.agents.agent import TradingAgent
from models.utils.config import Config
from models.utils.risk_management import RiskManager
from sklearn.preprocessing import MinMaxScaler

@pytest.fixture
def mock_config():
    config_data = {
        "threshold_buy": 0.7,
        "threshold_sell": 0.3,
        "trade_parameters": {
            "leverage": 20,
            "order_type": 1,
            "volume": 1,
            "open_type": 1
        }
    }
    with patch("models.utils.config.Config.__init__", return_value=None):
        config = Config("configs/config.yaml")
        config.threshold_buy = 0.7
        config.threshold_sell = 0.3
        config.trade_parameters = {
            "leverage": 20,
            "order_type": 1,
            "volume": 1,
            "open_type": 1
        }
        return config

@pytest.fixture
def mock_risk_manager():
    return AsyncMock(spec=RiskManager)

@pytest.fixture
def mock_model():
    model = AsyncMock(spec=torch.nn.Module)
    model.eval = AsyncMock()
    model.__call__ = AsyncMock(return_value=torch.sigmoid(torch.tensor([0.8])))
    return model

@pytest.fixture
def trading_agent(mock_config, mock_risk_manager, mock_model):
    scaler = MinMaxScaler()
    scaler.fit([[1,2,3], [4,5,6]])  # Dummy fit
    return TradingAgent(
        timeframes=["1m"],
        indicators=["price", "volume", "rsi", "macd", "fibonacci"],
        model=mock_model,
        config=mock_config,
        risk_manager=mock_risk_manager,
        scaler=scaler
    )

@pytest.mark.asyncio
async def test_trading_agent_make_decision_buy(trading_agent):
    # Mock market data indicating a buy signal
    market_data = {
        "1m": {
            "price": [35050.00],
            "volume": [100.5],
            "rsi": [50],
            "macd": [0.1],
            "fibonacci": [35000.00]
        }
    }

    with patch.object(trading_agent, '_execute_trade', new_callable=AsyncMock) as mock_execute_trade:
        with patch.object(trading_agent.risk_manager, 'check_risk', return_value=True):
            await trading_agent.make_decision(market_data)
            mock_execute_trade.assert_called_with('buy')

@pytest.mark.asyncio
async def test_trading_agent_make_decision_sell(trading_agent):
    # Mock market data indicating a sell signal
    trading_agent.model.return_value = torch.sigmoid(torch.tensor([-0.5]))
    market_data = {
        "1m": {
            "price": [35100.00],
            "volume": [101.0],
            "rsi": [60],
            "macd": [0.2],
            "fibonacci": [35050.00]
        }
    }

    with patch.object(trading_agent, '_execute_trade', new_callable=AsyncMock) as mock_execute_trade:
        with patch.object(trading_agent.risk_manager, 'check_risk', return_value=True):
            await trading_agent.make_decision(market_data)
            mock_execute_trade.assert_called_with('sell')

@pytest.mark.asyncio
async def test_trading_agent_make_decision_hold(trading_agent):
    # Mock market data indicating a hold signal
    trading_agent.model.return_value = torch.sigmoid(torch.tensor([0.5]))
    market_data = {
        "1m": {
            "price": [35150.00],
            "volume": [102.0],
            "rsi": [55],
            "macd": [0.15],
            "fibonacci": [35100.00]
        }
    }

    with patch.object(trading_agent, '_execute_trade', new_callable=AsyncMock) as mock_execute_trade:
        with patch.object(trading_agent.risk_manager, 'check_risk', return_value=True):
            await trading_agent.make_decision(market_data)
            mock_execute_trade.assert_not_called()

@pytest.mark.asyncio
async def test_trading_agent_risk_check_failed(trading_agent):
    # Mock market data indicating a buy signal but risk check fails
    trading_agent.model.return_value = torch.sigmoid(torch.tensor([0.8]))
    market_data = {
        "1m": {
            "price": [35200.00],
            "volume": [103.0],
            "rsi": [65],
            "macd": [0.25],
            "fibonacci": [35150.00]
        }
    }

    with patch.object(trading_agent.risk_manager, 'check_risk', return_value=False):
        with patch.object(trading_agent, '_execute_trade', new_callable=AsyncMock) as mock_execute_trade:
            await trading_agent.make_decision(market_data)
            mock_execute_trade.assert_not_called()


// File: tests\test_trading_agent_portfolio.py


// File: tests\test_train_lnn.py
# File: tests/test_train_lnn.py

import pytest
from unittest.mock import patch, MagicMock
from models.lnn.train_lnn import train_lnn

@pytest.mark.asyncio
async def test_train_lnn_success():
    # Mock Config
    with patch("models.utils.config.Config.__init__", return_value=None) as mock_config_init:
        config = Config("configs/config.yaml")
        config.lnn_hidden_size = 64
        config.lnn_training_epochs = 1
        config.training_history_length = 500
        config.lnn_learning_rate = 0.001
        config.lnn_model_path = "models/lnn/lnn_model.pth"

        # Mock data loading
        with patch("pandas.read_csv") as mock_read_csv:
            mock_df = MagicMock()
            mock_df.iloc.__getitem__.return_value = MagicMock()
            mock_read_csv.return_value = mock_df

            # Mock scaler
            with patch("sklearn.preprocessing.MinMaxScaler.fit_transform", return_value=[[1,2,3], [4,5,6]]):
                # Mock torch functionalities
                with patch("torch.tensor") as mock_torch_tensor:
                    mock_torch_tensor.return_value = MagicMock()
                    with patch("torch.optim.Adam") as mock_optimizer:
                        mock_optimizer.return_value.step = MagicMock()
                        mock_optimizer.return_value.zero_grad = MagicMock()
                        mock_optimizer.return_value.zero_grad.return_value = None
                        mock_optimizer.return_value.step.return_value = None

                        # Mock model save
                        with patch("torch.save") as mock_torch_save:
                            await train_lnn()
                            mock_torch_save.assert_called()

def test_train_lnn_file_not_found():
    # Simulate FileNotFoundError during data loading
    with patch("models.utils.config.Config.__init__", return_value=None):
        config = Config("configs/config.yaml")
        config.lnn_training_epochs = 1
        config.lnn_model_path = "models/lnn/lnn_model.pth"

        with patch("pandas.read_csv", side_effect=FileNotFoundError):
            with patch("logging.error") as mock_logging_error:
                with patch("models.lnn.train_lnn.Config", return_value=config):
                    asyncio.run(train_lnn())
                    mock_logging_error.assert_called_with("Training data file not found. Please provide a valid CSV file.")

def test_train_lnn_exception():
    # Simulate a generic exception during training
    with patch("models.utils.config.Config.__init__", return_value=None):
        config = Config("configs/config.yaml")
        config.lnn_training_epochs = 1
        config.lnn_model_path = "models/lnn/lnn_model.pth"

        with patch("pandas.read_csv", return_value=MagicMock()):
            with patch("sklearn.preprocessing.MinMaxScaler.fit_transform", side_effect=Exception("Scaling error")):
                with patch("logging.error") as mock_logging_error:
                    asyncio.run(train_lnn())
                    mock_logging_error.assert_called_with("Error during LNN training: Scaling error")


// File: tests\test_utils.py


// File: tests\__init__.py


