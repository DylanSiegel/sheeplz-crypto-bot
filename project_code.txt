// File: .gitattributes
# Auto detect text files and perform LF normalization
* text=auto


// File: .gitignore
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/#use-with-ide
.pdm.toml

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/


// File: README.md
AI-Powered Crypto Trading Bot - README
1. Project Overview

This repository contains the code for an AI-powered cryptocurrency trading bot built using deep reinforcement learning (DRL). The bot is designed as a research platform for developing and testing advanced trading strategies, emphasizing flexibility in reward function design, data ingestion, and multi-agent experimentation.

2. Key Features

Dynamic Reward Function Framework: Enables easy iteration and testing of various reward strategies to optimize trading performance based on desired metrics (e.g., profit, Sharpe ratio, custom metrics).

Flexible Data Ingestion: Supports multiple data sources, including live market data from exchanges (e.g., Binance), historical data from CSV files, and synthetic data generation for comprehensive testing and development in diverse scenarios.

Modular Gym Environment: Built using OpenAI Gym, the trading environment is easily configurable to adapt to different trading scenarios, asset pairs, and data inputs, allowing for focused experimentation and research on specific market conditions or trading strategies.

Online Learning: The AI model utilizes online learning to continuously update its parameters as new data streams in, enabling adaptation to evolving market dynamics and real-time trading decisions based on the latest information.

Multi-Agent Framework: Facilitates comparative testing of multiple trading agents simultaneously, each with potentially different configurations (e.g., reward functions, model architectures, hyperparameters), allowing for robust evaluation and selection of the most effective strategies.

Experiment Tracking: Integrates with tools like MLflow or Weights & Biases to log experiments, track metrics, visualize performance, and compare different configurations and outcomes, streamlining the research and development process.

3. Getting Started
3.1 Prerequisites

Python 3.9+: Ensure you have Python 3.9 or higher installed.

Poetry: We recommend using Poetry for dependency management. Install it using:

curl -sSL https://install.python-poetry.org | python3 -
content_copy
Use code with caution.
Bash

Required Packages: Install project dependencies using Poetry:

poetry install
content_copy
Use code with caution.
Bash

Exchange API Keys (Optional): If using live market data, obtain API keys from your chosen exchange (e.g., Binance) and configure them securely in the config/secrets.env file.

3.2 Installation

Clone the repository:

git clone https://github.com/your-username/crypto-trading-bot.git
content_copy
Use code with caution.
Bash

Navigate to the project directory:

cd crypto-trading-bot
content_copy
Use code with caution.
Bash

Activate the virtual environment:

poetry shell
content_copy
Use code with caution.
Bash
3.3 Configuration

config/: This directory contains YAML configuration files for various aspects of the bot, including data sources, model parameters, trading parameters, feature engineering settings, and more. Review and adjust these files as needed for your specific experiments.

secrets.env: This file (located in config/) stores sensitive information like API keys. Ensure it is properly configured and excluded from version control (add it to .gitignore).

3.4 Running the Bot

Training: To train a trading agent, use the scripts/run.py script with appropriate configurations. For example:

python scripts/run.py model=lstm reward_function=profit data_source=historical
content_copy
Use code with caution.
Bash

Backtesting: To backtest a trained agent on historical data, use the scripts/backtest.py script.

Hyperparameter Tuning: To optimize hyperparameters using Optuna, use the scripts/tune_hyperparameters.py script.

4. Project Structure

config/: Configuration files for various aspects of the bot.

data/: Stores raw and processed market data.

docs/: Project documentation.

environments/: Custom Gym environment for cryptocurrency trading.

models/: Deep reinforcement learning model implementations.

scripts/: Executable scripts for various tasks (training, backtesting, etc.).

src/: Core logic of the trading bot (data acquisition, feature engineering, trading execution, etc.).

tests/: Unit and integration tests.

5. Contributing

We welcome contributions from the community! Please refer to the CONTRIBUTING.md file for guidelines on how to contribute to this project.

6. License

This project is licensed under the MIT License - see the LICENSE file for details.

7. Disclaimer

This project is for educational and research purposes only. Trading cryptocurrencies involves significant financial risks, and this bot should not be used for live trading without thorough understanding and careful consideration of these risks. The developers are not responsible for any financial losses incurred while using this software.

Note: This README provides a general overview. Please refer to the documentation in the docs/ directory for more detailed information on specific aspects of the project, such as model architectures, reward function design, configuration options, and usage instructions.

// File: config\base_config.yaml
# Add content here


// File: config\exchange_config.yaml
# Add content here


// File: config\features.yaml
# Add content here


// File: config\model_config.yaml
# Add content here


// File: config\secrets.env


// File: config\trading_config.yaml
# Add content here


// File: docs\conf.py
# Add content here


// File: docs\index.rst
# Add content here


// File: environments\crypto_trading_env.py
# File: environments/crypto_trading_env.py

import gym
from gym import spaces
import numpy as np
from src.data_acquisition import DataProvider
from src.rewards import RewardFunction

class CryptoTradingEnv(gym.Env):
    """
    Custom Gym environment for cryptocurrency trading.
    """

    def __init__(self, data_provider: DataProvider, reward_function: RewardFunction, initial_balance: float = 1000):
        super(CryptoTradingEnv, self).__init__()

        self.data_provider = data_provider
        self.reward_function = reward_function
        self.initial_balance = initial_balance

        # Define action space (0: Hold, 1: Buy, 2: Sell)
        self.action_space = spaces.Discrete(3)

        # Define observation space (price, volume, indicators)
        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(10,), dtype=np.float32)  # Adjust shape as needed

        # Load data
        self.data = None  # This will be set in reset()

        # Initialize environment state
        self.reset()

    def reset(self):
        # Reset environment to initial state
        self.balance = self.initial_balance
        self.position = 0  # Number of units held
        self.current_step = 0

        # Load new data for this episode
        self.data = self.data_provider.get_data("BTC/USDT", "1h", "2023-01-01", "2023-01-31")  # Example parameters

        return self._get_observation()

    def step(self, action: int):
        # Execute trading action
        current_price = self._get_current_price()

        if action == 1:  # Buy
            shares_to_buy = self.balance / current_price
            self.position += shares_to_buy
            self.balance -= shares_to_buy * current_price
        elif action == 2:  # Sell
            self.balance += self.position * current_price
            self.position = 0

        # Move to next time step
        self.current_step += 1

        # Calculate reward
        next_price = self._get_next_price()
        portfolio_value = self.balance + self.position * next_price
        reward = self.reward_function.calculate_reward(action, current_price, next_price, portfolio_value)

        # Check if episode is done
        done = self.current_step >= len(self.data) - 1

        return self._get_observation(), reward, done, {}

    def _get_observation(self):
        # Construct observation vector
        obs = self.data.iloc[self.current_step]
        return np.array([
            obs['close'],
            obs['volume'],
            obs['SMA_20'],
            obs['RSI'],
            obs['MACD'],
            obs['ATR'],
            obs['pct_change'],
            obs['volatility'],
            self.balance,
            self.position
        ])

    def _get_current_price(self):
        return self.data.iloc[self.current_step]['close']

    def _get_next_price(self):
        return self.data.iloc[self.current_step + 1]['close']

# Example usage
# env = CryptoTradingEnv(data_provider=data_provider, reward_function=reward_function)

// File: models\evaluator.py
# Add content here


// File: models\model.py
# File: models/model.py

import torch
import torch.nn as nn

class TradingModel(nn.Module):
    """
    Deep reinforcement learning model for trading.
    """

    def __init__(self, input_size: int, hidden_size: int, output_size: int):
        super(TradingModel, self).__init__()

        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # Pass input through LSTM
        out, _ = self.lstm(x)

        # Take the output from the last timestep
        out = out[:, -1, :]

        # Pass through fully connected layer
        out = self.fc(out)

        return out

# Example usage
# model = TradingModel(input_size=10, hidden_size=64, output_size=3)  # 3 output neurons for 3 actions

// File: models\trainer.py
# Add content here


// File: models\__init__.py
# Add content here


// File: scripts\backtest.py
# Add content here


// File: scripts\deploy.sh
# Add content here


// File: scripts\download_data.py
# Add content here


// File: scripts\preprocess_data.py
# Add content here


// File: scripts\run.py
# Add content here


// File: scripts\tune_hyperparameters.py
# Add content here


// File: scripts\__init__.py
# Add content here


// File: src\agent_manager.py
# File: src/agent_manager.py

from typing import List
import ray
from environments.crypto_trading_env import CryptoTradingEnv
from models.model import TradingModel
from src.rewards import RewardFunction

@ray.remote
class TradingAgent:
    """
    Represents a single trading agent.
    """

    def __init__(self, env: CryptoTradingEnv, model: TradingModel, reward_function: RewardFunction):
        self.env = env
        self.model = model
        self.reward_function = reward_function

    def train(self, num_episodes: int):
        total_reward = 0
        for episode in range(num_episodes):
            state = self.env.reset()
            done = False
            episode_reward = 0

            while not done:
                # Get action from model
                action = self.model(torch.from_numpy(state).float().unsqueeze(0)).argmax().item()

                # Take action in environment
                next_state, reward, done, _ = self.env.step(action)

                # Update model (simplified, you might want to use a proper RL algorithm here)
                # This is a placeholder for the actual training logic
                loss = torch.nn.functional.mse_loss(
                    self.model(torch.from_numpy(state).float().unsqueeze(0)).squeeze(),
                    torch.tensor([reward]).float()
                )
                loss.backward()
                # Perform optimization step (omitted for brevity)

                state = next_state
                episode_reward += reward

            total_reward += episode_reward

        return total_reward / num_episodes

class AgentManager:
    """
    Manages multiple trading agents.
    """

    def __init__(self, agents: List[TradingAgent]):
        self.agents = agents

    def train_agents(self, num_episodes: int):
        # Train agents in parallel using Ray
        results = ray.get([agent.train.remote(num_episodes) for agent in self.agents])
        return results

# Example usage
# ray.init()
# agents = [TradingAgent.remote(env, model, reward_function) for _ in range(4)]  # Create 4 agents
# agent_manager = AgentManager(agents)
# results = agent_manager.train_agents(num_episodes=100)

// File: src\data_acquisition.py
# File: src/data_acquisition.py

from abc import ABC, abstractmethod
import ccxt
import pandas as pd

class DataProvider(ABC):
    """Abstract base class for data providers."""

    @abstractmethod
    def get_data(self, symbol: str, timeframe: str, start_date: str, end_date: str) -> pd.DataFrame:
        """
        Retrieves historical data for a given symbol and timeframe.
        """
        pass

class BinanceDataProvider(DataProvider):
    """Data provider for Binance exchange."""

    def __init__(self, api_key: str, api_secret: str):
        self.exchange = ccxt.binance({
            'apiKey': api_key,
            'secret': api_secret,
        })

    def get_data(self, symbol: str, timeframe: str, start_date: str, end_date: str) -> pd.DataFrame:
        # Fetch data from Binance API
        data = self.exchange.fetch_ohlcv(symbol, timeframe, since=start_date, limit=1000)  # Adjust limit as needed

        # Convert to pandas DataFrame
        df = pd.DataFrame(data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')

        return df

# Example usage
# data_provider = BinanceDataProvider(api_key="YOUR_API_KEY", api_secret="YOUR_API_SECRET")
# df = data_provider.get_data(symbol="BTC/USDT", timeframe="1h", start_date="2023-01-01", end_date="2023-01-31")

// File: src\data_loader.py
# Add content here


// File: src\experiment_manager.py
# File: src/experiment_manager.py

import mlflow
import optuna
from src.agent_manager import AgentManager, TradingAgent
from environments.crypto_trading_env import CryptoTradingEnv
from models.model import TradingModel
from src.rewards import ProfitReward, SharpeRatioReward
from src.data_acquisition import BinanceDataProvider

class ExperimentManager:
    """
    Manages experiment tracking and hyperparameter optimization.
    """

    def __init__(self, experiment_name: str):
        mlflow.set_experiment(experiment_name)

    def run_experiment(self, params: dict):
        with mlflow.start_run():
            # Log parameters
            mlflow.log_params(params)

            # Set up environment and agents
            data_provider = BinanceDataProvider(api_key=params['api_key'], api_secret=params['api_secret'])
            reward_function = ProfitReward() if params['reward_type'] == 'profit' else SharpeRatioReward()
            env = CryptoTradingEnv(data_provider, reward_function, initial_balance=params['initial_balance'])
            model = TradingModel(input_size=10, hidden_size=params['hidden_size'], output_size=3)
            
            agents = [TradingAgent.remote(env, model, reward_function) for _ in range(params['num_agents'])]
            agent_manager = AgentManager(agents)

            # Train agents
            results = agent_manager.train_agents(num_episodes=params['num_episodes'])

            # Calculate and log metrics
            avg_reward = sum(results) / len(results)
            mlflow.log_metric("average_reward", avg_reward)

            return avg_reward

    def optimize_hyperparameters(self, objective_function, num_trials: int):
        study = optuna.create_study(direction="maximize")
        study.optimize(objective_function, n_trials=num_trials)

        return study.best_params

# Example usage
# experiment_manager = ExperimentManager(experiment_name="Trading Experiment")

# def objective(trial):
#     params = {
#         'api_key': 'YOUR_API_KEY',
#         'api_secret': 'YOUR_API_SECRET',
#         'reward_type': trial.suggest_categorical('reward_type', ['profit', 'sharpe']),
#         'initial_balance': trial.suggest_float('initial_balance', 1000, 10000),
#         'hidden_size': trial.suggest_int('hidden_size', 32, 256),
#         'num_agents': trial.suggest_int('num_agents', 1, 8),
#         'num_episodes': 100

// File: src\feature_engineering.py
# File: src/feature_engineering.py

import talib
import pandas as pd

class FeatureEngineer:
    """
    Calculates technical indicators and other features.
    """

    def add_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Adds common technical indicators to the DataFrame.
        """
        df['SMA_20'] = talib.SMA(df['close'], timeperiod=20)
        df['RSI'] = talib.RSI(df['close'], timeperiod=14)
        df['MACD'], df['MACD_Signal'], df['MACD_Hist'] = talib.MACD(df['close'])
        df['ATR'] = talib.ATR(df['high'], df['low'], df['close'])

        return df

    def add_custom_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Adds custom engineered features to the DataFrame.
        """
        # Example: Calculate percentage change
        df['pct_change'] = df['close'].pct_change()

        # Example: Calculate rolling volatility
        df['volatility'] = df['close'].rolling(window=20).std()

        # Add more custom features as needed

        return df

# Example usage
# feature_engineer = FeatureEngineer()
# df = feature_engineer.add_technical_indicators(df)
# df = feature_engineer.add_custom_features(df)

// File: src\feature_selection.py
# Add content here


// File: src\feature_store.py
# Add content here


// File: src\main.py
# Add content here


// File: src\rewards.py
# File: src/rewards.py

from abc import ABC, abstractmethod
import numpy as np

class RewardFunction(ABC):
    """Abstract base class for reward functions."""

    @abstractmethod
    def calculate_reward(self, action: int, current_price: float, next_price: float, portfolio_value: float) -> float:
        """
        Calculates the reward for a given action and market state.
        """
        pass

class ProfitReward(RewardFunction):
    """Reward function based on realized profit."""

    def calculate_reward(self, action: int, current_price: float, next_price: float, portfolio_value: float) -> float:
        if action == 1:  # Buy
            return (next_price - current_price) / current_price
        elif action == 2:  # Sell
            return (current_price - next_price) / current_price
        else:  # Hold
            return 0

class SharpeRatioReward(RewardFunction):
    """Reward function based on Sharpe ratio."""

    def __init__(self, risk_free_rate: float = 0.0):
        self.risk_free_rate = risk_free_rate
        self.returns = []

    def calculate_reward(self, action: int, current_price: float, next_price: float, portfolio_value: float) -> float:
        # Calculate portfolio return
        return_ = (portfolio_value - self.prev_portfolio_value) / self.prev_portfolio_value if hasattr(self, 'prev_portfolio_value') else 0
        self.returns.append(return_)
        self.prev_portfolio_value = portfolio_value

        # Calculate Sharpe ratio (if enough data is available)
        if len(self.returns) > 2:
            sharpe_ratio = (np.mean(self.returns) - self.risk_free_rate) / np.std(self.returns)
            return sharpe_ratio
        else:
            return 0

# Example usage
# reward_function = ProfitReward()
# reward = reward_function.calculate_reward(action=1, current_price=100, next_price=110, portfolio_value=1000)

// File: src\sentiment_analysis.py
# Add content here


// File: src\trading.py
# Add content here


// File: src\utils.py
# Add content here


// File: src\__init__.py
# Add content here


// File: tests\test_agent_manager.py
# Add content here


// File: tests\test_data_acquisition.py
# Add content here


// File: tests\test_environment.py
# Add content here


// File: tests\test_feature_engineering.py
# Add content here


// File: tests\test_feature_selection.py
# Add content here


// File: tests\test_model.py
# Add content here


// File: tests\test_trading.py
# Add content here


// File: tests\test_trainer.py
# Add content here


// File: tests\test_utils.py
# Add content here


// File: tests\__init__.py
# Add content here


